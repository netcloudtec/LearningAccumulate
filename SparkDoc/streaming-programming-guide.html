

<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Spark Streaming - Spark 2.4.3 Documentation</title>

    <meta name="description" content="Spark Streaming programming guide and tutorial for Spark 2.4.3">




    <link rel="stylesheet" href="css/bootstrap.min.css">
    <style>
        body {
            padding-top: 60px;
            padding-bottom: 40px;
        }
    </style>
    <meta name="viewport" content="width=device-width">
    <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
    <link rel="stylesheet" href="css/main.css">

    <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

    <link rel="stylesheet" href="css/pygments-default.css">


    <!-- Google analytics script -->
    <script type="text/javascript">
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-32518208-2']);
        _gaq.push(['_trackPageview']);

        (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();
    </script>


</head>
<body>
<!--[if lt IE 7]>
<p class="chromeframe">You are using an outdated browser. <a href="https://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
<![endif]-->

<!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

<div class="navbar navbar-fixed-top" id="topbar">
    <div class="navbar-inner">
        <div class="container">
            <div class="brand"><a href="index.html">
                <img src="img/spark-logo-hd.png" style="height:50px;"/></a><span class="version">2.4.3</span>
            </div>
            <ul class="nav">
                <!--TODO(andyk): Add class="active" attribute to li some how.-->
                <li><a href="index.html">概述</a></li>

                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">编程指南<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        <li><a href="quick-start.html">Quick Start</a></li>
                        <li><a href="rdd-programming-guide.html">RDDs, Accumulators, Broadcasts Vars</a></li>
                        <li><a href="sql-programming-guide.html">SQL, DataFrames, and Datasets</a></li>
                        <li><a href="structured-streaming-programming-guide.html">Structured Streaming</a></li>
                        <li><a href="streaming-programming-guide.html">Spark Streaming (DStreams)</a></li>
                        <li><a href="ml-guide.html">MLlib (Machine Learning)</a></li>
                        <li><a href="graphx-programming-guide.html">GraphX (Graph Processing)</a></li>
                        <li><a href="sparkr.html">SparkR (R on Spark)</a></li>
                    </ul>
                </li>

                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">API 文档<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        <li><a href="api/scala/index.html#org.apache.spark.package">Scala</a></li>
                        <li><a href="api/java/index.html">Java</a></li>
                        <li><a href="api/python/index.html">Python</a></li>
                        <li><a href="api/R/index.html">R</a></li>
                        <li><a href="api/sql/index.html">SQL, Built-in Functions</a></li>
                    </ul>
                </li>

                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">部署<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        <li><a href="cluster-overview.html">Overview</a></li>
                        <li><a href="submitting-applications.html">Submitting Applications</a></li>
                        <li class="divider"></li>
                        <li><a href="spark-standalone.html">Spark Standalone</a></li>
                        <li><a href="running-on-mesos.html">Mesos</a></li>
                        <li><a href="running-on-yarn.html">YARN</a></li>
                        <li><a href="running-on-kubernetes.html">Kubernetes</a></li>
                    </ul>
                </li>

                <li class="dropdown">
                    <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">更多<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        <li><a href="configuration.html">Configuration</a></li>
                        <li><a href="monitoring.html">Monitoring</a></li>
                        <li><a href="tuning.html">Tuning Guide</a></li>
                        <li><a href="job-scheduling.html">Job Scheduling</a></li>
                        <li><a href="security.html">Security</a></li>
                        <li><a href="hardware-provisioning.html">Hardware Provisioning</a></li>
                        <li class="divider"></li>
                        <li><a href="building-spark.html">Building Spark</a></li>
                        <li><a href="https://spark.apache.org/contributing.html">Contributing to Spark</a></li>
                        <li><a href="https://spark.apache.org/third-party-projects.html">Third Party Projects</a></li>
                    </ul>
                </li>
            </ul>
            <!--<p class="navbar-text pull-right"><span class="version-text">v2.4.3</span></p>-->
        </div>
    </div>
</div>

<div class="container-wrapper">


    <div class="content" id="content">

        <h1 class="title">Spark Streaming 编程指南</h1>


        <ul id="markdown-toc">
            <li><a href="#overview" id="markdown-toc-overview">概述</a></li>
            <li><a href="#a-quick-example" id="markdown-toc-a-quick-example">入门示例</a></li>
            <li><a href="#basic-concepts" id="markdown-toc-basic-concepts">基本概念</a>    <ul>
                <li><a href="#linking" id="markdown-toc-linking">依赖</a></li>
                <li><a href="#initializing-streamingcontext" id="markdown-toc-initializing-streamingcontext">初始化 StreamingContext</a></li>
                <li><a href="#discretized-streams-dstreams" id="markdown-toc-discretized-streams-dstreams">DStreams（离散化流）</a></li>
                <li><a href="#input-dstreams-and-receivers" id="markdown-toc-input-dstreams-and-receivers">Input DStreams 和 Receivers （接收器）</a></li>
                <li><a href="#transformations-on-dstreams" id="markdown-toc-transformations-on-dstreams">DStreams 上的 Transformations on（转换）</a></li>
                <li><a href="#output-operations-on-dstreams" id="markdown-toc-output-operations-on-dstreams">DStreams 的输出操作</a></li>
                <li><a href="#dataframe-and-sql-operations" id="markdown-toc-dataframe-and-sql-operations">DataFrame 和 SQL 操作</a></li>
                <li><a href="#mllib-operations" id="markdown-toc-mllib-operations">MLlib 操作</a></li>
                <li><a href="#caching--persistence" id="markdown-toc-caching--persistence">缓存 / 持久化</a></li>
                <li><a href="#checkpointing" id="markdown-toc-checkpointing">Checkpointing</a></li>
                <li><a href="#accumulators-broadcast-variables-and-checkpoints" id="markdown-toc-accumulators-broadcast-variables-and-checkpoints">Accumulators, Broadcast Variables, 和 Checkpoints</a></li>
                <li><a href="#deploying-applications" id="markdown-toc-deploying-applications">应用程序部署</a></li>
                <li><a href="#monitoring-applications" id="markdown-toc-monitoring-applications">监控应用程序</a></li>
            </ul>
            </li>
            <li><a href="#performance-tuning" id="markdown-toc-performance-tuning">性能调优</a>    <ul>
                <li><a href="#reducing-the-batch-processing-times" id="markdown-toc-reducing-the-batch-processing-times">减少批处理时间</a></li>
                <li><a href="#setting-the-right-batch-interval" id="markdown-toc-setting-the-right-batch-interval">设置合适的 Batch Interval</a></li>
                <li><a href="#memory-tuning" id="markdown-toc-memory-tuning">内存调优</a></li>
            </ul>
            </li>
            <li><a href="#fault-tolerance-semantics" id="markdown-toc-fault-tolerance-semantics">容错</a></li>
            <li><a href="#where-to-go-from-here" id="markdown-toc-where-to-go-from-here">快速链接</a></li>
        </ul>

        <h1 id="overview">概述</h1>
        <p>Spark Streaming 是 Spark Core API 的扩展，它支持弹性的，高吞吐的，容错的实时数据流的处理。数据可以通过多种数据源获取，例如 Kafka，Flume，Kinesis 以及 TCP sockets，SparkStreaming 可以通过例如 <code>map</code>, <code>reduce</code>, <code>join</code> 和 <code>window</code>
            等高级函数进行数据的处理。最终，处理后的数据可以输出到文件系统，数据库以及实时仪表盘中。实际上，你还可以在 data streams（数据流）上使用
            <a href="ml-guide.html">机器学习</a> 和
            <a href="graphx-programming-guide.html">图计算</a>算法。</p>

        <p style="text-align: center;">
            <img src="img/streaming-arch.png" title="Spark Streaming architecture" alt="Spark Streaming" width="70%" />
        </p>

        <p>
            下图展示了Spark Streaming的内部工作原理: Spark Streaming从实时数据流接入数据，再将其划分为一个个 batch（批）数据供后续 Spark engine 处理；最终生成 stream of results in batches（分批流结果）。
            所以实际上，Spark Streaming 是按一个个小批量（batch）来处理数据流的。
        </p>

        <p style="text-align: center;">
            <img src="img/streaming-flow.png" title="Spark Streaming data flow" alt="Spark Streaming" width="70%" />
        </p>
        <p>Spark Streaming 提供了一个名为 <em>discretized stream</em> 或 <em>DStream （离散化数据流）</em>的高级抽象,
            它代表一个持续的数据流（重点理解：它代表着整个持续的流而不是每个 batch）。DStream 可以从输入数据源创建，例如 Kafka，Flume 以及 Kinesis，或者从其他 DStream 经一些算子操作得到（参考RDD理解）。在其内部，一个 DStream 包含了一系列的 <a href="api/scala/index.html#org.apache.spark.rdd.RDD">RDDs</a>。
            。</p>

        <p>本指南告诉你如何使用 DStream 来编写一个 Spark Streaming 程序。你可以使用 Scala，Java 或者 Python（Spark 1.2 版本后引进）来编写 Spark Streaming 程序。所有这些都在本指南中介绍。您可以在本指南中找到标签，让您可以选择不同语言的代码段。</p>

        <p><strong>Note（注意）:</strong> 在 Python 中有些 API 尚不支持，或者和Scala、Java不同。在本指南，您将找到 <span class="badge" style="background-color: grey">Python API</span> 的标签来高亮显示不同的地方。</p>

        <p><strong>疑问: </strong>官方文档说 DStream 是一个 rdds 的集合，里面包含多个rdd （每个 RDD 对应一个batch、每个batch中含有多个block）。而通过 KafkaUtils.createDirectStream来生成InputDStream，自己测试发现，DStream 中只有一个RDD，这个有没有高手可以指导一下，生成rdd的个数的依据是什么？？？<br>
         解答：DStream是整个持续数据流的高级抽象，而我们程序中指定的 batchDuration 是用来数据被划分为一个个的batch的，每个batch 对应着一个RDD 所以说 KafkaUtils.createDirectStream来生成InputDStream 其实就是处理的当前 batch 的数据也就是只有一个RDD。<br>
        <p><strong>foreachRDD疑问？</strong>
            既然每个 batch interval只产生一个 RDD 那么这个 each RDD怎么理解？？？<br>
            DStream可以理解为是基于时间的，即每个interval产生一个RDD，所以如果以时间为轴，每隔一段时间就会产生一个RDD，那么定义中的“each RDD”应该理解为每个interval的RDD，而不是一个interval中的每个RDD。<br>
        </p>

        <p><strong>扩展: 结合上图理解</strong><br>
            1）切分batch: Streaming engine就会用一个batch来收集数据，凑够一个batch了就发走了（batch的大小是时间大小）， 问题又来了那batch里是啥？
            <br>
            2) block:batch里也不会直接是数据，batch会有block（熟悉把，就是那个64m的block），block内会接受数据流，当一个block存满了，就去存下一个block，直到batch时间到了。block的大小（conf：spark.streaming.blockInterval ）
            <br>
            3）RDD:然后这个batch（里面有一些block）就会发送到Spark Engine 生成RDD， 而里面的block就变成的RDD的partition。所以一个 batch里只有一个RDD<br>
            4）Dstream就简单了，就把他想象成一堆连续的RDD就行， 没啥特别的。<br>
            5）<br>
              RDD = （ 一堆partitions 组成）<br>
              Batch = （一堆block组成）<br>
            （一堆block） ------->经过spark engine------->(一堆partitions，生成RDD)<br>
              所以要是说Batch和RDD是一个东西好像也不太对，可能就是进化关系把。。。<br>
              所以Spark Streaming里最基本的划分是从batch来划分流生成RDD，所以如果你打算生成window， window必须要是batch interval的整数倍（数据是这么切的啊。。。）
        </p>
        <hr />

        <h1 id="a-quick-example">一个入门示例</h1>
        <p>在我们详细介绍如何编写你自己的 Spark Streaming 程序的细节之前，让我们先来看一看一个简单的 Spark Streaming 程序的样子。比方说，我们从一个TCP端口上监听一个数据服务器上的数据，并对收到的文本数据中的单词计数，你需要做的就是照着下面的步骤做。</p>

        <div class="codetabs">
            <div data-lang="scala">
                <p>首先，我们需要导入Spark Streaming的相关class的一些包，以及一些支持StreamingContext隐式转换的包。<a href="api/scala/index.html#org.apache.spark.streaming.StreamingContext">StreamingContext</a>
                    是所有流功能的主要入口点。我们将会创建一个本地 StreamingContext对象，包含 2 个执行线程，批次间隔设为1秒。</p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">import</span> <span class="nn">org.apache.spark._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.streaming._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.streaming.StreamingContext._</span> <span class="c1">// not necessary since Spark 1.3</span>

<span class="c1">// 创建一个具有两个工作线程（working thread）并且批次间隔为 1 秒的本地 StreamingContext。</span>
<span class="c1">// master至少需要2个CPU核，以避免出现任务饿死的情况。一个用来接收数据，一个用来计算。</span>


<span class="k">val</span> <span class="n">conf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">().</span><span class="n">setMaster</span><span class="o">(</span><span class="s">&quot;local[2]&quot;</span><span class="o">).</span><span class="n">setAppName</span><span class="o">(</span><span class="s">&quot;NetworkWordCount&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">ssc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StreamingContext</span><span class="o">(</span><span class="n">conf</span><span class="o">,</span> <span class="nc">Seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span></code></pre></figure>

                <p>使用该 StreamingContext，我们可以创建 DStream, 该 DStream 代表从前面的 TCP 数据源不断流入的数据流。同时 TCP 数据源是由主机名 (例如 <code>localhost</code>) 和端口 (例如 <code>9999</code>)来描述的。</p>
                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="c1">// 创建一个将要连接到 hostname:port 的 DStream，如 localhost:9999 </span>
<span class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">ssc</span><span class="o">.</span><span class="n">socketTextStream</span><span class="o">(</span><span class="s">&quot;localhost&quot;</span><span class="o">,</span> <span class="mi">9999</span><span class="o">)</span></code></pre></figure>

                <p>
                    这里的 lines DStream 表示从数据server接收到的数据流。在这个离散流（DStream）中的每一条记录都是一行文本（text）。接下来，我们就需要把这些文本行按空格分割成单词。
                </p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="c1">// 将每一行分割成多个单词</span>
<span class="k">val</span> <span class="n">words</span> <span class="k">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">))</span></code></pre></figure>

                <p><code>flatMap</code> 是一种一对多的映射算子，它可以将 DStream 的每条记录映射成多条记录，从而产生一个新的DStream对象。在这种情况下，在本例中，lines中的每一行都会被flatMap映射为多个单词，从而生成新的words DStream对象。然后，我们就能对这些单词进行计数了。</p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">import</span> <span class="nn">org.apache.spark.streaming.StreamingContext._</span> <span class="c1">// not necessary since Spark 1.3</span>
<span class="c1">// 对每一批次中的单词进行计数</span>
<span class="k">val</span> <span class="n">pairs</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">word</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">word</span><span class="o">,</span> <span class="mi">1</span><span class="o">))</span>
<span class="k">val</span> <span class="n">wordCounts</span> <span class="k">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">reduceByKey</span><span class="o">(</span><span class="k">_</span> <span class="o">+</span> <span class="k">_</span><span class="o">)</span>

<span class="c1">// 将该DStream产生的RDD的头十个元素打印到控制台上</span>
<span class="c1">// 注意：必需要触发 action（很多初学者会忘记触发 action 操作，导致报错：No output operations registered, so nothing to execute）</span>

<span class="n">wordCounts</span><span class="o">.</span><span class="n">print</span><span class="o">()</span></code></pre></figure>

                <p>
                     <code>words</code> 这个 DStream 经过map算子（一到一的映射）转换为一个包含 <code>(word,1)</code> 键值对的DStream对象pairs，
                    再对pairs使用reduce算子，得到每个批次中各个单词的出现频率。最后，<code>wordCounts.print()</code> 将会每秒（前面设定的批次间隔）打印一些单词计数到控制台上。
                </p>

                <p>注意，执行以上代码后，Spark Streaming只是将计算逻辑设置好，此时并未真正的开始处理数据。要启动之前的处理逻辑，我们还需要如下调用：</p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">ssc</span><span class="o">.</span><span class="n">start</span><span class="o">()</span>             <span class="c1">// 启动流式计算</span>
<span class="n">ssc</span><span class="o">.</span><span class="n">awaitTermination</span><span class="o">()</span>  <span class="c1">// 等待直到计算终止</span></code></pre></figure>

                <p>完整的代码可以在 Spark Streaming 示例
                    <a href="https://github.com/apache/spark/blob/v2.4.3/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala">NetworkWordCount</a>中找到。
                    <br /></p>

            </div>
            <div data-lang="java">

                <p>First, we create a
                    <a href="api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html">JavaStreamingContext</a> object,
                    which is the main entry point for all streaming
                    functionality. We create a local StreamingContext with two execution threads, and a batch interval of 1 second.</p>

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.*</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.api.java.function.*</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.streaming.*</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.streaming.api.java.*</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">scala.Tuple2</span><span class="o">;</span>

<span class="c1">// Create a local StreamingContext with two working thread and batch interval of 1 second</span>
<span class="n">SparkConf</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SparkConf</span><span class="o">().</span><span class="na">setMaster</span><span class="o">(</span><span class="s">&quot;local[2]&quot;</span><span class="o">).</span><span class="na">setAppName</span><span class="o">(</span><span class="s">&quot;NetworkWordCount&quot;</span><span class="o">);</span>
<span class="n">JavaStreamingContext</span> <span class="n">jssc</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JavaStreamingContext</span><span class="o">(</span><span class="n">conf</span><span class="o">,</span> <span class="n">Durations</span><span class="o">.</span><span class="na">seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">));</span></code></pre></figure>

                <p>Using this context, we can create a DStream that represents streaming data from a TCP
                    source, specified as hostname (e.g. <code>localhost</code>) and port (e.g. <code>9999</code>).</p>

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="c1">// Create a DStream that will connect to hostname:port, like localhost:9999</span>
<span class="n">JavaReceiverInputDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">lines</span> <span class="o">=</span> <span class="n">jssc</span><span class="o">.</span><span class="na">socketTextStream</span><span class="o">(</span><span class="s">&quot;localhost&quot;</span><span class="o">,</span> <span class="mi">9999</span><span class="o">);</span></code></pre></figure>

                <p>This <code>lines</code> DStream represents the stream of data that will be received from the data
                    server. Each record in this stream is a line of text. Then, we want to split the lines by
                    space into words.</p>

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="c1">// Split each line into words</span>
<span class="n">JavaDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">words</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="n">x</span> <span class="o">-&gt;</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">)).</span><span class="na">iterator</span><span class="o">());</span></code></pre></figure>

                <p><code>flatMap</code> is a DStream operation that creates a new DStream by
                    generating multiple new records from each record in the source DStream. In this case,
                    each line will be split into multiple words and the stream of words is represented as the
                    <code>words</code> DStream. Note that we defined the transformation using a
                    <a href="api/scala/index.html#org.apache.spark.api.java.function.FlatMapFunction">FlatMapFunction</a> object.
                    As we will discover along the way, there are a number of such convenience classes in the Java API
                    that help defines DStream transformations.</p>

                <p>Next, we want to count these words.</p>

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="c1">// Count each word in each batch</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="na">mapToPair</span><span class="o">(</span><span class="n">s</span> <span class="o">-&gt;</span> <span class="k">new</span> <span class="n">Tuple2</span><span class="o">&lt;&gt;(</span><span class="n">s</span><span class="o">,</span> <span class="mi">1</span><span class="o">));</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="n">wordCounts</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="na">reduceByKey</span><span class="o">((</span><span class="n">i1</span><span class="o">,</span> <span class="n">i2</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">i1</span> <span class="o">+</span> <span class="n">i2</span><span class="o">);</span>

<span class="c1">// Print the first ten elements of each RDD generated in this DStream to the console</span>
<span class="n">wordCounts</span><span class="o">.</span><span class="na">print</span><span class="o">();</span></code></pre></figure>

                <p>The <code>words</code> DStream is further mapped (one-to-one transformation) to a DStream of <code>(word,
                    1)</code> pairs, using a <a href="api/scala/index.html#org.apache.spark.api.java.function.PairFunction">PairFunction</a>
                    object. Then, it is reduced to get the frequency of words in each batch of data,
                    using a <a href="api/scala/index.html#org.apache.spark.api.java.function.Function2">Function2</a> object.
                    Finally, <code>wordCounts.print()</code> will print a few of the counts generated every second.</p>

                <p>Note that when these lines are executed, Spark Streaming only sets up the computation it
                    will perform after it is started, and no real processing has started yet. To start the processing
                    after all the transformations have been setup, we finally call <code>start</code> method.</p>

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">jssc</span><span class="o">.</span><span class="na">start</span><span class="o">();</span>              <span class="c1">// Start the computation</span>
<span class="n">jssc</span><span class="o">.</span><span class="na">awaitTermination</span><span class="o">();</span>   <span class="c1">// Wait for the computation to terminate</span></code></pre></figure>

                <p>The complete code can be found in the Spark Streaming example
                    <a href="https://github.com/apache/spark/blob/v2.4.3/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java">JavaNetworkWordCount</a>.
                    <br /></p>

            </div>
            <div data-lang="python">
                <p>First, we import <a href="api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext">StreamingContext</a>, which is the main entry point for all streaming functionality. We create a local StreamingContext with two execution threads, and batch interval of 1 second.</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span>
<span class="kn">from</span> <span class="nn">pyspark.streaming</span> <span class="kn">import</span> <span class="n">StreamingContext</span>

<span class="c1"># Create a local StreamingContext with two working thread and batch interval of 1 second</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="s2">&quot;local[2]&quot;</span><span class="p">,</span> <span class="s2">&quot;NetworkWordCount&quot;</span><span class="p">)</span>
<span class="n">ssc</span> <span class="o">=</span> <span class="n">StreamingContext</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span></code></pre></figure>

                <p>Using this context, we can create a DStream that represents streaming data from a TCP
                    source, specified as hostname (e.g. <code>localhost</code>) and port (e.g. <code>9999</code>).</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="c1"># Create a DStream that will connect to hostname:port, like localhost:9999</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">ssc</span><span class="o">.</span><span class="n">socketTextStream</span><span class="p">(</span><span class="s2">&quot;localhost&quot;</span><span class="p">,</span> <span class="mi">9999</span><span class="p">)</span></code></pre></figure>

                <p>This <code>lines</code> DStream represents the stream of data that will be received from the data
                    server. Each record in this DStream is a line of text. Next, we want to split the lines by
                    space into words.</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="c1"># Split each line into words</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">))</span></code></pre></figure>

                <p><code>flatMap</code> is a one-to-many DStream operation that creates a new DStream by
                    generating multiple new records from each record in the source DStream. In this case,
                    each line will be split into multiple words and the stream of words is represented as the
                    <code>words</code> DStream.  Next, we want to count these words.</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="c1"># Count each word in each batch</span>
<span class="n">pairs</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">word</span><span class="p">:</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">wordCounts</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Print the first ten elements of each RDD generated in this DStream to the console</span>
<span class="n">wordCounts</span><span class="o">.</span><span class="n">pprint</span><span class="p">()</span></code></pre></figure>

                <p>The <code>words</code> DStream is further mapped (one-to-one transformation) to a DStream of <code>(word,
                    1)</code> pairs, which is then reduced to get the frequency of words in each batch of data.
                    Finally, <code>wordCounts.pprint()</code> will print a few of the counts generated every second.</p>

                <p>Note that when these lines are executed, Spark Streaming only sets up the computation it
                    will perform when it is started, and no real processing has started yet. To start the processing
                    after all the transformations have been setup, we finally call</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">ssc</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>             <span class="c1"># Start the computation</span>
<span class="n">ssc</span><span class="o">.</span><span class="n">awaitTermination</span><span class="p">()</span>  <span class="c1"># Wait for the computation to terminate</span></code></pre></figure>

                <p>The complete code can be found in the Spark Streaming example
                    <a href="https://github.com/apache/spark/blob/v2.4.3/examples/src/main/python/streaming/network_wordcount.py">NetworkWordCount</a>.
                    <br /></p>

            </div>
        </div>

        <p>
            如果你已经 <a href="index.html#downloading">下载</a> 和 <a href="index.html#building">构建</a> 了 Spark，您可以使用如下方式来运行该示例。你首先需要运行 Netcat（一个在大多数类似 Unix 系统中的小工具）作为我们使用的数据服务器。
        </p>

        <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ nc -lk <span class="m">9999</span></code></pre></figure>

        <p>然后，在另一个不同的终端，你可以通过执行如下命令来运行该示例:

        </p>

        <div class="codetabs">
            <div data-lang="scala">

                <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ ./bin/run-example streaming.NetworkWordCount localhost <span class="m">9999</span></code></pre></figure>

            </div>
            <div data-lang="java">

                <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ ./bin/run-example streaming.JavaNetworkWordCount localhost <span class="m">9999</span></code></pre></figure>

            </div>
            <div data-lang="python">

                <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ ./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost <span class="m">9999</span></code></pre></figure>

            </div>
        </div>

        <p>好了，现在你尝试可以在运行netcat的终端里敲几个单词，你会发现这些单词以及相应的计数会出现在启动Spark Streaming例子的终端屏幕上。看上去应该和下面这个示意图类似：</p>

        <table width="100%">
            <td>

                <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span><span class="c1"># TERMINAL 1:</span>
<span class="c1"># Running Netcat</span>

$ nc -lk <span class="m">9999</span>

hello world



...</code></pre></figure>

            </td>
            <td width="2%"></td>
            <td>
                <div class="codetabs">

                    <div data-lang="scala">

                        <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span><span class="c1"># TERMINAL 2: RUNNING NetworkWordCount</span>

$ ./bin/run-example streaming.NetworkWordCount localhost <span class="m">9999</span>
...
-------------------------------------------
Time: <span class="m">1357008430000</span> ms
-------------------------------------------
<span class="o">(</span>hello,1<span class="o">)</span>
<span class="o">(</span>world,1<span class="o">)</span>
...</code></pre></figure>

                    </div>

                    <div data-lang="java">

                        <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span><span class="c1"># TERMINAL 2: RUNNING JavaNetworkWordCount</span>

$ ./bin/run-example streaming.JavaNetworkWordCount localhost <span class="m">9999</span>
...
-------------------------------------------
Time: <span class="m">1357008430000</span> ms
-------------------------------------------
<span class="o">(</span>hello,1<span class="o">)</span>
<span class="o">(</span>world,1<span class="o">)</span>
...</code></pre></figure>

                    </div>
                    <div data-lang="python">

                        <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span><span class="c1"># TERMINAL 2: RUNNING network_wordcount.py</span>

$ ./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost <span class="m">9999</span>
...
-------------------------------------------
Time: <span class="m">2014</span>-10-14 <span class="m">15</span>:25:21
-------------------------------------------
<span class="o">(</span>hello,1<span class="o">)</span>
<span class="o">(</span>world,1<span class="o">)</span>
...</code></pre></figure>

                    </div>
                </div>
            </td>
        </table>
        <div class="codetabs">
            <div data-lang="接收 TCP数据将处理的结果输出到 mysql 中">

    <pre><code>
package com.spark.test
import java.sql.DriverManager
import org.apache.spark.sql.SparkSession
import org.apache.spark.streaming.{Seconds, StreamingContext}

object TestStreaming {

  def main(args: Array[String]): Unit = {

    val spark  = SparkSession.builder()
      .master("local[2]") // 启动两个线程，一个接收数据、一个处理数据
      .appName("streaming").getOrCreate()

    val sc =spark.sparkContext
    // 每 5s 生成一个 batch
    val ssc = new StreamingContext(sc, Seconds(5))
    //接收 TCP sockets 数据
    val lines = ssc.socketTextStream("localhost", 9999)
    // 读取的行数据按照空格切分为单词数组，每个单词记为 1 然后再进行聚合
    val words = lines.flatMap(_.split(" ")).map(word => (word, 1)).reduceByKey(_ + _)
    // 聚合后的结果遍历 对当前RDD中每个分区的数据进行处理
    words.foreachRDD(rdd => rdd.foreachPartition(line => {
         // 数据库连接 每处理一个分区时候进行连接 而不是每条数据进行连接 减少数据库连接次数提高执行效率
         Class.forName("com.mysql.jdbc.Driver")
         val conn = DriverManager
           .getConnection("jdbc:mysql://123.57.75.98:3306/sparkstreaming_result","root","netcloud@123")
         try{
            // 遍历分区中的数据
            for(row <- line){
              val sql = "insert into webcount(titlename,count)values('"+row._1+"',"+row._2+")"
              conn.prepareStatement(sql).executeUpdate()
            }
         }finally {
            conn.close()
         }
    }))
      ssc.start()// 启动流式计算 否则程序无法执行
      ssc.awaitTermination()// 等待直到计算终止、程序挂起一致执行
  }
}
</code></pre>
            </div>
        </div>
        <hr />
        <hr />

        <h1 id="basic-concepts">基本概念</h1>

        <p>接下来，我们了解完了简单的例子，开始阐述 Spark Streaming 的基本知识。</p>

        <h2 id="linking">依赖</h2>

        <p> 与 Spark 类似，Spark Streaming 可以通过 Maven 来管理依赖。如果你需要编写Spark Streaming程序，你就需要将以下依赖加入到你的SBT或Maven工程依赖中。</p>

        <div class="codetabs">
            <div data-lang="Maven">

    <pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-streaming_2.12&lt;/artifactId&gt;
    &lt;version&gt;2.4.3&lt;/version&gt;
    &lt;scope&gt;provided&lt;/scope&gt;
&lt;/dependency&gt;
</code></pre>
            </div>
            <div data-lang="SBT">

    <pre><code>libraryDependencies += "org.apache.spark" % "spark-streaming_2.12" % "2.4.3" % "provided"
</code></pre>
            </div>
        </div>

        <p>还有，对于从Kafka、Flume以及Kinesis这类数据源提取数据的流式应用来说，还需要额外增加相应的依赖项，下表列出了各种数据源对应的额外依赖项：
        </p>

        <table class="table">
            <tr><th>Source</th><th>Artifact</th></tr>
            <tr><td> Kafka </td><td> spark-streaming-kafka-0-10_2.12 </td></tr>
            <tr><td> Flume </td><td> spark-streaming-flume_2.12 </td></tr>
            <tr><td> Kinesis<br /></td><td>spark-streaming-kinesis-asl_2.12 [Amazon Software License] </td></tr>
            <tr><td></td><td></td></tr>
        </table>

        <p>最新的依赖项信息，请参阅
            <a href="https://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.spark%22%20AND%20v%3A%222.4.3%22">Maven repository</a>
            包括源代码和Maven artifacts 。</p>

        <hr />

        <h2 id="initializing-streamingcontext">初始化 StreamingContext</h2>

        <p>

            要初始化任何一个Spark Streaming程序，都需要在入口代码中创建一个 <strong>StreamingContext</strong> 对象。</p>

        <div class="codetabs">
            <div data-lang="scala">

                <p><strong>一个<a href="api/scala/index.html#org.apache.spark.streaming.StreamingContext">StreamingContext</a> 对象需要一个<a href="api/scala/index.html#org.apache.spark.SparkConf">SparkConf</a> 对象作为其构造参数。</strong></p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">import</span> <span class="nn">org.apache.spark._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.streaming._</span>

<span class="k">val</span> <span class="n">conf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">().</span><span class="n">setAppName</span><span class="o">(</span><span class="n">appName</span><span class="o">).</span><span class="n">setMaster</span><span class="o">(</span><span class="n">master</span><span class="o">)</span>
<span class="k">val</span> <span class="n">ssc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StreamingContext</span><span class="o">(</span><span class="n">conf</span><span class="o">,</span> <span class="nc">Seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span></code></pre></figure>

                <p>
                    这个 <code>appName</code> 参数是展示在集群 UI 界面上的应用程序的名称。<code>master</code> 是一个 <a href="submitting-applications.html#master-urls">Spark, Mesos or YARN cluster URL</a>，或者一个特殊的 <strong>&#8220;local[*]&#8221;</strong> 字符串以使用 local mode（本地模式）来运行。在实践中，当在集群上运行时，你不会想在应用程序中硬编码 master，而是 <a href="submitting-applications.html"> </a>使用 spark-submit 来启动应用程序，并且接受该参数。然而，对于本地测试和单元测试，你可以传递 &#8220;local[*]&#8221; 来运行 Spark Streaming 进程（检测本地系统中内核的个数）。请注意，内部创建了一个 <a href="api/scala/index.html#org.apache.spark.SparkContext">SparkContext</a>（所有 Spark 功能的出发点），它可以像 ssc.sparkContext 这样被访问。</p>

                <p>
                    这个 batch interval（批间隔）必须根据您的应用程序和可用的集群资源的等待时间要求进行设置。更多详情请参阅 <a href="#setting-the-right-batch-interval">优化指南</a> 部分。
                </p>

                <p><strong>一个<code>StreamingContext</code> 对象也可以从一个现有的 <code>SparkContext</code> 对象来创建。</strong></p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">import</span> <span class="nn">org.apache.spark.streaming._</span>

<span class="k">val</span> <span class="n">sc</span> <span class="k">=</span> <span class="o">...</span>                <span class="c1">// 已存在的 SparkContext</span>
<span class="k">val</span> <span class="n">ssc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StreamingContext</span><span class="o">(</span><span class="n">sc</span><span class="o">,</span> <span class="nc">Seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span></code></pre></figure>

            </div>
            <div data-lang="java">

                <p>A <a href="api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html">JavaStreamingContext</a> object can be created from a <a href="api/java/index.html?org/apache/spark/SparkConf.html">SparkConf</a> object.</p>

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.*</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.streaming.api.java.*</span><span class="o">;</span>

<span class="n">SparkConf</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SparkConf</span><span class="o">().</span><span class="na">setAppName</span><span class="o">(</span><span class="n">appName</span><span class="o">).</span><span class="na">setMaster</span><span class="o">(</span><span class="n">master</span><span class="o">);</span>
<span class="n">JavaStreamingContext</span> <span class="n">ssc</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JavaStreamingContext</span><span class="o">(</span><span class="n">conf</span><span class="o">,</span> <span class="k">new</span> <span class="n">Duration</span><span class="o">(</span><span class="mi">1000</span><span class="o">));</span></code></pre></figure>

                <p>The <code>appName</code> parameter is a name for your application to show on the cluster UI.
                    <code>master</code> is a <a href="submitting-applications.html#master-urls">Spark, Mesos or YARN cluster URL</a>,
                    or a special <strong>&#8220;local[*]&#8221;</strong> string to run in local mode. In practice, when running on a cluster,
                    you will not want to hardcode <code>master</code> in the program,
                    but rather <a href="submitting-applications.html">launch the application with <code>spark-submit</code></a> and
                    receive it there. However, for local testing and unit tests, you can pass &#8220;local[*]&#8221; to run Spark Streaming
                    in-process. Note that this internally creates a <a href="api/java/index.html?org/apache/spark/api/java/JavaSparkContext.html">JavaSparkContext</a> (starting point of all Spark functionality) which can be accessed as <code>ssc.sparkContext</code>.</p>

                <p>The batch interval must be set based on the latency requirements of your application
                    and available cluster resources. See the <a href="#setting-the-right-batch-interval">Performance Tuning</a>
                    section for more details.</p>

                <p>A <code>JavaStreamingContext</code> object can also be created from an existing <code>JavaSparkContext</code>.</p>

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.streaming.api.java.*</span><span class="o">;</span>

<span class="n">JavaSparkContext</span> <span class="n">sc</span> <span class="o">=</span> <span class="o">...</span>   <span class="c1">//existing JavaSparkContext</span>
<span class="n">JavaStreamingContext</span> <span class="n">ssc</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JavaStreamingContext</span><span class="o">(</span><span class="n">sc</span><span class="o">,</span> <span class="n">Durations</span><span class="o">.</span><span class="na">seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">));</span></code></pre></figure>

            </div>
            <div data-lang="python">

                <p>A <a href="api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext">StreamingContext</a> object can be created from a <a href="api/python/pyspark.html#pyspark.SparkContext">SparkContext</a> object.</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span>
<span class="kn">from</span> <span class="nn">pyspark.streaming</span> <span class="kn">import</span> <span class="n">StreamingContext</span>

<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">master</span><span class="p">,</span> <span class="n">appName</span><span class="p">)</span>
<span class="n">ssc</span> <span class="o">=</span> <span class="n">StreamingContext</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span></code></pre></figure>

                <p>The <code>appName</code> parameter is a name for your application to show on the cluster UI.
                    <code>master</code> is a <a href="submitting-applications.html#master-urls">Spark, Mesos or YARN cluster URL</a>,
                    or a special <strong>&#8220;local[*]&#8221;</strong> string to run in local mode. In practice, when running on a cluster,
                    you will not want to hardcode <code>master</code> in the program,
                    but rather <a href="submitting-applications.html">launch the application with <code>spark-submit</code></a> and
                    receive it there. However, for local testing and unit tests, you can pass &#8220;local[*]&#8221; to run Spark Streaming
                    in-process (detects the number of cores in the local system).</p>

                <p>The batch interval must be set based on the latency requirements of your application
                    and available cluster resources. See the <a href="#setting-the-right-batch-interval">Performance Tuning</a>
                    section for more details.</p>
            </div>
        </div>

        <p>context对象创建后，你还需要如下步骤：</p>

        <ol>
            <li>创建DStream对象，并定义好输入数据源。</li>
            <li>基于数据源DStream定义好计算逻辑和输出。</li>
            <li>调用 <code>streamingContext.start()</code> 启动接收并处理数据。</li>
            <li>使用 <code>streamingContext.awaitTermination()</code> 等待处理被终止（手动或者由于任何错误）。</li>
            <li>你可以主动调用 <code>streamingContext.stop()</code> 来手动停止处理流程。</li>
        </ol>

        <h5 class="no_toc" id="points-to-remember">需要关注的重点:</h5>
        <ul>
            <li>一旦 streamingContext 启动，就不能再对其计算逻辑进行添加或修改</li>
            <li>一旦一个 streamingContext 已经停止，它不会被重新启动。</li>
            <li>同一时间内在 JVM 中只有一个 StreamingContext 可以被激活。</li>
            <li>在 StreamingContext 上的 <code>stop()</code> 同样也停止了 SparkContext。为了只停止 StreamingContext，设置 stop() 的可选参数，名叫 <code>stopSparkContext</code> 为 false。</li>
            <li>一个 SparkContext 就可以被重用以创建多个 StreamingContexts，只要前一个 StreamingContext 在下一个StreamingContext 被创建之前停止（不停止 SparkContext）。</li>
        </ul>

        <hr />

        <h2 id="discretized-streams-dstreams">DStreams</h2>
        <p>
            <strong>离散数据流（DStream）</strong> <strong></strong> 是 Spark Streaming 最基本的抽象。它代表了一种连续的数据流，要么从某种数据源提取数据，要么从其他数据流映射转换而来。在内部，DStream是由一系列连续的RDD组成的，每个RDD都是不可变、分布式的数据集。 (更多细节请看 <a href="rdd-programming-guide.html#resilient-distributed-datasets-rdds">Spark 编程指南</a>）每个RDD都包含了特定时间间隔内的一批数据，如下图所示：
        </p>

        <p style="text-align: center;">
            <img src="img/streaming-dstream.png" title="Spark Streaming data flow" alt="Spark Streaming" width="70%" />
        </p>

        <p>
            任何作用于 DStream 的算子，其实都会被转化为对其内部 RDD 的操作。例如，在 <a href="#a-quick-example">前面的例子中</a>，我们将 lines 这个DStream转成words DStream对象，其实作用于lines上的flatMap算子，会施加于lines中的每个RDD上，并生成新的对应的RDD，而这些新生成的RDD对象就组成了words这个DStream对象。其过程如下图所示：
        </p>

        <p style="text-align: center;">
            <img src="img/streaming-dstream-ops.png" title="Spark Streaming data flow" alt="Spark Streaming" width="70%" />
        </p>

        <p>
            底层的 RDD 转换仍然是由 Spark 引擎来计算。DStream 的算子将这些细节隐藏了起来，并为开发者提供了更为方便的高级API。后续会详细讨论这些高级算子。
        </p>

        <hr />

        <h2 id="input-dstreams-and-receivers">输入DStream和接收器</h2>
        <p>在 <a href="#a-quick-example">入门示例</a>中, <code>lines</code> 对象就是输入 DStream ，它代表着从 netcat 服务器接收到的数据的流。
            每个输入 DStream（除了文件数据流外，会在本章的后面来讨论）都和一个 <strong>Receiver</strong>
            (<a href="api/scala/index.html#org.apache.spark.streaming.receiver.Receiver">Scala doc</a>,
            <a href="api/java/org/apache/spark/streaming/receiver/Receiver.html">Java doc</a>) 关联，而接收器则是专门从数据源拉取数据到内存中的对象。</p>

        <p>Spark Streaming 提供了两种内置的流式数据源：

        </p>

        <ul>
            <li><em>基础数据源</em>: 在 StreamingContext API 中直接可以使用的数据源。例如：文件系统 和 套接字连接。</li>
            <li><em>高级数据源</em>: 需要依赖额外工具类的源，如： Kafka，Flume，Kinesis，等数据源。。像在  <a href="#linking">依赖</a> 中讨论的一样，这些数据源都需要增加额外的依赖。
            </li>
        </ul>

        <p>本节中，我们将会从每种数据源中挑几个继续深入讨论。
        </p>

        <p>注意，如果你需要同时从多个数据源拉取数据，那么你就需要创建多个DStream对象（在性能优化部分进一步讨论）。这将创建同时接收多个数据流的多个 receivers（接收器）。但需要注意，一个 Spark 的 worker/executor 是一个长期运行的任务（task），因此它将占用分配给 Spark Streaming 的应用程序的所有核中的一个核（core）。因此，要记住，一个 Spark Streaming 应用需要分配足够的核（core）（或线程（threads），如果本地运行的话）来处理所接收的数据，以及来运行接收器（receiver(s)）。</p>

        <h5 class="no_toc" id="points-to-remember-1">要记住的几点</h5>

        <ul>
            <li>
                <p>当在本地运行一个 Spark Streaming 程序的时候，不要使用 “local” 或者 “local[1]” 作为 master 的 URL。这两种方法中的任何一个都意味着只有一个线程将用于运行本地任务。如果你正在使用一个基于接收器（receiver）的输入离散流（input DStream）（例如，sockets，Kafka，Flume 等），则该单独的线程将用于运行接收器（receiver），而没有留下任何的线程用于处理接收到的数据。因此，在本地运行时，总是用 “local[n]” 作为 master URL，其中的 n > 运行接收器的数量
                    (查看 <a href="configuration.html#spark-properties">Spark Properties</a> 来了解怎样去设置 master 的信息）。
            </li>
            <li>
                <p>将Spark Streaming应用置于集群中运行时，同样，分配给该应用的CPU core数必须大于接收器的总数。否则，该应用就只会接收数据，而不会处理数据。</p>
            </li>
        </ul>

        <h3 class="no_toc" id="basic-sources">基础数据源</h3>

        <p>在 <a href="#a-quick-example">入门程序</a> 中 <code>ssc.socketTextStream(...)</code> 的例子，例子中是通过从一个 TCP socket 连接接收到的文本数据来创建了一个离散流（DStream）。除了 sockets 之外，StreamingContext API 也提供了根据文件作为输入来源创建离散流（DStreams）的方法。</p>

        <h4 class="no_toc" id="file-streams">文件数据流（File Streams）</h4>

        <p>
            用于从文件中读取数据，可以从任何兼容HDFS API（即，HDFS，S3，NFS 等）的文件系统，创建方式如下：
            <code>StreamingContext.fileStream[KeyClass, ValueClass, InputFormatClass]</code>
        </p>

        <p>文件流不需要运行接收器，因此不需要为接收文件数据分配任何内核。</p>

        <p>对于简单的文本文件，更简单的方式是调用 <code>StreamingContext.textFileStream(dataDirectory)</code>.</p>

        <div class="codetabs">
            <div data-lang="scala">

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">streamingContext</span><span class="o">.</span><span class="n">fileStream</span><span class="o">[</span><span class="kt">KeyClass</span>, <span class="kt">ValueClass</span>, <span class="kt">InputFormatClass</span><span class="o">](</span><span class="n">dataDirectory</span><span class="o">)</span></code></pre></figure>

                <p>For text files</p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">streamingContext</span><span class="o">.</span><span class="n">textFileStream</span><span class="o">(</span><span class="n">dataDirectory</span><span class="o">)</span></code></pre></figure>

            </div>

            <div data-lang="java">

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">streamingContext</span><span class="o">.</span><span class="na">fileStream</span><span class="o">&lt;</span><span class="n">KeyClass</span><span class="o">,</span> <span class="n">ValueClass</span><span class="o">,</span> <span class="n">InputFormatClass</span><span class="o">&gt;(</span><span class="n">dataDirectory</span><span class="o">);</span></code></pre></figure>

                <p>For text files</p>

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">streamingContext</span><span class="o">.</span><span class="na">textFileStream</span><span class="o">(</span><span class="n">dataDirectory</span><span class="o">);</span></code></pre></figure>

            </div>

            <div data-lang="python">
                <p><code>fileStream</code> is not available in the Python API; only <code>textFileStream</code> is available.</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">streamingContext</span><span class="o">.</span><span class="n">textFileStream</span><span class="p">(</span><span class="n">dataDirectory</span><span class="p">)</span></code></pre></figure>

            </div>

        </div>

        <h5 class="no_toc" id="how-directories-are-monitored">目录如何被监控(了解)</h5>

        <p>Spark Streaming将监视该 <code>dataDirectory</code> 目录，并处理该目录下任何新建的文件（目前还不支持嵌套目录）注意：</p>

        <ul>
            <li>各个文件数据格式必须一致。</li>
            <li>dataDirectory中的文件必须通过moving或者renaming来创建。</li>
            <li>一旦文件move进dataDirectory之后，就不能再改动。所以如果这个文件后续还有写入，这些新写入的数据不会被读取。</li>
        </ul>

        <h4 class="no_toc" id="streams-based-on-custom-receivers">基于自定义Actor的数据流（Streams based on Custom Actors）</h4>

        <p>
            DStream可以由Akka actor创建得到，只需调用 streamingContext.actorStream(actorProps, actor-name)。详见自定义接收器<a href="streaming-custom-receivers.html">（Custom Receiver Guide）</a>。actorStream暂时不支持Python API。

        </p>

        <h4 class="no_toc" id="queue-of-rdds-as-a-stream">RDD队列数据流（Queue of RDDs as a Stream）</h4>

        <p>如果需要测试Spark Streaming应用，你可以创建一个基于一批RDD的DStream对象，只需调用 <code>streamingContext.queueStream(queueOfRDDs)</code>。RDD会被一个个依次推入队列，而DStream则会依次以数据流形式处理这些RDD的数据。</p>

        <p>关于套接字、文件以及Akka actor数据流更详细信息，请参考相关文档：
            <a href="api/scala/index.html#org.apache.spark.streaming.StreamingContext">StreamingContext</a> for
            Scala, <a href="api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html">JavaStreamingContext</a>
            for Java, and <a href="api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext">StreamingContext</a> for Python.</p>

        <h3 class="no_toc" id="advanced-sources">高级数据源（重点）</h3>

        <p><span class="badge" style="background-color: grey">Python API</span> 从 Spark 2.4.3 开始,
            在 Python API 中的 Kafka，Kinesis 和 Flume 这样的外部数据源都是可用的。</p>

        <p>这一类别的数据源需要使用非 Spark 库中的外部接口, 需要单独引入依赖。</p>

        <p>注意: 基于这些高级数据源的应用程序不能在 spark shell 中被测试;如果你真的想要在 Spark shell 中使用它们，你必须下载带有它的依赖的相应的 Maven 组件的 JAR，并且将其添加到 classpath。</p>

        <p>一些高级的数据源如下：</p>

        <ul>
            <li>
                <p><strong>Kafka:</strong> Spark Streaming 2.4.3 与 Kafka broker 0.8.2.1 版本或者更高是兼容的。更多细节参阅 <a href="streaming-kafka-integration.html">Kafka 集成指南 Guide</a>。</p>
            </li>
            <li>
                <p><strong>Flume:</strong> Spark Streaming 2.4.3 与 Flume 1.6.0 相兼容。更多细节请参阅 <a href="streaming-flume-integration.html">Flume 集成指南</a>。</p>
            </li>
            <li>
                <p><strong>Kinesis:</strong> Spark Streaming 2.4.3 与 Kinesis Client Library 1.2.1 相兼容。更多细节请参阅<a href="streaming-kinesis-integration.html">Kinesis 集成指南</a>。</p>
            </li>
        </ul>

        <h3 class="no_toc" id="custom-sources">自定义 Sources（数据源）</h3>

        <p><span class="badge" style="background-color: grey">Python API</span> 自定义数据源目前还不支持Python。</p>

        <p>输入DStream也可以用自定义的方式创建。你需要做的只是实现一个自定义的接收器（receiver），以便从自定义的数据源接收数据，然后将数据推入Spark中。详情请参考自定义接收器指南<a href="streaming-custom-receivers.html">Custom Receiver
                Guide</a> 。</p>

        <h3 class="no_toc" id="receiver-reliability">接收器的可靠性</h3>

        <p>从可靠性角度来划分，大致有两种数据源。其中，像Kafka、Flume这样的数据源，它们支持对所传输的数据进行确认。系统收到这类可靠数据源过来的数据，然后发出确认信息，这样就能够确保任何失败情况下，都不会丢数据。因此我们可以将接收器也相应地分为两类：</p>

        <ol>
            <li><em>可靠接收器（Reliable Receiver）</em> - 可靠接收器会在成功接收并保存好Spark数据副本后，向可靠数据源发送确认信息。</li>
            <li><em>不可靠接收器（Unreliable Receiver）</em> - 不可靠接收器不会发送任何确认信息。不过这种接收器常用语于不支持确认的数据源，或者不想引入数据确认的复杂性的数据源。</li>
        </ol>

        <p>自定义接收器指南
            <a href="streaming-custom-receivers.html">Custom Receiver Guide</a>中详细讨论了如何写一个可靠接收器。</p>

        <hr />

        <h2 id="transformations-on-dstreams">DStream支持的transformation算子</h2>
        <p>
            和RDD类似，DStream也支持从输入DStream经过各种transformation算子映射成新的DStream。DStream支持很多RDD上常见的transformation算子，一些常用的见下表：
        </p>

        <table class="table">
            <tr><th style="width:25%">转换</th><th>含义</th></tr>
            <tr>
                <td> <b>map</b>(<i>func</i>) </td>
                <td> 利用函数 func 处理原 DStream 的每个元素，返回一个新的 DStream。
                </td>
            </tr>
            <tr>
                <td> <b>flatMap</b>(<i>func</i>) </td>
                <td> 和map类似，不过每个输入元素不再是映射为一个输出，而是映射为0到多个输出
                </td>
            </tr>
            <tr>
                <td> <b>filter</b>(<i>func</i>) </td>
                <td> 返回一个新的 DStream，它仅仅包含原 DStream 中函数 func 返回值为 true 的元素。
                </td>
            </tr>
            <tr>
                <td> <b>repartition</b>(<i>numPartitions</i>) </td>
                <td> 更改DStream的并行度（增加或减少每个 RDD 的分区数）
                </td>
            </tr>
            <tr>
                <td> <b>union</b>(<i>otherStream</i>) </td>
                <td> 返回新的DStream，包含源DStream和otherDStream元素的并集。
                </td>
            </tr>
            <tr>
                <td> <b>count</b>() </td>
                <td>通过计算源 DStream 的每个 batch（或RDD） 中的元素数量，返回单元素RDD的新DStream。
                </td>
            </tr>
            <tr>
                <td> <b>reduce</b>(<i>func</i>) </td>
                <td> 返回一个包含单元素RDDs的DStream，其中每个元素是通过源RDD中各个RDD的元素经func（func输入两个参数并返回一个同类型结果数据）聚合得到的结果。func必须满足结合律，以便支持并行计算。</td>
            </tr>
            <tr>
                <td> <b>countByValue</b>() </td>
                <td> 如果源DStream包含的元素类型为K，那么该算子返回新的DStream包含元素为(K, Long)键值对，其中K为源DStream各个元素，而Long为该元素出现的次数。 </td>
            </tr>
            <tr>
                <td> <b>reduceByKey</b>(<i>func</i>, [<i>numTasks</i>]) </td>
                <td> 如果源DStream 包含的元素为 (K, V) 键值对，则该算子返回一个新的也包含(K, V)键值对的DStream，其中V是由func聚合得到的。注意：默认情况下，该算子使用Spark的默认并发任务数（本地模式为2，集群模式下由spark.default.parallelism 决定）。你可以通过可选参数numTasks来指定并发任务个数。</td>
            </tr>
            <tr>
                <td> <b>join</b>(<i>otherStream</i>, [<i>numTasks</i>]) </td>
                <td> 如果源DStream包含元素为(K, V)，同时otherDStream包含元素为(K, W)键值对，则该算子返回一个新的DStream，其中源DStream和otherDStream中每个K都对应一个 (K, (V, W))键值对元素。 </td>
            </tr>
            <tr>
                <td> <b>cogroup</b>(<i>otherStream</i>, [<i>numTasks</i>]) </td>
                <td> 当应用于两个 DStream（一个包含（K,V）对，一个包含 (K,W) 对），返回一个包含 (K, Seq[V], Seq[W]) 的 tuples（元组）。
                </br>
                    有两个元组Tuple的集合A与B,先对A组集合中key相同的value进行聚合,然后对B组集合中key相同的value进行聚合,之后对A组与B组进行"join"操作;
                    </br><a>https://blog.csdn.net/huitoukest/article/details/51098134</a>
                </td>
            </tr>
            <tr>
                <td> <b>transform</b>(<i>func</i>) </td>
                <td> 返回一个新的DStream，其包含的RDD为源RDD经过func操作后得到的结果。利用该算子可以对DStream施加任意的操作。
                </td>
            </tr>
            <tr>
                <td> <b>updateStateByKey</b>(<i>func</i>) </td>
                <td>返回一个包含新”状态”的DStream。源DStream中每个key及其对应的values会作为func的输入，而func可以用于对每个key的“状态”数据作任意的更新操作。
                </td>
            </tr>
            <tr><td></td><td></td></tr>
        </table>

        <p>下面我们会挑几个transformation算子深入讨论一下。</p>

        <h4 class="no_toc" id="updatestatebykey-operation">UpdateStateByKey 算子（计算累加值）</h4>
        <p><code>updateStateByKey</code> 算子支持维护一个任意的状态。要实现这一点，只需要两步：</p>

        <ol>
            <li>定义状态 – 状态数据可以是任意类型。</li>
            <li>定义状态更新函数 – 定义好一个函数，其输入为数据流之前的状态和新的数据流数据，且可其更新步骤1中定义的输入数据流的状态。</li>
        </ol>

        <p>在每一个批次数据到达后，Spark都会调用状态更新函数，来更新所有已有key（不管key是否存在于本批次中）的状态。如果状态更新函数返回 <code>None</code>，则对应的键值对会被删除。 </p>

        <p>举例如下。假设你需要维护一个流式应用，统计数据流中每个单词的出现次数。这里将各个单词的出现次数（这个整型数)定义为状态。我们接下来定义状态更新函数如下：</p>

        <div class="codetabs">
            <div data-lang="scala">

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">def</span> <span class="n">updateFunction</span><span class="o">(</span><span class="n">currValues</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">Int</span><span class="o">],</span> <span class="n">prevValueState</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">Int</span><span class="o">])</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">currentCount</span> <span class="k">=</span> <span class="o">currValues.sum</span>  <span class="c1">// 对当前Key所有的value求和</span>
    <span class="k">val</span> <span class="n">previousCount</span> <span class="k">=</span> <span class="o">prevValueState.getOrElse(0)</span>  <span class="c1">// 获取当前Key历史value值</span>

    <span class="nc">Some</span><span class="o">(</span><span class="n">(currentCount + previousCount)</span><span class="o">)</span><span class="c1">// 将当前Key的value和与历史value求和；返回累加后的结果，是一个Option[Int]类型</span>
<span class="o">}</span></code></pre></figure>

                <p>该状态更新函数可以作用于一个包括(word, 1) 键值对的DStream上 <a href="#a-quick-example">见本文开头的小栗子</a>)。</p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">runningCounts</span> <span class="k">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">updateStateByKey</span><span class="o">[</span><span class="kt">Int</span><span class="o">](</span><span class="n">updateFunction</span> <span class="k">_</span><span class="o">)</span></code></pre></figure>

                <p>该状态更新函数会为每个单词调用一次，且相应的 currentCount 是一个包含很多个”1″的数组（这些1来自于(word,1)键值对），而runningCount包含之前该单词的计数。本例的完整代码请参考
                    <a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala">stateful_network_wordcount.scala</a>。</p>

            </div>
            <div data-lang="java">

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">Function2</span><span class="o">&lt;</span><span class="n">List</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;,</span> <span class="n">Optional</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;,</span> <span class="n">Optional</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="n">updateFunction</span> <span class="o">=</span>
  <span class="o">(</span><span class="n">values</span><span class="o">,</span> <span class="n">state</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="n">Integer</span> <span class="n">newSum</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1">// add the new values with the previous running count to get the new count</span>
    <span class="k">return</span> <span class="n">Optional</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="n">newSum</span><span class="o">);</span>
  <span class="o">};</span></code></pre></figure>

                <p>This is applied on a DStream containing words (say, the <code>pairs</code> DStream containing <code>(word,
                    1)</code> pairs in the <a href="#a-quick-example">quick example</a>).</p>

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="n">runningCounts</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="na">updateStateByKey</span><span class="o">(</span><span class="n">updateFunction</span><span class="o">);</span></code></pre></figure>

                <p>The update function will be called for each word, with <code>newValues</code> having a sequence of 1&#8217;s (from
                    the <code>(word, 1)</code> pairs) and the <code>runningCount</code> having the previous count. For the complete
                    Java code, take a look at the example
                    <a href="https://github.com/apache/spark/blob/v2.4.3/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java">JavaStatefulNetworkWordCount.java</a>.</p>

            </div>
            <div data-lang="python">

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">def</span> <span class="nf">updateFunction</span><span class="p">(</span><span class="n">newValues</span><span class="p">,</span> <span class="n">runningCount</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">runningCount</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">runningCount</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">newValues</span><span class="p">,</span> <span class="n">runningCount</span><span class="p">)</span>  <span class="c1"># add the new values with the previous running count to get the new count</span></code></pre></figure>

                <p>该状态更新函数可以作用于一个包括(word, 1) 键值对的DStream上 <a href="#a-quick-example">见本文开头的小栗子</a>)。</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">runningCounts</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">updateStateByKey</span><span class="p">(</span><span class="n">updateFunction</span><span class="p">)</span></code></pre></figure>

                <p>该状态更新函数会为每个单词调用一次，且相应的newValues是一个包含很多个”1″的数组（这些1来自于(word,1)键值对），而runningCount包含之前该单词的计数。本例的完整代码请参考
                    <a href="https://github.com/apache/spark/blob/v2.4.3/examples/src/main/python/streaming/stateful_network_wordcount.py">stateful_network_wordcount.py</a>。</p>

            </div>
        </div>

        <p><strong>注意: </strong>调用<code>updateStateByKey</code> 前需要配置检查点目录，后续对此有详细的讨论，见检查点 <a href="#checkpointing">checkpointing</a> 。</p>

        <p><strong>补充:</strong> <br>
            updateStateByKey作用：统计的数据是从最开始批次传入的数据开始统计。而不是统计每一批次的数据。它不能统计某一天或者某个小时的数据。<br>
            1、Spark Streaming中为每一个Key维护一份state状态，state类型可以是任意类型的的，可以是一个自定义的对象，那么更新函数也可以是自定义的。
            <br>
            2、通过更新函数对该key的状态不断更新，对于每个新的batch而言，SparkStreaming在使用updateStateByKey的时候早已经为已经存在的key进行state的状态更新（对于每个新出现的key，会同样的执行state的更新函数操作）<br>

            <strong>注意：</strong>如果要不断的更新每个key的state，就一定涉及到了状态的保存和容错，这个时候就需要开启checkpoint机制和功能 。
        </p>
        <h4 class="no_toc" id="transform-operation">transform算子</h4>
        <p>
            <code>transform</code> 算子（及其变体transformWith）将DStream 转换为一个个底层的RDD,从而实现将DStream中的RDD到其他类型RDD任意转换（如:将非K、V格式的RDD转换为K、V格式的RDD 那么最后返回的DStream中的RDD格式就是转换后的RDD的格式）。<br>
            也就是说，你可以用tranform算子来包装任何 DStream API 所不支持的 RDD 算子。
            例如，将 DStream 每个批次中的 RDD 和另一个 Dataset 进行关联（join）操作，这个功能DStream API并没有直接支持。
            不过你可以用 transform 来实现这个功能，可见 <code>transform</code> 其实为 DStream 提供了非常强大的功能支持。
            比如说，你可以用事先算好的垃圾信息，对 DStream 进行实时过滤。

        <p>补充: transform算子之内，获取到的RDD算子之外的代码是在Driver端执行的。transform算子之内，获取到的RDD算子之内的代码在Executor执行。</p>
        </p>

        <div class="codetabs">
            <div data-lang="scala">

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">spamInfoRDD</span> <span class="k">=</span> <span class="n">ssc</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">newAPIHadoopRDD</span><span class="o">(...)</span> <span class="c1">// RDD containing spam information</span>

<span class="k">val</span> <span class="n">cleanedDStream</span> <span class="k">=</span> <span class="n">wordCounts</span><span class="o">.</span><span class="n">transform</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span>
  <span class="n">rdd</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">spamInfoRDD</span><span class="o">).</span><span class="n">filter</span><span class="o">(...)</span> <span class="c1">// join data stream with spam information to do data cleaning</span>
  <span class="o">...</span>
<span class="o">}</span></code></pre></figure>

            </div>
            <div data-lang="java">

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.streaming.api.java.*</span><span class="o">;</span>
<span class="c1">// RDD containing spam information</span>
<span class="n">JavaPairRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Double</span><span class="o">&gt;</span> <span class="n">spamInfoRDD</span> <span class="o">=</span> <span class="n">jssc</span><span class="o">.</span><span class="na">sparkContext</span><span class="o">().</span><span class="na">newAPIHadoopRDD</span><span class="o">(...);</span>

<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="n">cleanedDStream</span> <span class="o">=</span> <span class="n">wordCounts</span><span class="o">.</span><span class="na">transform</span><span class="o">(</span><span class="n">rdd</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="n">rdd</span><span class="o">.</span><span class="na">join</span><span class="o">(</span><span class="n">spamInfoRDD</span><span class="o">).</span><span class="na">filter</span><span class="o">(...);</span> <span class="c1">// join data stream with spam information to do data cleaning</span>
  <span class="o">...</span>
<span class="o">});</span></code></pre></figure>

            </div>
            <div data-lang="python">

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">spamInfoRDD</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">pickleFile</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># RDD containing spam information</span>

<span class="c1"># join data stream with spam information to do data cleaning</span>
<span class="n">cleanedDStream</span> <span class="o">=</span> <span class="n">wordCounts</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="k">lambda</span> <span class="n">rdd</span><span class="p">:</span> <span class="n">rdd</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">spamInfoRDD</span><span class="p">)</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="o">...</span><span class="p">))</span></code></pre></figure>

            </div>
        </div>

        <p>注意，这里transform包含的算子，其调用时间间隔和批次间隔是相同的。所以你可以基于时间改变对RDD的操作，如：在不同批次，调用不同的RDD算子，设置不同的RDD分区或者广播变量等。</p>

        <h4 class="no_toc" id="window-operations">Window Operations （窗口操作）</h4>
        <p>Spark Streaming 同样也提供基于时间窗口的计算，也就是说，你可以对某一个滑动时间窗内的数据使用特定tranformation算子。如下图所示：</p>

        <p style="text-align: center;">
            <img src="img/streaming-dstream-window.png" title="Spark Streaming data flow" alt="Spark Streaming" width="60%" />
        </p>

        <p>如上图所示，每次窗口滑动时，源DStream中落入窗口的RDDs就会被合并成新的windowed DStream。在上图的例子中，这个操作会施加于3个RDD单元，而滑动距离是2个RDD单元。由此可以得出任何窗口相关操作都需要指定一下两个参数：</p>

        <ul>
            <li><i>（窗口长度）window length</i> - 窗口覆盖的时间长度（上图中为3）</li>
            <li><i>（滑动距离）sliding interval</i> - 窗口启动的时间间隔（上图中为2）</li>
        </ul>

        <p>这两个参数必须是 source DStream 的 batch interval（批间隔）的倍数（图 1）。<br>
            window length 长度 为20秒； sliding interval 时间间隔 10s，也就是说这个窗口函数式每隔10秒执行最近20秒的的数据。</p>

        <p>下面咱们举个栗子。假设，你需要扩展前面的那个小栗子，你需要每隔10秒统计一下前30秒内的单词计数。为此，我们需要在包含(word, 1)键值对的DStream上，对最近30秒的数据调用reduceByKey算子。不过这些都可以简单地用一个 reduceByKeyAndWindow搞定。</p>

        <div class="codetabs">
            <div data-lang="scala">

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="c1">// 每隔10秒聚合一次最近30秒的数据</span>
<span class="k">val</span> <span class="n">windowedWordCounts</span> <span class="k">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">reduceByKeyAndWindow</span><span class="o">((</span><span class="n">a</span><span class="k">:</span><span class="kt">Int</span><span class="o">,</span><span class="n">b</span><span class="k">:</span><span class="kt">Int</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">),</span> <span class="nc">Seconds</span><span class="o">(</span><span class="mi">30</span><span class="o">),</span> <span class="nc">Seconds</span><span class="o">(</span><span class="mi">10</span><span class="o">))</span></code></pre></figure>

            </div>
            <div data-lang="java">

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="c1">// Reduce last 30 seconds of data, every 10 seconds</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="n">windowedWordCounts</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="na">reduceByKeyAndWindow</span><span class="o">((</span><span class="n">i1</span><span class="o">,</span> <span class="n">i2</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">i1</span> <span class="o">+</span> <span class="n">i2</span><span class="o">,</span> <span class="n">Durations</span><span class="o">.</span><span class="na">seconds</span><span class="o">(</span><span class="mi">30</span><span class="o">),</span> <span class="n">Durations</span><span class="o">.</span><span class="na">seconds</span><span class="o">(</span><span class="mi">10</span><span class="o">));</span></code></pre></figure>

            </div>
            <div data-lang="python">

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="c1"># Reduce last 30 seconds of data, every 10 seconds</span>
<span class="n">windowedWordCounts</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">reduceByKeyAndWindow</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span></code></pre></figure>

            </div>
        </div>

        <p>以下列出了常用的窗口算子。所有这些算子都有前面提到的那两个参数 – <i>窗口长度</i> 和 <i>滑动距离</i>。</p>

        <table class="table">
            <tr><th style="width:25%">Transformation窗口算子</th><th>含义</th></tr>
            <tr>
                <td> <b>window</b>(<i>windowLength</i>, <i>slideInterval</i>) </td>
                <td> 将源DStream窗口化，并返回转化后的DStream
                </td>
            </tr>
            <tr>
                <td> <b>countByWindow</b>(<i>windowLength</i>, <i>slideInterval</i>) </td>
                <td> 返回数据流在一个滑动窗口内的元素个数
                </td>
            </tr>
            <tr>
                <td> <b>reduceByWindow</b>(<i>func</i>, <i>windowLength</i>, <i>slideInterval</i>) </td>
                <td> 基于数据流在一个滑动窗口内的元素，用func做聚合，返回一个单元素数据流。func必须满足结合律，以便支持并行计算。
                </td>
            </tr>
            <tr>
                <td> <b>reduceByKeyAndWindow</b>(<i>func</i>, <i>windowLength</i>, <i>slideInterval</i>,
                    [<i>numTasks</i>]) </td>
                <td> 基于(K, V)键值对DStream，将一个滑动窗口内的数据进行聚合，返回一个新的包含(K,V)键值对的DStream，其中每个value都是各个key经过func聚合后的结果。
                    注意：如果不指定numTasks，其值将使用Spark的默认并行任务数（本地模式下为2，集群模式下由 spark.default.parallelism决定）。当然，你也可以通过numTasks来指定任务个数。
                </td>
            </tr>
            <tr>
                <td> <b>reduceByKeyAndWindow</b>(<i>func</i>, <i>invFunc</i>, <i>windowLength</i>,
                    <i>slideInterval</i>, [<i>numTasks</i>]) </td>
                <td>
                    <p> 上述 <code>reduceByKeyAndWindow()</code> 的更有效的一个版本,避免数据的重复计算，提高计算效率。
                        只是这个版本会用之前滑动窗口计算结果，递增地计算每个窗口的归约结果。当新的数据进入窗口时，这些values会被输入func做归约计算，而这些数据离开窗口时，对应的这些values又会被输入 invFunc 做”反归约”计算。举个简单的例子，就是把新进入窗口数据中各个单词个数“增加”到各个单词统计结果上，同时把离开窗口数据中各个单词的统计个数从相应的统计结果中“减掉”。不过，你的自己定义好”反归约”函数，即：该算子不仅有归约函数（见参数func），还得有一个对应的”反归约”函数（见参数中的 invFunc）。和前面的reduceByKeyAndWindow() 类似，该算子也有一个可选参数numTasks来指定并行任务数。注意，这个算子需要配置好检查点（checkpointing）才能用。<br>
                        <br>
                        func(v1:Int,v2:Int):这里的v1是上一个窗口没有划出的数据的value和（也就是invFunc的返回值）；v2为本次的读取进来的值。<br>
                        invFunc(a:Int,b:Int):windowLength时间后才开始执行,v1是上面一个函数刚刚加上最近读取过来的key的value值的最新值, v2是窗口滑动后，滑动间隔中出去的那一批值,返回的值又是上面函数的v1 的输入值。
                    </p>
                </td>
            </tr>
            <tr>
                <td> <b>countByValueAndWindow</b>(<i>windowLength</i>,
                    <i>slideInterval</i>, [<i>numTasks</i>]) </td>
                <td> 对输入的每个元素当做一个value，返回新的包含(K, Long)键值对的DStream。其中的Long value都是滑动窗口内key出现次数的计数。
                    和前面的reduceByKeyAndWindow() 类似，该算子也有一个可选参数numTasks来指定并行任务数。
                </td>
            </tr>
            <tr><td></td><td></td></tr>
        </table>
         <p><strong>各个窗口算子详细使用:</strong> <a>https://blog.csdn.net/legotime/article/details/51836040</a> </p>
        <h4 class="no_toc" id="join-operations">Join相关算子</h4>
        <p>最后，值得一提的是，你在Spark Streaming中做各种关联（join）操作非常简单。</p>

        <h5 class="no_toc" id="stream-stream-joins">流-流（Stream-stream）关联</h5>
        <p>一个数据流可以和另一个数据流直接关联。</p>

        <div class="codetabs">
            <div data-lang="scala">

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">stream1</span><span class="k">:</span> <span class="kt">DStream</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">stream2</span><span class="k">:</span> <span class="kt">DStream</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">joinedStream</span> <span class="k">=</span> <span class="n">stream1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">stream2</span><span class="o">)</span></code></pre></figure>

            </div>
            <div data-lang="java">

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">stream1</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">stream2</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">joinedStream</span> <span class="o">=</span> <span class="n">stream1</span><span class="o">.</span><span class="na">join</span><span class="o">(</span><span class="n">stream2</span><span class="o">);</span></code></pre></figure>

            </div>
            <div data-lang="python">

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">stream1</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">stream2</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">joinedStream</span> <span class="o">=</span> <span class="n">stream1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">stream2</span><span class="p">)</span></code></pre></figure>

            </div>
        </div>
        <p> 上面代码中，<code>stream1</code> 的每个批次中的RDD会和 <code>stream2</code> 相应批次中的RDD进行join。同样，你可以类似地使用<code>leftOuterJoin</code>, <code>rightOuterJoin</code>, <code>fullOuterJoin</code>等。此外，你还可以基于窗口来join不同的数据流，其实现也很简单，如下：
        </p>

        <div class="codetabs">
            <div data-lang="scala">

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">windowedStream1</span> <span class="k">=</span> <span class="n">stream1</span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Seconds</span><span class="o">(</span><span class="mi">20</span><span class="o">))</span>
<span class="k">val</span> <span class="n">windowedStream2</span> <span class="k">=</span> <span class="n">stream2</span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Minutes</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
<span class="k">val</span> <span class="n">joinedStream</span> <span class="k">=</span> <span class="n">windowedStream1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">windowedStream2</span><span class="o">)</span></code></pre></figure>

            </div>
            <div data-lang="java">

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">windowedStream1</span> <span class="o">=</span> <span class="n">stream1</span><span class="o">.</span><span class="na">window</span><span class="o">(</span><span class="n">Durations</span><span class="o">.</span><span class="na">seconds</span><span class="o">(</span><span class="mi">20</span><span class="o">));</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">windowedStream2</span> <span class="o">=</span> <span class="n">stream2</span><span class="o">.</span><span class="na">window</span><span class="o">(</span><span class="n">Durations</span><span class="o">.</span><span class="na">minutes</span><span class="o">(</span><span class="mi">1</span><span class="o">));</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">joinedStream</span> <span class="o">=</span> <span class="n">windowedStream1</span><span class="o">.</span><span class="na">join</span><span class="o">(</span><span class="n">windowedStream2</span><span class="o">);</span></code></pre></figure>

            </div>
            <div data-lang="python">

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">windowedStream1</span> <span class="o">=</span> <span class="n">stream1</span><span class="o">.</span><span class="n">window</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="n">windowedStream2</span> <span class="o">=</span> <span class="n">stream2</span><span class="o">.</span><span class="n">window</span><span class="p">(</span><span class="mi">60</span><span class="p">)</span>
<span class="n">joinedStream</span> <span class="o">=</span> <span class="n">windowedStream1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">windowedStream2</span><span class="p">)</span></code></pre></figure>

            </div>
        </div>

        <h5 class="no_toc" id="stream-dataset-joins">流-数据集（stream-dataset）关联</h5>
        <p>其实这种情况已经在前面的 <code>DStream.transform</code> 算子中介绍过了，这里再举个基于滑动窗口的例子。</p>

        <div class="codetabs">
            <div data-lang="scala">

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">dataset</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">windowedStream</span> <span class="k">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Seconds</span><span class="o">(</span><span class="mi">20</span><span class="o">))...</span>
<span class="k">val</span> <span class="n">joinedStream</span> <span class="k">=</span> <span class="n">windowedStream</span><span class="o">.</span><span class="n">transform</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span> <span class="n">rdd</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">dataset</span><span class="o">)</span> <span class="o">}</span></code></pre></figure>

            </div>
            <div data-lang="java">

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">JavaPairRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">dataset</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">windowedStream</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="na">window</span><span class="o">(</span><span class="n">Durations</span><span class="o">.</span><span class="na">seconds</span><span class="o">(</span><span class="mi">20</span><span class="o">));</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">joinedStream</span> <span class="o">=</span> <span class="n">windowedStream</span><span class="o">.</span><span class="na">transform</span><span class="o">(</span><span class="n">rdd</span> <span class="o">-&gt;</span> <span class="n">rdd</span><span class="o">.</span><span class="na">join</span><span class="o">(</span><span class="n">dataset</span><span class="o">));</span></code></pre></figure>

            </div>
            <div data-lang="python">

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># some RDD</span>
<span class="n">windowedStream</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">window</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="n">joinedStream</span> <span class="o">=</span> <span class="n">windowedStream</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="k">lambda</span> <span class="n">rdd</span><span class="p">:</span> <span class="n">rdd</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span></code></pre></figure>

            </div>
        </div>

        <p>实际上，在上面代码里，你可以动态地更改 join 的数据集（dataset）。传给tranform算子的操作函数会在每个批次重新求值，所以每次该函数都会用最新的 <code>dataset></code> 值，所以不同批次间你可以改变 <code>dataset</code> 的值。</p>

        <p>完整的 DStream transformation 算子列表见API文档。Scala 请参考
             <a href="api/scala/index.html#org.apache.spark.streaming.dstream.DStream">DStream</a>
            和 <a href="api/scala/index.html#org.apache.spark.streaming.dstream.PairDStreamFunctions">PairDStreamFunctions</a>.
            Java 请参考 <a href="api/java/index.html?org/apache/spark/streaming/api/java/JavaDStream.html">JavaDStream</a>
            和 <a href="api/java/index.html?org/apache/spark/streaming/api/java/JavaPairDStream.html">JavaPairDStream</a>.
            Python 见 <a href="api/python/pyspark.streaming.html#pyspark.streaming.DStream">DStream</a>.</p>

        <hr />

        <h2 id="output-operations-on-dstreams">DStream Action 算子 </h2>
        <p>输出算子可以将DStream的数据推送到外部系统，如：数据库或者文件系统。因为输出算子会将最终完成转换的数据输出到外部系统，因此只有输出算子调用时，才会真正触发DStream transformation算子的真正执行（这一点类似于RDD 的action算子）。目前所支持的输出算子如下表：</p>

        <table class="table">
            <tr><th style="width:30%">输出算子</th><th>含义</th></tr>
            <tr>
                <td> <b>print</b>()</td>
                <td> 在运行流应用程序的 driver 节点上的DStream中打印每批数据的前十个元素。这对于开发和调试很有用。
                    <br />
                    <span class="badge" style="background-color: grey">Python API</span> 在 Python API 中调用
                    <b>pprint()</b>
                </td>
            </tr>
            <tr>
                <td> <b>saveAsTextFiles</b>(<i>prefix</i>, [<i>suffix</i>]) </td>
                <td>将 DStream 的内容保存到文本文件。每个批次一个文件，各文件命名规则为 “<i>prefix-TIME_IN_MS[.suffix]</i>”

                </td>
            </tr>
            <tr>
                <td> <b>saveAsObjectFiles</b>(<i>prefix</i>, [<i>suffix</i>]) </td>
                <td> 将DStream内容以序列化Java对象的形式保存到顺序文件中。
                    每个批次一个文件，各文件命名规则为 “prefix-TIME_IN_MS[.suffix]”Python API 暂不支持Python
                </td>
            </tr>
            <tr>
                <td> <b>saveAsHadoopFiles</b>(<i>prefix</i>, [<i>suffix</i>]) </td>
                <td> 将DStream内容保存到Hadoop文件中。
                    每个批次一个文件，各文件命名规则为 “prefix-TIME_IN_MS[.suffix]”Python API 暂不支持Python
                </td>
            </tr>
            <tr>
                <td> <b>foreachRDD</b>(<i>func</i>) </td>
                <td> 这是最通用的输出算子了，该算子接收一个函数func，func将作用于DStream的每个RDD上。
                    func应该实现将每个RDD的数据推到外部系统中，比如：保存到文件或者写到数据库中。
                    注意，func函数是在streaming应用的驱动器进程中执行的，所以如果其中包含RDD的action算子，就会触发对DStream中RDDs的实际计算过程。</td>
            </tr>
            <tr><td></td><td></td></tr>
        </table>

        <h3 class="no_toc" id="design-patterns-for-using-foreachrdd">使用foreachRDD的设计模式</h3>
        <p><code>dstream.foreachRDD</code> 是一个非常强大的原生工具函数，<strong>用户可以基于此算子将DStream数据推送到外部系统中</strong>。不过用户需要了解如何正确而高效地使用这个工具。以下列举了一些常见的错误。</p>

        <p>通常，对外部系统写入数据需要一些连接对象（如：远程server的TCP连接），以便发送数据给远程系统。因此，开发人员可能会不经意地在Spark驱动器（driver）进程中创建一个连接对象，然后又试图在Spark worker节点上使用这个连接。如下例所示：</p>

        <div class="codetabs">
            <div data-lang="scala">

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">dstream</span><span class="o">.</span><span class="n">foreachRDD</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span>
  <span class="k">val</span> <span class="n">connection</span> <span class="k">=</span> <span class="n">createNewConnection</span><span class="o">()</span>  <span class="c1">// 这行在驱动器（driver）进程执行</span>
  <span class="n">rdd</span><span class="o">.</span><span class="n">foreach</span> <span class="o">{</span> <span class="n">record</span> <span class="k">=&gt;</span>
    <span class="n">connection</span><span class="o">.</span><span class="n">send</span><span class="o">(</span><span class="n">record</span><span class="o">)</span> <span class="c1">// 而这行将在worker节点上执行</span>
  <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

            </div>
            <div data-lang="java">

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">dstream</span><span class="o">.</span><span class="na">foreachRDD</span><span class="o">(</span><span class="n">rdd</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="n">Connection</span> <span class="n">connection</span> <span class="o">=</span> <span class="n">createNewConnection</span><span class="o">();</span> <span class="c1">// executed at the driver</span>
  <span class="n">rdd</span><span class="o">.</span><span class="na">foreach</span><span class="o">(</span><span class="n">record</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="n">connection</span><span class="o">.</span><span class="na">send</span><span class="o">(</span><span class="n">record</span><span class="o">);</span> <span class="c1">// executed at the worker</span>
  <span class="o">});</span>
<span class="o">});</span></code></pre></figure>

            </div>
            <div data-lang="python">

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">def</span> <span class="nf">sendRecord</span><span class="p">(</span><span class="n">rdd</span><span class="p">):</span>
    <span class="n">connection</span> <span class="o">=</span> <span class="n">createNewConnection</span><span class="p">()</span>  <span class="c1"># executed at the driver</span>
    <span class="n">rdd</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="k">lambda</span> <span class="n">record</span><span class="p">:</span> <span class="n">connection</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">record</span><span class="p">))</span>
    <span class="n">connection</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">dstream</span><span class="o">.</span><span class="n">foreachRDD</span><span class="p">(</span><span class="n">sendRecord</span><span class="p">)</span></code></pre></figure>

            </div>
        </div>

        <p>这段代码是错误的，因为它需要把连接对象序列化，再从驱动器节点发送到worker节点。而这些连接对象通常都是不能跨节点（机器）传递的。比如，<strong>连接对象通常都不能序列化</strong>，或者在另一个进程中反序列化后再次初始化（连接对象通常都需要初始化，因此从驱动节点发到worker节点后可能需要重新初始化）等。解决此类错误的办法就是在worker节点上创建连接对象。</p>

        <p>然而，有些开发人员可能会走到另一个极端 – 为每条记录都创建一个连接对象，例如：

        </p>

        <div class="codetabs">
            <div data-lang="scala">

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">dstream</span><span class="o">.</span><span class="n">foreachRDD</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span>
  <span class="n">rdd</span><span class="o">.</span><span class="n">foreach</span> <span class="o">{</span> <span class="n">record</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="n">connection</span> <span class="k">=</span> <span class="n">createNewConnection</span><span class="o">()</span>
    <span class="n">connection</span><span class="o">.</span><span class="n">send</span><span class="o">(</span><span class="n">record</span><span class="o">)</span>
    <span class="n">connection</span><span class="o">.</span><span class="n">close</span><span class="o">()</span>
  <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

            </div>
            <div data-lang="java">

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">dstream</span><span class="o">.</span><span class="na">foreachRDD</span><span class="o">(</span><span class="n">rdd</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="n">rdd</span><span class="o">.</span><span class="na">foreach</span><span class="o">(</span><span class="n">record</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="n">Connection</span> <span class="n">connection</span> <span class="o">=</span> <span class="n">createNewConnection</span><span class="o">();</span>
    <span class="n">connection</span><span class="o">.</span><span class="na">send</span><span class="o">(</span><span class="n">record</span><span class="o">);</span>
    <span class="n">connection</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
  <span class="o">});</span>
<span class="o">});</span></code></pre></figure>

            </div>
            <div data-lang="python">

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">def</span> <span class="nf">sendRecord</span><span class="p">(</span><span class="n">record</span><span class="p">):</span>
    <span class="n">connection</span> <span class="o">=</span> <span class="n">createNewConnection</span><span class="p">()</span>
    <span class="n">connection</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
    <span class="n">connection</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">dstream</span><span class="o">.</span><span class="n">foreachRDD</span><span class="p">(</span><span class="k">lambda</span> <span class="n">rdd</span><span class="p">:</span> <span class="n">rdd</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">sendRecord</span><span class="p">))</span></code></pre></figure>

            </div>
        </div>

        <p>一般来说，连接对象是有时间和资源开销限制的。因此，对每条记录都进行一次连接对象的创建和销毁会增加很多不必要的开销，同时也大大减小了系统的吞吐量。一个比较好的解决方案是使用 rdd.foreachPartition – 为RDD的每个分区创建一个单独的连接对象，示例如下：</p>

        <div class="codetabs">
            <div data-lang="scala">

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">dstream</span><span class="o">.</span><span class="n">foreachRDD</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span>
  <span class="n">rdd</span><span class="o">.</span><span class="n">foreachPartition</span> <span class="o">{</span> <span class="n">partitionOfRecords</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="n">connection</span> <span class="k">=</span> <span class="n">createNewConnection</span><span class="o">()</span>
    <span class="n">partitionOfRecords</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">record</span> <span class="k">=&gt;</span> <span class="n">connection</span><span class="o">.</span><span class="n">send</span><span class="o">(</span><span class="n">record</span><span class="o">))</span>
    <span class="n">connection</span><span class="o">.</span><span class="n">close</span><span class="o">()</span>
  <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

            </div>
            <div data-lang="java">

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">dstream</span><span class="o">.</span><span class="na">foreachRDD</span><span class="o">(</span><span class="n">rdd</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="n">rdd</span><span class="o">.</span><span class="na">foreachPartition</span><span class="o">(</span><span class="n">partitionOfRecords</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="n">Connection</span> <span class="n">connection</span> <span class="o">=</span> <span class="n">createNewConnection</span><span class="o">();</span>
    <span class="k">while</span> <span class="o">(</span><span class="n">partitionOfRecords</span><span class="o">.</span><span class="na">hasNext</span><span class="o">())</span> <span class="o">{</span>
      <span class="n">connection</span><span class="o">.</span><span class="na">send</span><span class="o">(</span><span class="n">partitionOfRecords</span><span class="o">.</span><span class="na">next</span><span class="o">());</span>
    <span class="o">}</span>
    <span class="n">connection</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
  <span class="o">});</span>
<span class="o">});</span></code></pre></figure>

            </div>
            <div data-lang="python">

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">def</span> <span class="nf">sendPartition</span><span class="p">(</span><span class="nb">iter</span><span class="p">):</span>
    <span class="n">connection</span> <span class="o">=</span> <span class="n">createNewConnection</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="nb">iter</span><span class="p">:</span>
        <span class="n">connection</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
    <span class="n">connection</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">dstream</span><span class="o">.</span><span class="n">foreachRDD</span><span class="p">(</span><span class="k">lambda</span> <span class="n">rdd</span><span class="p">:</span> <span class="n">rdd</span><span class="o">.</span><span class="n">foreachPartition</span><span class="p">(</span><span class="n">sendPartition</span><span class="p">))</span></code></pre></figure>

            </div>
        </div>

        <p>这样一来，连接对象的创建开销就摊到很多条记录上了。</p>

        <p>最后，还有一个更优化的办法，就是在多个RDD批次之间复用连接对象。开发者可以维护一个静态连接池来保存连接对象，以便在不同批次的多个RDD之间共享同一组连接对象，示例如下：</p>

        <div class="codetabs">
            <div data-lang="scala">

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">dstream</span><span class="o">.</span><span class="n">foreachRDD</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span>
  <span class="n">rdd</span><span class="o">.</span><span class="n">foreachPartition</span> <span class="o">{</span> <span class="n">partitionOfRecords</span> <span class="k">=&gt;</span>
    <span class="c1">// ConnectionPool 是一个静态的、懒惰初始化的连接池</span>
    <span class="k">val</span> <span class="n">connection</span> <span class="k">=</span> <span class="nc">ConnectionPool</span><span class="o">.</span><span class="n">getConnection</span><span class="o">()</span>
    <span class="n">partitionOfRecords</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">record</span> <span class="k">=&gt;</span> <span class="n">connection</span><span class="o">.</span><span class="n">send</span><span class="o">(</span><span class="n">record</span><span class="o">))</span>
    <span class="nc">ConnectionPool</span><span class="o">.</span><span class="n">returnConnection</span><span class="o">(</span><span class="n">connection</span><span class="o">)</span>  <span class="c1"> // 将连接返还给连接池，以便后续复用</span>
  <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

            </div>

            <div data-lang="java">

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">dstream</span><span class="o">.</span><span class="na">foreachRDD</span><span class="o">(</span><span class="n">rdd</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="n">rdd</span><span class="o">.</span><span class="na">foreachPartition</span><span class="o">(</span><span class="n">partitionOfRecords</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="c1">// ConnectionPool is a static, lazily initialized pool of connections</span>
    <span class="n">Connection</span> <span class="n">connection</span> <span class="o">=</span> <span class="n">ConnectionPool</span><span class="o">.</span><span class="na">getConnection</span><span class="o">();</span>
    <span class="k">while</span> <span class="o">(</span><span class="n">partitionOfRecords</span><span class="o">.</span><span class="na">hasNext</span><span class="o">())</span> <span class="o">{</span>
      <span class="n">connection</span><span class="o">.</span><span class="na">send</span><span class="o">(</span><span class="n">partitionOfRecords</span><span class="o">.</span><span class="na">next</span><span class="o">());</span>
    <span class="o">}</span>
    <span class="n">ConnectionPool</span><span class="o">.</span><span class="na">returnConnection</span><span class="o">(</span><span class="n">connection</span><span class="o">);</span> <span class="c1">// return to the pool for future reuse</span>
  <span class="o">});</span>
<span class="o">});</span></code></pre></figure>

            </div>
            <div data-lang="python">

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">def</span> <span class="nf">sendPartition</span><span class="p">(</span><span class="nb">iter</span><span class="p">):</span>
    <span class="c1"># ConnectionPool is a static, lazily initialized pool of connections</span>
    <span class="n">connection</span> <span class="o">=</span> <span class="n">ConnectionPool</span><span class="o">.</span><span class="n">getConnection</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="nb">iter</span><span class="p">:</span>
        <span class="n">connection</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
    <span class="c1"># return to the pool for future reuse</span>
    <span class="n">ConnectionPool</span><span class="o">.</span><span class="n">returnConnection</span><span class="p">(</span><span class="n">connection</span><span class="p">)</span>

<span class="n">dstream</span><span class="o">.</span><span class="n">foreachRDD</span><span class="p">(</span><span class="k">lambda</span> <span class="n">rdd</span><span class="p">:</span> <span class="n">rdd</span><span class="o">.</span><span class="n">foreachPartition</span><span class="p">(</span><span class="n">sendPartition</span><span class="p">))</span></code></pre></figure>

            </div>
        </div>

        <p> 注意，连接池中的连接应该是懒惰创建的，并且有确定的超时时间，超时后自动销毁。这个实现应该是目前发送数据最高效的实现方式。</p>

        <h5 class="no_toc" id="other-points-to-remember">其他要点:</h5>
        <ul>
            <li>
                <p> DStream的转化执行也是懒惰的，需要输出算子来触发，这一点和RDD的懒惰执行由action算子触发很类似。特别地，DStream输出算子中包含的RDD action算子会强制触发对所接收数据的处理。因此，如果你的Streaming应用中没有输出算子，或者你用了dstream.foreachRDD(func)却没有在func中调用RDD action算子，那么这个应用只会接收数据，而不会处理数据，接收到的数据最后只是被简单地丢弃掉了。</p>
            </li>
            <li>
                <p>默认地，输出算子只能一次执行一个，且按照它们在应用程序代码中定义的顺序执行；不能并行执行。</p>
            </li>
        </ul>

        <hr />
        <p><strong>补充说明: DStream中的transform和foreachRDD函数</strong> <br>
            　　DStream 的 API 不能满足使用的时候，我们可以使用这两个函数，将 Dstream 转换为 RDD，然后使用 RDD 中的算子进行操作。<br>
            <strong> 区别:</strong><br>
            transform：将DStream的操作转换为RDD的操作，调用该RDD的算子操作最终返回一个新的DStreamRDD(其RDD格式就是我们转换后的格式) <br>
            foreachRDD：作用和transform类似，将DStream的操作转换为RDD进行操作，区别：该api没有返回值，通常用它作为数据的发送输出等操作。
        </p>
        <p style="color: red"><strong>注意: 一个批次，DStream内部就只对应一个RDD，transform和foreachRDD API使用的过程中，不要考虑多个RDD的问题。网上好多解释不靠谱。</strong></p>
        <h2 id="dataframe-and-sql-operations">DataFrame 和 SQL 操作</h2>
        <p>在 Streaming 应用中可以调用<a href="sql-programming-guide.html">DataFrames and SQL</a> 来处理流式数据。开发者可以用通过 StreamingContext 中的 SparkContext 对象来创建一个 SparkSession，并且，此外，必须这样做，以便可以在 driver 故障时重被创建出来。同样，你还是可以使用懒惰创建的单例模式来实例化 SparkSession。如下面的代码所示，这里我们将最开始的那个
            <a href="#a-quick-example">入门示例</a>  做了一些修改，使用DataFrame和SQL来统计单词计数。其实就是，将每个RDD都转化成一个DataFrame，然后注册成临时表，再用SQL查询这些临时表。</p>

        <div class="codetabs">
            <div data-lang="scala">

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="cm">/** DataFrame operations inside your streaming program */</span>

<span class="k">val</span> <span class="n">words</span><span class="k">:</span> <span class="kt">DStream</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">words</span><span class="o">.</span><span class="n">foreachRDD</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span>

  <span class="c1">// 得到一个单例的 SparkSession</span>
  <span class="k">val</span> <span class="n">spark</span> <span class="k">=</span> <span class="nc">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">config</span><span class="o">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">getConf</span><span class="o">).</span><span class="n">getOrCreate</span><span class="o">()</span>
  <span class="k">import</span> <span class="nn">spark.implicits._</span>

  <span class="c1">// 将RDD[String] 转为 DataFrame</span>
  <span class="k">val</span> <span class="n">wordsDataFrame</span> <span class="k">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">toDF</span><span class="o">(</span><span class="s">&quot;word&quot;</span><span class="o">)</span>

  <span class="c1">// DataFrame注册为临时表</span>
  <span class="n">wordsDataFrame</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;words&quot;</span><span class="o">)</span>

  <span class="c1">// 再用SQL语句查询，并打印出来</span>
  <span class="k">val</span> <span class="n">wordCountsDataFrame</span> <span class="k">=</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;select word, count(*) as total from words group by word&quot;</span><span class="o">)</span>
  <span class="n">wordCountsDataFrame</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
<span class="o">}</span></code></pre></figure>

                <p>请参阅完整的 <a href="https://github.com/apache/spark/blob/v2.4.3/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala">source code</a>.</p>
            </div>
            <div data-lang="java">

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="cm">/** Java Bean class for converting RDD to DataFrame */</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">JavaRow</span> <span class="kd">implements</span> <span class="n">java</span><span class="o">.</span><span class="na">io</span><span class="o">.</span><span class="na">Serializable</span> <span class="o">{</span>
  <span class="kd">private</span> <span class="n">String</span> <span class="n">word</span><span class="o">;</span>

  <span class="kd">public</span> <span class="n">String</span> <span class="nf">getWord</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">word</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">setWord</span><span class="o">(</span><span class="n">String</span> <span class="n">word</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">this</span><span class="o">.</span><span class="na">word</span> <span class="o">=</span> <span class="n">word</span><span class="o">;</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="o">...</span>

<span class="cm">/** DataFrame operations inside your streaming program */</span>

<span class="n">JavaDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">words</span> <span class="o">=</span> <span class="o">...</span>

<span class="n">words</span><span class="o">.</span><span class="na">foreachRDD</span><span class="o">((</span><span class="n">rdd</span><span class="o">,</span> <span class="n">time</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="c1">// Get the singleton instance of SparkSession</span>
  <span class="n">SparkSession</span> <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="na">builder</span><span class="o">().</span><span class="na">config</span><span class="o">(</span><span class="n">rdd</span><span class="o">.</span><span class="na">sparkContext</span><span class="o">().</span><span class="na">getConf</span><span class="o">()).</span><span class="na">getOrCreate</span><span class="o">();</span>

  <span class="c1">// Convert RDD[String] to RDD[case class] to DataFrame</span>
  <span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">JavaRow</span><span class="o">&gt;</span> <span class="n">rowRDD</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="n">JavaRow</span> <span class="n">record</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JavaRow</span><span class="o">();</span>
    <span class="n">record</span><span class="o">.</span><span class="na">setWord</span><span class="o">(</span><span class="n">word</span><span class="o">);</span>
    <span class="k">return</span> <span class="n">record</span><span class="o">;</span>
  <span class="o">});</span>
  <span class="n">DataFrame</span> <span class="n">wordsDataFrame</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">createDataFrame</span><span class="o">(</span><span class="n">rowRDD</span><span class="o">,</span> <span class="n">JavaRow</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>

  <span class="c1">// Creates a temporary view using the DataFrame</span>
  <span class="n">wordsDataFrame</span><span class="o">.</span><span class="na">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;words&quot;</span><span class="o">);</span>

  <span class="c1">// Do word count on table using SQL and print it</span>
  <span class="n">DataFrame</span> <span class="n">wordCountsDataFrame</span> <span class="o">=</span>
    <span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;select word, count(*) as total from words group by word&quot;</span><span class="o">);</span>
  <span class="n">wordCountsDataFrame</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="o">});</span></code></pre></figure>

                <p>See the full <a href="https://github.com/apache/spark/blob/v2.4.3/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java">source code</a>.</p>
            </div>
            <div data-lang="python">

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="c1"># Lazily instantiated global instance of SparkSession</span>
<span class="k">def</span> <span class="nf">getSparkSessionInstance</span><span class="p">(</span><span class="n">sparkConf</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;sparkSessionSingletonInstance&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">()):</span>
        <span class="nb">globals</span><span class="p">()[</span><span class="s2">&quot;sparkSessionSingletonInstance&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">SparkSession</span> \
            <span class="o">.</span><span class="n">builder</span> \
            <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">sparkConf</span><span class="p">)</span> \
            <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="k">return</span> <span class="nb">globals</span><span class="p">()[</span><span class="s2">&quot;sparkSessionSingletonInstance&quot;</span><span class="p">]</span>

<span class="o">...</span>

<span class="c1"># DataFrame operations inside your streaming program</span>

<span class="n">words</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># DStream of strings</span>

<span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;========= </span><span class="si">%s</span><span class="s2"> =========&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">time</span><span class="p">))</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Get the singleton instance of SparkSession</span>
        <span class="n">spark</span> <span class="o">=</span> <span class="n">getSparkSessionInstance</span><span class="p">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">getConf</span><span class="p">())</span>

        <span class="c1"># Convert RDD[String] to RDD[Row] to DataFrame</span>
        <span class="n">rowRdd</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">Row</span><span class="p">(</span><span class="n">word</span><span class="o">=</span><span class="n">w</span><span class="p">))</span>
        <span class="n">wordsDataFrame</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rowRdd</span><span class="p">)</span>

        <span class="c1"># Creates a temporary view using the DataFrame</span>
        <span class="n">wordsDataFrame</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;words&quot;</span><span class="p">)</span>

        <span class="c1"># Do word count on table using SQL and print it</span>
        <span class="n">wordCountsDataFrame</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;select word, count(*) as total from words group by word&quot;</span><span class="p">)</span>
        <span class="n">wordCountsDataFrame</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">pass</span>

<span class="n">words</span><span class="o">.</span><span class="n">foreachRDD</span><span class="p">(</span><span class="n">process</span><span class="p">)</span></code></pre></figure>

                <p>See the full <a href="https://github.com/apache/spark/blob/v2.4.3/examples/src/main/python/streaming/sql_network_wordcount.py">source code</a>.</p>

            </div>
        </div>

        <p>线程里执行SQL查询（异步查询，即：执行SQL查询的线程和运行StreamingContext的线程不同）。不过这种情况下，你需要确保查询的时候 StreamingContext 没有把所需的数据丢弃掉，否则StreamingContext有可能已将老的RDD数据丢弃掉了，那么异步查询的SQL语句也可能无法得到查询结果。举个栗子，如果你需要查询上一个批次的数据，但是你的SQL查询可能要执行5分钟，那么你就需要StreamingContext至少保留最近5分钟的数据：streamingContext.remember(Minutes(5)) （这是Scala为例，其他语言差不多）</p>

        <p>更多DataFrame和SQL的文档见这里： <a href="sql-programming-guide.html">DataFrames and SQL</a>。</p>

        <hr />

        <h2 id="mllib-operations">MLlib 操作</h2>
        <p><a href="ml-guide.html">MLlib</a> 提供了很多机器学习算法。首先，你需要关注的是流式计算相关的机器学习算法 (如： <a href="mllib-linear-methods.html#streaming-linear-regression">Streaming Linear Regression</a>, <a href="mllib-clustering.html#streaming-k-means">Streaming KMeans</a>, 等。)
            ，这些流式算法可以在流式数据上一边学习训练模型，一边用最新的模型处理数据。除此以外，对更多的机器学习算法而言，你需要离线数据训练这些模型（即使用历史数据），然后将训练好的模型用于在线的流式数据。详见 <a href="ml-guide.html">MLlib</a> 。</p>
        <hr />

        <h2 id="caching--persistence">缓存 / 持久性</h2>
        <p>和 RDD 类似，DStream 也支持将数据持久化到内存中。只需要调用 DStream 的 <code>persist()</code>
            方法，该方法内部会自动调用 DStream 中每个 RDD 的 persist 方法进而将数据持久化到内存中。这对于可能需要计算很多次的 DStream 非常有用 (例如：对于同一个批数据调用多个算子)。
            对于基于滑动窗口的算子，如： <code>reduceByWindow</code> 和
            <code>reduceByKeyAndWindow</code> 或者有状态的算子，如： <code>updateStateByKey</code>, 数据持久化就更重要了。因此，滑动窗口算子产生的DStream对象默认会自动持久化到内存中（不需要开发者调用 <code>persist()</code>）。</p>

        <p>对于从网络接收数据的输入数据流（如：Kafka、Flume、socket等），默认的持久化级别会将数据持久化到两个不同的节点上互为备份副本，以便支持容错。</p>

        <p>注意，与RDD不同的是，DStream的默认持久化级别是将数据序列化到内存中。进一步的讨论见<a href="#memory-tuning">性能调优</a>这一小节。关于持久化级别（或者存储级别）的更详细说明见
            <a href="rdd-programming-guide.html#rdd-persistence">Spark编程指南</a>。</p>

        <hr />

        <h2 id="checkpointing"> 检查点 (Checkpointing)</h2>
        <p>一般来说Streaming 应用都需要7*24小时长期运行，所以必须对一些与业务逻辑无关的故障有很好的容错（如：系统故障、JVM崩溃等）。对于这些可能性，Spark Streaming 必须在检查点保存足够的信息到一些可容错的外部存储系统中，以便能够随时从故障中恢复回来。所以，检查点需要保存以下两种数据：</p>

        <ul>
            <li><em>元数据检查点（Metadata checkpointing）</em> - 保存流式计算逻辑的定义信息到外部可容错存储系统（如：HDFS）。主要用途是用于在故障后回复应用程序本身（后续详谈）。元数包括：
                <ul>
                    <li><em>Configuration</em> - 创建Streaming应用程序的配置信息。</li>
                    <li><em>DStream operations</em> - 定义流式处理逻辑的DStream操作信息。</li>
                    <li><em>Incomplete batches</em> - 已经排队但未处理完的批次信息。</li>
                </ul>
            </li>
            <li><em>数据检查点（Data checkpointing）</em> - 将生成的RDD保存到可靠的存储中。这对一些需要跨批次组合数据或者有状态的算子来说很有必要。在这种转换算子中，往往新生成的RDD是依赖于前几个批次的RDD，因此随着时间的推移，有可能产生很长的依赖链条。为了避免在恢复数据的时候需要恢复整个依赖链条上所有的数据，检查点需要周期性地保存一些中间RDD状态信息，以斩断无限制增长的依赖链条和恢复时间。</li>
        </ul>

        <p>总之，元数据检查点主要是为了恢复驱动器节点上的故障，而数据或RDD检查点是为了支持对有状态转换操作的恢复。</p>

        <h4 class="no_toc" id="when-to-enable-checkpointing">何时启用检查点</h4>

        <p>如果有以下情况出现，你就必须启用检查点了：</p>

        <ul>
            <li><em>使用了有状态的转换算子</em> - 不管是用了 updateStateByKey 还是用了 reduceByKeyAndWindow（有”反归约”函数的那个版本），你都必须配置检查点目录来周期性地保存RDD检查点。</li>
            <li><em>支持驱动器故障中恢复</em> - 这时候需要元数据检查点以便恢复流式处理的进度信息。</li>
        </ul>

        <p>注意，一些简单的流式应用，如果没有用到前面所说的有状态转换算子，则完全可以不开启检查点。不过这样的话，驱动器（driver）故障恢复后，有可能会丢失部分数据（有些已经接收但还未处理的数据可能会丢失）。不过通常这点丢失时可接受的，很多Spark Streaming应用也是这样运行的。对非Hadoop环境的支持未来还会继续改进。</p>

        <h4 class="no_toc" id="how-to-configure-checkpointing">如何配置检查点</h4>

        <p>检查点的启用，只需要设置好保存检查点信息的检查点目录即可，一般会会将这个目录设为一些可容错的、可靠性较高的文件系统（如：HDFS、S3等）。开发者只需要调用 streamingContext.checkpoint(checkpointDirectory)。设置好检查点，你就可以使用前面提到的有状态转换算子了。另外，如果你需要你的应用能够支持从驱动器故障中恢复，你可能需要重写部分代码，实现以下行为：</p>

        <ul>
            <li>如果程序是首次启动，就需要 new 一个新的 StreamingContext，并定义好所有的数据流处理，然后调用 StreamingContext.start()。</li>
            <li>如果程序是故障后重启，就需要从检查点目录中的数据中重新构建 StreamingContext 对象。</li>
        </ul>

        <div class="codetabs">
            <div data-lang="scala">

                <p>不过这个行为可以用StreamingContext.getOrCreate来实现，示例如下：
                </p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="c1">// 首次创建StreamingContext并定义好数据流处理逻辑</span>
<span class="k">def</span> <span class="n">functionToCreateContext</span><span class="o">()</span><span class="k">:</span> <span class="kt">StreamingContext</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">ssc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StreamingContext</span><span class="o">(...)</span>   <span class="c1">// 新建一个StreamingContext对象</span>
  <span class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">ssc</span><span class="o">.</span><span class="n">socketTextStream</span><span class="o">(...)</span> <span class="c1">// 创建DStreams</span>
  <span class="o">...</span>
  <span class="n">ssc</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">(</span><span class="n">checkpointDirectory</span><span class="o">)</span>   <span class="c1"> // 设置好检查点目录</span>
  <span class="n">ssc</span>
<span class="o">}</span>

<span class="c1">// 创建新的StreamingContext对象，或者从检查点构造一个</span>
<span class="k">val</span> <span class="n">context</span> <span class="k">=</span> <span class="nc">StreamingContext</span><span class="o">.</span><span class="n">getOrCreate</span><span class="o">(</span><span class="n">checkpointDirectory</span><span class="o">,</span> <span class="n">functionToCreateContext</span> <span class="k">_</span><span class="o">)</span>

<span class="c1">// 无论是否是首次启动都需要设置的工作在这里</span>
<span class="n">context</span><span class="o">.</span> <span class="o">...</span>

<span class="c1">// 启动StreamingContext对象</span>
<span class="n">context</span><span class="o">.</span><span class="n">start</span><span class="o">()</span>
<span class="n">context</span><span class="o">.</span><span class="n">awaitTermination</span><span class="o">()</span></code></pre></figure>

                <p>如果 <code>checkpointDirectory</code> 目录存在，则context对象会从检查点数据重新构建出来。如果该目录不存在（如：首次运行），则<code>functionToCreateContext</code> 函数会被调用，创建一个新的StreamingContext对象并定义好DStream数据流。完整的示例请参见
                    <a href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala">RecoverableNetworkWordCount</a>这个例子会将网络数据中的单词计数统计结果添加到一个文件中。
                   </p>

            </div>
            <div data-lang="java">

                <p>This behavior is made simple by using <code>JavaStreamingContext.getOrCreate</code>. This is used as follows.</p>

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="c1">// Create a factory object that can create and setup a new JavaStreamingContext</span>
<span class="n">JavaStreamingContextFactory</span> <span class="n">contextFactory</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JavaStreamingContextFactory</span><span class="o">()</span> <span class="o">{</span>
  <span class="nd">@Override</span> <span class="kd">public</span> <span class="n">JavaStreamingContext</span> <span class="nf">create</span><span class="o">()</span> <span class="o">{</span>
    <span class="n">JavaStreamingContext</span> <span class="n">jssc</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JavaStreamingContext</span><span class="o">(...);</span>  <span class="c1">// new context</span>
    <span class="n">JavaDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">lines</span> <span class="o">=</span> <span class="n">jssc</span><span class="o">.</span><span class="na">socketTextStream</span><span class="o">(...);</span>     <span class="c1">// create DStreams</span>
    <span class="o">...</span>
    <span class="n">jssc</span><span class="o">.</span><span class="na">checkpoint</span><span class="o">(</span><span class="n">checkpointDirectory</span><span class="o">);</span>                       <span class="c1">// set checkpoint directory</span>
    <span class="k">return</span> <span class="n">jssc</span><span class="o">;</span>
  <span class="o">}</span>
<span class="o">};</span>

<span class="c1">// Get JavaStreamingContext from checkpoint data or create a new one</span>
<span class="n">JavaStreamingContext</span> <span class="n">context</span> <span class="o">=</span> <span class="n">JavaStreamingContext</span><span class="o">.</span><span class="na">getOrCreate</span><span class="o">(</span><span class="n">checkpointDirectory</span><span class="o">,</span> <span class="n">contextFactory</span><span class="o">);</span>

<span class="c1">// Do additional setup on context that needs to be done,</span>
<span class="c1">// irrespective of whether it is being started or restarted</span>
<span class="n">context</span><span class="o">.</span> <span class="o">...</span>

<span class="c1">// Start the context</span>
<span class="n">context</span><span class="o">.</span><span class="na">start</span><span class="o">();</span>
<span class="n">context</span><span class="o">.</span><span class="na">awaitTermination</span><span class="o">();</span></code></pre></figure>

                <p>If the <code>checkpointDirectory</code> exists, then the context will be recreated from the checkpoint data.
                    If the directory does not exist (i.e., running for the first time),
                    then the function <code>contextFactory</code> will be called to create a new
                    context and set up the DStreams. See the Java example
                    <a href="https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java">JavaRecoverableNetworkWordCount</a>.
                    This example appends the word counts of network data into a file.</p>

            </div>
            <div data-lang="python">

                <p>This behavior is made simple by using <code>StreamingContext.getOrCreate</code>. This is used as follows.</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="c1"># Function to create and setup a new StreamingContext</span>
<span class="k">def</span> <span class="nf">functionToCreateContext</span><span class="p">():</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># new context</span>
    <span class="n">ssc</span> <span class="o">=</span> <span class="n">StreamingContext</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="n">ssc</span><span class="o">.</span><span class="n">socketTextStream</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># create DStreams</span>
    <span class="o">...</span>
    <span class="n">ssc</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">(</span><span class="n">checkpointDirectory</span><span class="p">)</span>  <span class="c1"># set checkpoint directory</span>
    <span class="k">return</span> <span class="n">ssc</span>

<span class="c1"># Get StreamingContext from checkpoint data or create a new one</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">StreamingContext</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">(</span><span class="n">checkpointDirectory</span><span class="p">,</span> <span class="n">functionToCreateContext</span><span class="p">)</span>

<span class="c1"># Do additional setup on context that needs to be done,</span>
<span class="c1"># irrespective of whether it is being started or restarted</span>
<span class="n">context</span><span class="o">.</span> <span class="o">...</span>

<span class="c1"># Start the context</span>
<span class="n">context</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
<span class="n">context</span><span class="o">.</span><span class="n">awaitTermination</span><span class="p">()</span></code></pre></figure>

                <p>If the <code>checkpointDirectory</code> exists, then the context will be recreated from the checkpoint data.
                    If the directory does not exist (i.e., running for the first time),
                    then the function <code>functionToCreateContext</code> will be called to create a new
                    context and set up the DStreams. See the Python example
                    <a href="https://github.com/apache/spark/tree/master/examples/src/main/python/streaming/recoverable_network_wordcount.py">recoverable_network_wordcount.py</a>.
                    This example appends the word counts of network data into a file.</p>

                <p>You can also explicitly create a <code>StreamingContext</code> from the checkpoint data and start the
                    computation by using <code>StreamingContext.getOrCreate(checkpointDirectory, None)</code>.</p>

            </div>
        </div>

        <p>除了使用getOrCreate之外，开发者还需要确保驱动器进程能在故障后重启。这一点只能由应用的部署环境基础设施来保证。进一步的讨论见部署
            <a href="#deploying-applications">Deployment</a> 这一节。

        </p>

        <p>另外需要注意的是，RDD检查点会增加额外的保存数据的开销。这可能会导致数据流的处理时间变长。因此，你必须仔细的调整检查点间隔时间。如果批次间隔太小（比如：1秒），那么对每个批次保存检查点数据将大大减小吞吐量。另一方面，检查点保存过于频繁又会导致血统信息和任务个数的增加，这同样会影响系统性能。对于需要RDD检查点的有状态转换算子，默认的间隔是批次间隔的整数倍，且最小10秒。开发人员可以这样来自定义这个间隔：dstream.checkpoint(checkpointInterval)。一般推荐设为批次间隔时间的5~10倍。
        </p>

        <hr />

        <h2 id="accumulators-broadcast-variables-and-checkpoints">累加器、广播变量 和 检查点</h2>

        <p>首先需要注意的是，<a href="rdd-programming-guide.html#accumulators">Accumulators</a> 和 <a href="rdd-programming-guide.html#broadcast-variables">Broadcast variables</a>
            是无法从 Spark Streaming 的检查点中恢复回来的。所以如果你开启了检查点功能，并同时在使用 <a href="rdd-programming-guide.html#accumulators">Accumulators</a> 或 <a href="rdd-programming-guide.html#broadcast-variables">Broadcast variables</a>，那么你最好是使用懒惰实例化的单例模式，因为这样 <a href="rdd-programming-guide.html#accumulators">Accumulators</a> 和 <a href="rdd-programming-guide.html#broadcast-variables">Broadcast variables</a> 才能在驱动器（driver）故障恢复后重新实例化。代码示例如下：
        </p>

        <div class="codetabs">
            <div data-lang="scala">

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">object</span> <span class="nc">WordBlacklist</span> <span class="o">{</span>

  <span class="nd">@volatile</span> <span class="k">private</span> <span class="k">var</span> <span class="n">instance</span><span class="k">:</span> <span class="kt">Broadcast</span><span class="o">[</span><span class="kt">Seq</span><span class="o">[</span><span class="kt">String</span><span class="o">]]</span> <span class="k">=</span> <span class="kc">null</span>

  <span class="k">def</span> <span class="n">getInstance</span><span class="o">(</span><span class="n">sc</span><span class="k">:</span> <span class="kt">SparkContext</span><span class="o">)</span><span class="k">:</span> <span class="kt">Broadcast</span><span class="o">[</span><span class="kt">Seq</span><span class="o">[</span><span class="kt">String</span><span class="o">]]</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">instance</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">synchronized</span> <span class="o">{</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">instance</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
          <span class="k">val</span> <span class="n">wordBlacklist</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">&quot;a&quot;</span><span class="o">,</span> <span class="s">&quot;b&quot;</span><span class="o">,</span> <span class="s">&quot;c&quot;</span><span class="o">)</span>
          <span class="n">instance</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">broadcast</span><span class="o">(</span><span class="n">wordBlacklist</span><span class="o">)</span>
        <span class="o">}</span>
      <span class="o">}</span>
    <span class="o">}</span>
    <span class="n">instance</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="k">object</span> <span class="nc">DroppedWordsCounter</span> <span class="o">{</span>

  <span class="nd">@volatile</span> <span class="k">private</span> <span class="k">var</span> <span class="n">instance</span><span class="k">:</span> <span class="kt">LongAccumulator</span> <span class="o">=</span> <span class="kc">null</span>

  <span class="k">def</span> <span class="n">getInstance</span><span class="o">(</span><span class="n">sc</span><span class="k">:</span> <span class="kt">SparkContext</span><span class="o">)</span><span class="k">:</span> <span class="kt">LongAccumulator</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">instance</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">synchronized</span> <span class="o">{</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">instance</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
          <span class="n">instance</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">longAccumulator</span><span class="o">(</span><span class="s">&quot;WordsInBlacklistCounter&quot;</span><span class="o">)</span>
        <span class="o">}</span>
      <span class="o">}</span>
    <span class="o">}</span>
    <span class="n">instance</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="n">wordCounts</span><span class="o">.</span><span class="n">foreachRDD</span> <span class="o">{</span> <span class="o">(</span><span class="n">rdd</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)],</span> <span class="n">time</span><span class="k">:</span> <span class="kt">Time</span><span class="o">)</span> <span class="k">=&gt;</span>
  <span class="c1">// 获取现有或注册新的blacklist广播变量 Driver端执行</span>
  <span class="k">val</span> <span class="n">blacklist</span> <span class="k">=</span> <span class="nc">WordBlacklist</span><span class="o">.</span><span class="n">getInstance</span><span class="o">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">)</span>
  <span class="c1">// 获取现有或注册新的 droppedWordsCounter 累加器 Driver端执行</span>
  <span class="k">val</span> <span class="n">droppedWordsCounter</span> <span class="k">=</span> <span class="nc">DroppedWordsCounter</span><span class="o">.</span><span class="n">getInstance</span><span class="o">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">)</span>
  <span class="c1"> // 基于blacklist来过滤词，并将过滤掉的词的个数累加到 droppedWordsCounter 中 Executor端执行</span>
  <span class="k">val</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">filter</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">word</span><span class="o">,</span> <span class="n">count</span><span class="o">)</span> <span class="k">=&gt;</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">blacklist</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="n">word</span><span class="o">))</span> <span class="o">{</span>
      <span class="n">droppedWordsCounter</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">count</span><span class="o">)</span>
      <span class="kc">false</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
      <span class="kc">true</span>
    <span class="o">}</span>
  <span class="o">}.</span><span class="n">collect</span><span class="o">().</span><span class="n">mkString</span><span class="o">(</span><span class="s">&quot;[&quot;</span><span class="o">,</span> <span class="s">&quot;, &quot;</span><span class="o">,</span> <span class="s">&quot;]&quot;</span><span class="o">)</span>
  <span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="s">&quot;Counts at time &quot;</span> <span class="o">+</span> <span class="n">time</span> <span class="o">+</span> <span class="s">&quot; &quot;</span> <span class="o">+</span> <span class="n">counts</span>
<span class="o">})</span></code></pre></figure>

                <p>请参阅完整的<a href="https://github.com/apache/spark/blob/v2.4.3/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala">source code</a>。</p>
            </div>
            <div data-lang="java">

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kd">class</span> <span class="nc">JavaWordBlacklist</span> <span class="o">{</span>

  <span class="kd">private</span> <span class="kd">static</span> <span class="kd">volatile</span> <span class="n">Broadcast</span><span class="o">&lt;</span><span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">instance</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>

  <span class="kd">public</span> <span class="kd">static</span> <span class="n">Broadcast</span><span class="o">&lt;</span><span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;&gt;</span> <span class="nf">getInstance</span><span class="o">(</span><span class="n">JavaSparkContext</span> <span class="n">jsc</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">instance</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
      <span class="kd">synchronized</span> <span class="o">(</span><span class="n">JavaWordBlacklist</span><span class="o">.</span><span class="na">class</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">instance</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
          <span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">wordBlacklist</span> <span class="o">=</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="s">&quot;a&quot;</span><span class="o">,</span> <span class="s">&quot;b&quot;</span><span class="o">,</span> <span class="s">&quot;c&quot;</span><span class="o">);</span>
          <span class="n">instance</span> <span class="o">=</span> <span class="n">jsc</span><span class="o">.</span><span class="na">broadcast</span><span class="o">(</span><span class="n">wordBlacklist</span><span class="o">);</span>
        <span class="o">}</span>
      <span class="o">}</span>
    <span class="o">}</span>
    <span class="k">return</span> <span class="n">instance</span><span class="o">;</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="kd">class</span> <span class="nc">JavaDroppedWordsCounter</span> <span class="o">{</span>

  <span class="kd">private</span> <span class="kd">static</span> <span class="kd">volatile</span> <span class="n">LongAccumulator</span> <span class="n">instance</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>

  <span class="kd">public</span> <span class="kd">static</span> <span class="n">LongAccumulator</span> <span class="nf">getInstance</span><span class="o">(</span><span class="n">JavaSparkContext</span> <span class="n">jsc</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">instance</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
      <span class="kd">synchronized</span> <span class="o">(</span><span class="n">JavaDroppedWordsCounter</span><span class="o">.</span><span class="na">class</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">instance</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
          <span class="n">instance</span> <span class="o">=</span> <span class="n">jsc</span><span class="o">.</span><span class="na">sc</span><span class="o">().</span><span class="na">longAccumulator</span><span class="o">(</span><span class="s">&quot;WordsInBlacklistCounter&quot;</span><span class="o">);</span>
        <span class="o">}</span>
      <span class="o">}</span>
    <span class="o">}</span>
    <span class="k">return</span> <span class="n">instance</span><span class="o">;</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="n">wordCounts</span><span class="o">.</span><span class="na">foreachRDD</span><span class="o">((</span><span class="n">rdd</span><span class="o">,</span> <span class="n">time</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="c1">// Get or register the blacklist Broadcast</span>
  <span class="n">Broadcast</span><span class="o">&lt;</span><span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">blacklist</span> <span class="o">=</span> <span class="n">JavaWordBlacklist</span><span class="o">.</span><span class="na">getInstance</span><span class="o">(</span><span class="k">new</span> <span class="n">JavaSparkContext</span><span class="o">(</span><span class="n">rdd</span><span class="o">.</span><span class="na">context</span><span class="o">()));</span>
  <span class="c1">// Get or register the droppedWordsCounter Accumulator</span>
  <span class="n">LongAccumulator</span> <span class="n">droppedWordsCounter</span> <span class="o">=</span> <span class="n">JavaDroppedWordsCounter</span><span class="o">.</span><span class="na">getInstance</span><span class="o">(</span><span class="k">new</span> <span class="n">JavaSparkContext</span><span class="o">(</span><span class="n">rdd</span><span class="o">.</span><span class="na">context</span><span class="o">()));</span>
  <span class="c1">// Use blacklist to drop words and use droppedWordsCounter to count them</span>
  <span class="n">String</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="na">filter</span><span class="o">(</span><span class="n">wordCount</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">blacklist</span><span class="o">.</span><span class="na">value</span><span class="o">().</span><span class="na">contains</span><span class="o">(</span><span class="n">wordCount</span><span class="o">.</span><span class="na">_1</span><span class="o">()))</span> <span class="o">{</span>
      <span class="n">droppedWordsCounter</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">wordCount</span><span class="o">.</span><span class="na">_2</span><span class="o">());</span>
      <span class="k">return</span> <span class="kc">false</span><span class="o">;</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
      <span class="k">return</span> <span class="kc">true</span><span class="o">;</span>
    <span class="o">}</span>
  <span class="o">}).</span><span class="na">collect</span><span class="o">().</span><span class="na">toString</span><span class="o">();</span>
  <span class="n">String</span> <span class="n">output</span> <span class="o">=</span> <span class="s">&quot;Counts at time &quot;</span> <span class="o">+</span> <span class="n">time</span> <span class="o">+</span> <span class="s">&quot; &quot;</span> <span class="o">+</span> <span class="n">counts</span><span class="o">;</span>
<span class="o">}</span></code></pre></figure>

                <p>See the full <a href="https://github.com/apache/spark/blob/v2.4.3/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java">source code</a>.</p>
            </div>
            <div data-lang="python">

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">def</span> <span class="nf">getWordBlacklist</span><span class="p">(</span><span class="n">sparkContext</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;wordBlacklist&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">()):</span>
        <span class="nb">globals</span><span class="p">()[</span><span class="s2">&quot;wordBlacklist&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sparkContext</span><span class="o">.</span><span class="n">broadcast</span><span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">])</span>
    <span class="k">return</span> <span class="nb">globals</span><span class="p">()[</span><span class="s2">&quot;wordBlacklist&quot;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">getDroppedWordsCounter</span><span class="p">(</span><span class="n">sparkContext</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;droppedWordsCounter&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">()):</span>
        <span class="nb">globals</span><span class="p">()[</span><span class="s2">&quot;droppedWordsCounter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sparkContext</span><span class="o">.</span><span class="n">accumulator</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">globals</span><span class="p">()[</span><span class="s2">&quot;droppedWordsCounter&quot;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">echo</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
    <span class="c1"># Get or register the blacklist Broadcast</span>
    <span class="n">blacklist</span> <span class="o">=</span> <span class="n">getWordBlacklist</span><span class="p">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">context</span><span class="p">)</span>
    <span class="c1"># Get or register the droppedWordsCounter Accumulator</span>
    <span class="n">droppedWordsCounter</span> <span class="o">=</span> <span class="n">getDroppedWordsCounter</span><span class="p">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">context</span><span class="p">)</span>

    <span class="c1"># Use blacklist to drop words and use droppedWordsCounter to count them</span>
    <span class="k">def</span> <span class="nf">filterFunc</span><span class="p">(</span><span class="n">wordCount</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">wordCount</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">blacklist</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
            <span class="n">droppedWordsCounter</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">wordCount</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="bp">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">True</span>

    <span class="n">counts</span> <span class="o">=</span> <span class="s2">&quot;Counts at time </span><span class="si">%s</span><span class="s2"> </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">rdd</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">filterFunc</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>

<span class="n">wordCounts</span><span class="o">.</span><span class="n">foreachRDD</span><span class="p">(</span><span class="n">echo</span><span class="p">)</span></code></pre></figure>

                <p>See the full <a href="https://github.com/apache/spark/blob/v2.4.3/examples/src/main/python/streaming/recoverable_network_wordcount.py">source code</a>.</p>

            </div>
        </div>

        <hr />

        <h2 id="deploying-applications">部署应用</h2>
        <p>本节中将主要讨论一下如何部署Spark Streaming应用。</p>

        <h3 class="no_toc" id="requirements">前提条件</h3>

        <p>要运行一个Spark Streaming 应用，你首先需要具备以下条件：</p>

        <ul>
            <li>
                <p><em>集群以及集群管理器</em> - 这是任何 Spark 应用程序的一般要求，并在 <a href="cluster-overview.html">部署指南</a>中详细讨论。</p>
            </li>
            <li>
                <p><em>打包应用程序 JAR</em> - 您必须将 streaming 应用程序编译为 JAR。
                    如果您正在使用 <a href="submitting-applications.html"><code>spark-submit</code></a> t启动应用程序，则不需要在 JAR 中提供 Spark 和 Spark Streaming。但是，如果您的应用程序使用高级资源（例如：Kafka，Flume），那么您将必须将他们链接的额外工件及其依赖项打包在用于部署应用程序的 JAR 中。例如，使用
                   <code>KafkaUtils</code> 的应用程序必须在应用程序 JAR 中包含 <code>spark-streaming-kafka-0-10_2.12</code>  及其所有传递依赖关系。</p>
            </li>
            <li>
                <p><em>为 executor 配置足够的内存</em> - 执行器必须配置预留好足够的内存，因为接受到的数据都得存在内存里。注意，如果某些窗口长度达到10分钟，那也就是说你的系统必须知道保留10分钟的数据在内存里。可见，到底预留多少内存是取决于你的应用处理逻辑的。</p>
            </li>
            <li>
                <p><em>配置 checkpoint </em> - 如果你的流式应用需要检查点，那么你需要配置一个Hadoop API兼容的可容错存储目录作为检查点目录，流式应用的信息会写入这个目录，故障恢复时会用到这个目录下的数据。详见前面的<a href="#checkpointing">checkpointing</a>小节。
                </p>
            </li>
            <li><em>配置应用程序 driver 的自动重新启动 </em> - 要从 driver 故障自动恢复，用于运行流应用程序的部署基础架构必须监视 driver 进程，并在 driver 发生故障时重新启动 driver。不同的<a href="cluster-overview.html#cluster-manager-types">集群管理器</a>有不同的工具来实现这一点。
                <ul>
                    <li><em>Spark Standalone</em> - Spark独立部署集群 (<a href="spark-standalone.html#launching-spark-applications">cluster deploy mode</a>) 可以支持将Spark应用的驱动器（Driver）提交到集群的某个worker节点上运行。同时，Spark的集群管理器可以对该驱动器进程进行监控，一旦驱动器退出且返回非0值，或者因worker节点原始失败，Spark集群管理器将自动重启这个驱动器。
                        详见 <a href="spark-standalone.html">Spark独立部署指南</a>
                    </li>
                    <li><em>YARN</em> - YARN支持和独立部署类似的重启机制。详细请参考YARN的文档。</li>
                    <li><em>Mesos</em> - Mesos上需要用 <a href="https://github.com/mesosphere/marathon">Marathon</a> 来实现这一功能。</li>
                </ul>
            </li>
            <li>
                <p><em>配置预写日志 (WAL)</em> - 从Spark 1.2起，我们引入了write ahead log来提高容错性。如果启用这个功能，则所有接收到的数据都会以write ahead log形式写入配置好的检查点目录中。这样就能确保数据零丢失 (在 <a href="#fault-tolerance-semantics">容错语义</a> 部分中详细讨论)。用户只需将

                    <code>spark.streaming.receiver.writeAheadLog.enable</code> 设置为 <code>true</code>。 不过，这同样可能会导致接收器的吞吐量下降。不过你可以启动多个接收器并行接收数据，从而提升整体的吞吐量
                     <a href="#level-of-parallelism-in-data-receiving">more receivers in parallel</a>
                    另外，建议在启用WAL后禁用掉接收数据多副本功能，因为WAL其实已经是存储在一个多副本存储系统中了。你只需要把存储级别设为 StorageLevel.MEMORY_AND_DISK_SER。如果是使用S3（或者其他不支持flushing的文件系统）存储WAL，一定要记得启用这两个标识：spark.streaming.driver.writeAheadLog.closeFileAfterWrite 和 spark.streaming.receiver.writeAheadLog.closeFileAfterWrite。更详细请参考：
                    <a href="configuration.html#spark-streaming">Spark Streaming Configuration</a></p>
            </li>
            <li><em>设置最大接收速率</em> - 如果集群可用资源不足以跟上接收数据的速度，那么可以在接收器设置一下最大接收速率，即：每秒接收记录的条数。相关的主要配置有：spark.streaming.receiver.maxRate，如果使用Kafka Direct API 还需要设置 spark.streaming.kafka.maxRatePerPartition。从Spark 1.5起，我们引入了backpressure的概念来动态地根据集群处理速度，评估并调整该接收速率。用户只需将<a href="configuration.html#spark-streaming">配置参数</a> spark.streaming.backpressure.enabled设为true即可启用该功能。
              </li>
        </ul>

        <h3 class="no_toc" id="upgrading-application-code">升级应用程序代码</h3>

        <p>升级 Spark Streaming 应用程序代码，可以使用以下两种方式：

        </p>

        <ul>
            <li>
                <p>新的 Streaming 程序和老的并行跑一段时间，新程序完成初始化以后，再关闭老的。注意，这种方式适用于能同时发送数据到多个目标的数据源（即：数据源同时将数据发给新老两个 Streaming 应用程序）。</p>
            </li>
            <li>
                <p>老程序能够优雅地退出(参考
                    <a href="api/scala/index.html#org.apache.spark.streaming.StreamingContext"><code>StreamingContext.stop(...)</code></a>
                    或 <a href="api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html"><code>JavaStreamingContext.stop(...)</code></a>)
                    即：确保所收到的数据都已经处理完毕后再退出。然后再启动新的Streaming程序，而新程序将接着在老程序退出点上继续拉取数据。注意，这种方式需要数据源支持数据缓存（或者叫数据堆积，如：Kafka、Flume），因为在新旧程序交接的这个空档时间，数据需要在数据源处缓存。目前还不能支持从检查点重启，因为检查点存储的信息包含老程序中的序列化对象信息，在新程序中将其反序列化可能会出错。这种情况下，只能要么指定一个新的检查点目录，要么删除老的检查点目录。</p>
            </li>
        </ul>

        <hr />

        <h2 id="monitoring-applications">监控应用程序</h2>
        <p>除了Spark自身的监控能力 <a href="monitoring.html">monitoring capabilities</a>之外，对Spark Streaming还有一些额外的监控功能可用。如果实例化了StreamingContext，那么你可以在
            <a href="monitoring.html#web-interfaces">Spark web UI</a> 上看到多出了一个Streaming tab页，上面显示了正在运行的接收器（是否活跃，接收记录的条数，失败信息等）和处理完的批次信息（批次处理时间，查询延时等）。这些信息都可以用来监控streaming应用。</p>

        <p>web UI上有两个度量特别重要：</p>

        <ul>
            <li><em>批次处理耗时（Processing Time）</em> - 处理单个批次耗时</li>
            <li><em>批次调度延时（Scheduling Delay）</em> - 各批次在队列中等待时间（等待上一个批次处理完）
                </li>
        </ul>

        <p>如果批次处理耗时一直比批次间隔时间大，或者批次调度延时持续上升，就意味着系统处理速度跟不上数据接收速度。这时候你就得考虑一下怎么把批次处理时间降下来
            <a href="#reducing-the-batch-processing-times">reducing</a>。</p>

        <p> Spark Streaming 程序的处理进度可以用
            <a href="api/scala/index.html#org.apache.spark.streaming.scheduler.StreamingListener">StreamingListener</a> 接口来监听，这个接口可以监听到接收器的状态和处理时间。不过需要注意的是，这是一个 developer API 接口，换句话说这个接口未来很可能会变动（可能会增加更多度量信息）。</p>

        <hr />
        <hr />

        <h1 id="performance-tuning">性能调优</h1>
        <p>要获得Spark Streaming应用的最佳性能需要一点点调优工作。本节将深入解释一些能够改进Streaming应用性能的配置和参数。总体上来说，你需要考虑这两方面的事情：</p>

        <ol>
            <li>
                <p>提高集群资源利用率，减少单批次处理耗时。</p>
            </li>
            <li>
                <p>设置合适的批次大小，以便使数据处理速度能跟上数据接收速度。</p>
            </li>
        </ol>

        <h2 id="reducing-the-batch-processing-times">减少批次处理时间</h2>
        <p>有不少优化手段都可以减少Spark对每个批次的处理时间。细节将在 <a href="tuning.html">优化指南</a> 中详谈。这里仅列举一些最重要的。</p>

        <h3 class="no_toc" id="level-of-parallelism-in-data-receiving">数据接收并发度</h3>
        <p>
            跨网络接收数据（如：从Kafka、Flume、socket等接收数据）需要在Spark中序列化并存储数据。<br>

            如果接收数据的过程是系统瓶颈，那么可以考虑增加数据接收的并行度。注意，每个输入DStream只包含一个单独的接收器（receiver，运行在worker节点），每个接收器单独接收一路数据流。所以，配置多个输入DStream就能从数据源的不同分区分别接收多个数据流。例如，可以将从Kafka拉取两个topic的数据流分成两个Kafka输入数据流，每个数据流拉取其中一个topic的数据，这样一来会同时有两个接收器并行地接收数据，因而增加了总体的吞吐量。同时，另一方面我们又可以把这些DStream数据流合并成一个，然后可以在合并后的DStream上使用任何可用的transformation算子。示例代码如下：
        </p>

        <div class="codetabs">
            <div data-lang="scala">

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">numStreams</span> <span class="k">=</span> <span class="mi">5</span>
<span class="k">val</span> <span class="n">kafkaStreams</span> <span class="k">=</span> <span class="o">(</span><span class="mi">1</span> <span class="n">to</span> <span class="n">numStreams</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span> <span class="n">i</span> <span class="k">=&gt;</span> <span class="nc">KafkaUtils</span><span class="o">.</span><span class="n">createStream</span><span class="o">(...)</span> <span class="o">}</span>
<span class="k">val</span> <span class="n">unifiedStream</span> <span class="k">=</span> <span class="n">streamingContext</span><span class="o">.</span><span class="n">union</span><span class="o">(</span><span class="n">kafkaStreams</span><span class="o">)</span>
<span class="n">unifiedStream</span><span class="o">.</span><span class="n">print</span><span class="o">()</span></code></pre></figure>

            </div>
            <div data-lang="java">

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kt">int</span> <span class="n">numStreams</span> <span class="o">=</span> <span class="mi">5</span><span class="o">;</span>
<span class="n">List</span><span class="o">&lt;</span><span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">kafkaStreams</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;&gt;(</span><span class="n">numStreams</span><span class="o">);</span>
<span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">numStreams</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
  <span class="n">kafkaStreams</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">KafkaUtils</span><span class="o">.</span><span class="na">createStream</span><span class="o">(...));</span>
<span class="o">}</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">unifiedStream</span> <span class="o">=</span> <span class="n">streamingContext</span><span class="o">.</span><span class="na">union</span><span class="o">(</span><span class="n">kafkaStreams</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="n">kafkaStreams</span><span class="o">.</span><span class="na">subList</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="n">kafkaStreams</span><span class="o">.</span><span class="na">size</span><span class="o">()));</span>
<span class="n">unifiedStream</span><span class="o">.</span><span class="na">print</span><span class="o">();</span></code></pre></figure>

            </div>
            <div data-lang="python">

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">numStreams</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">kafkaStreams</span> <span class="o">=</span> <span class="p">[</span><span class="n">KafkaUtils</span><span class="o">.</span><span class="n">createStream</span><span class="p">(</span><span class="o">...</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">numStreams</span><span class="p">)]</span>
<span class="n">unifiedStream</span> <span class="o">=</span> <span class="n">streamingContext</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="o">*</span><span class="n">kafkaStreams</span><span class="p">)</span>
<span class="n">unifiedStream</span><span class="o">.</span><span class="n">pprint</span><span class="p">()</span></code></pre></figure>

            </div>
        </div>

        <p>另一个可以考虑优化的参数就是接收器的阻塞间隔，该参数由 <a href="configuration.html#spark-streaming">配置参数</a>
            <code>spark.streaming.blockInterval</code>决定。大多数接收器都会将数据合并成一个个数据块，然后再保存到spark内存中。对于map类算子来说，每个批次中数据块的个数将会决定处理这批数据并行任务的个数，每个接收器每批次数据处理任务数约等于 （批次间隔 / 数据块间隔）。例如，对于2秒的批次间隔，如果数据块间隔为200ms，则创建的并发任务数为10。如果任务数太少（少于单机cpu core个数），则资源利用不够充分。如需增加这个任务数，对于给定的批次间隔来说，只需要减少数据块间隔即可。不过，我们还是建议数据块间隔至少要50ms，否则任务的启动开销占比就太高了。</p>

        <p>另一个切分接收数据流的方法是，显示地将输入数据流划分为多个分区（使用 inputStream.repartition(<number of partitions>)）。该操作会在处理前，将数据散开重新分发到集群中多个节点上。</p>

        <p>对于 direct stream, 请参考 <a href="streaming-kafka-integration.html">Spark Streaming + Kafka Integration Guide</a></p>

        <h3 class="no_toc" id="level-of-parallelism-in-data-processing">数据处理并发度</h3>
        <p>在计算各个阶段（stage）中，任何一个阶段的并发任务数不足都有可能造成集群资源利用率低。
            例如，对于reduce类的算子，如：reduceByKey 和 reduceByKeyAndWindow，其默认的并发任务数是由 spark.default.parallelism 决定的。
            你既可以修改这个默认值（spark.default.parallelism），也可以通过参数指定这个并发数量（见 <a href="configuration.html#spark-properties">configuration property</a>）。
        </p>

        <h3 class="no_toc" id="data-serialization">数据序列化</h3>
        <p>调整数据的序列化格式可以大大减少数据序列化的开销。在spark Streaming中主要有两种类型的数据需要序列化：</p>

        <ul>
            <li>
                <p><strong>输入数据</strong>: 默认地，接收器收到的数据是以
                    <a href="api/scala/index.html#org.apache.spark.storage.StorageLevel$">StorageLevel.MEMORY_AND_DISK_SER_2</a>
                    的存储级别存储到执行器（executor）内存中的。也就是说，收到的数据会被序列化以减少GC开销，同时保存两个副本以容错。同时，数据会优先保存在内存里，当内存不足时才吐出到磁盘上。很明显，这个过程中会有数据序列化的开销 – 接收器首先将收到的数据反序列化，然后再以spark所配置指定的格式来序列化数据。</p>
            </li>
            <li>
                <p><strong>Streaming算子所生产的持久化的RDDs</strong>: Streaming计算所生成的RDD可能会持久化到内存中。例如，基于窗口的算子会将数据持久化到内存，因为窗口数据可能会多次处理。所不同的是，spark core默认用 <a href="api/scala/index.html#org.apache.spark.storage.StorageLevel$">StorageLevel.MEMORY_ONLY</a>, 级别持久化RDD数据，而spark streaming默认使用 <a href="api/scala/index.html#org.apache.spark.storage.StorageLevel$">StorageLevel.MEMORY_ONLY_SER</a> 级别持久化接收到的数据，以便尽量减少GC开销。</p>
            </li>
        </ul>

        <p>不管是上面哪一种数据，都可以使用Kryo序列化来减少CPU和内存开销，详见 <a href="tuning.html#data-serialization">Spark Tuning Guide</a> , 对于Kryo，你可以考虑这些优化：注册自定义类型，禁用对象引用跟踪(详见 <a href="configuration.html#compression-and-serialization">Configuration Guide</a>).</p>

        <p>
            在一些特定的场景下，如果数据量不是很大，那么你可以考虑不用序列化格式，不过你需要注意的是取消序列化是否会导致大量的GC开销。例如，如果你的批次间隔比较短（几秒）并且没有使用基于窗口的算子，这种情况下你可以考虑禁用序列化格式。这样可以减少序列化的CPU开销以优化性能，同时GC的增长也不多。        </p>

        <h3 class="no_toc" id="task-launching-overheads">任务启动开销</h3>
        <p>如果每秒启动的任务数过多（比如每秒50个以上），那么将任务发送给slave节点的开销会明显增加，那么你也就很难达到亚秒级（sub-second）的延迟。不过以下两个方法可以减少任务的启动开销：</p>

        <ul>
            <li><strong>任务序列化（Task Serialization）</strong>: 使用Kryo来序列化任务，以减少任务本身的大小，从而提高发送任务的速度。任务的序列化格式是由 spark.closure.serializer 属性决定的。不过，目前还不支持闭包序列化，未来的版本可能会增加对此的支持。</a> 。/li>
        </ul>
        <ul>
            <li><strong>执行模式（Execution mode）</strong>: Spark独立部署或者Mesos粗粒度模式下任务的启动时间比Mesos细粒度模式下的任务启动时间要短。详见
                <a href="running-on-mesos.html">Running on Mesos guide</a> 。/li>
        </ul>

        <p>这些调整有可能能够减少100ms的批次处理时间，这也使得亚秒级的批次间隔成为可能。</p>

        <hr />

        <h2 id="setting-the-right-batch-interval">设置合适的批次间隔</h2>
        <p>要想streaming应用在集群上稳定运行，那么系统处理数据的速度必须能跟上其接收数据的速度。换句话说，批次数据的处理速度应该和其生成速度一样快。对于特定的应用来说，可以从其对应的监控
            <a href="#monitoring-applications">monitoring</a> 页面上观察验证，页面上显示的处理耗时应该要小于批次间隔时间。</p>

        <p>根据spark streaming计算的性质，在一定的集群资源限制下，批次间隔的值会极大地影响系统的数据处理能力。例如，在WordCountNetwork示例中，对于特定的数据速率，一个系统可能能够在批次间隔为2秒时跟上数据接收速度，但如果把批次间隔改为500毫秒系统可能就处理不过来了。所以，批次间隔需要谨慎设置，以确保生产系统能够处理得过来。</p>

        <p>
            要找出适合的批次间隔，你可以从一个比较保守的批次间隔值（如5~10秒）开始测试。要验证系统是否能跟上当前的数据接收速率，你可能需要检查一下端到端的批次处理延迟（可以看看Spark驱动器log4j日志中的Total delay，也可以用 <a href="api/scala/index.html#org.apache.spark.streaming.scheduler.StreamingListener">StreamingListener</a>接口来检测）。如果这个延迟能保持和批次间隔差不多，那么系统基本就是稳定的。否则，如果这个延迟持久在增长，也就是说系统跟不上数据接收速度，那也就意味着系统不稳定。一旦系统文档下来后，你就可以尝试提高数据接收速度，或者减少批次间隔值。不过需要注意，瞬间的延迟增长可以只是暂时的，只要这个延迟后续会自动降下来就没有问题（如：降到小于批次间隔值）
        </p>

        <hr />

        <h2 id="memory-tuning">内存调优</h2>
        <p>Spark应用内存占用和GC调优已经在调优指南 <a href="tuning.html#memory-tuning">Tuning Guide</a>。
            中有详细的讨论。强烈建议你读一读那篇文档。本节中，我们只是讨论一下几个专门用于Spark Streaming的调优参数。</p>

        <p>
            Spark Streaming应用在集群中占用的内存量严重依赖于具体所使用的tranformation算子。例如，如果想要用一个窗口算子操纵最近10分钟的数据，那么你的集群至少需要在内存里保留10分钟的数据；另一个例子是updateStateByKey，如果key很多的话，相对应的保存的key的state也会很多，而这些都需要占用内存。而如果你的应用只是做一个简单的 “映射-过滤-存储”（map-filter-store）操作的话，那需要的内存就很少了。        </p>

        <p>
            一般情况下，streaming接收器接收到的数据会以 StorageLevel.MEMORY_AND_DISK_SER_2 这个存储级别存到spark中，也就是说，如果内存装不下，数据将被吐到磁盘上。数据吐到磁盘上会大大降低streaming应用的性能，因此还是建议根据你的应用处理的数据量，提供充足的内存。最好就是，一边小规模地放大内存，再观察评估，然后再放大，再评估。
        </p>

        <p>
            另一个内存调优的方向就是垃圾回收。因为streaming应用往往都需要低延迟，所以肯定不希望出现大量的或耗时较长的JVM垃圾回收暂停。
        </p>

        <p>
            以下是一些能够帮助你减少内存占用和GC开销的参数或手段：
        </p>

        <ul>
            <li>
                <p><strong>DStream持久化级别（Persistence Level of DStreams）</strong>: 前面数据序列化 <a href="#data-serialization">Data Serialization</a> 这小节已经提到过，默认streaming的输入RDD会被持久化成序列化的字节流。相对于非序列化数据，这样可以减少内存占用和GC开销。如果启用Kryo序列化，还能进一步减少序列化数据大小和内存占用量。如果你还需要进一步减少内存占用的话，可以开启数据压缩（通过spark.rdd.compress这个配置设定），只不过数据压缩会增加CPU消耗。
                </p>
            </li>
            <li>
                <p><strong>清除老数据（Clearing old data）</strong>:
                    默认情况下，所有的输入数据以及DStream的transformation算子产生的持久化RDD都是自动清理的。Spark Streaming会根据所使用的transformation算子来清理老数据。例如，你用了一个窗口操作处理最近10分钟的数据，那么Spark Streaming会保留至少10分钟的数据，并且会主动把更早的数据都删掉。当然，你可以设置 streamingContext.remember 以保留更长时间段的数据（比如：你可能会需要交互式地查询更老的数据）。
                </p>
            </li>
            <li>
                <p><strong>CMS垃圾回收器（CMS Garbage Collector）</strong>:

                    为了尽量减少GC暂停的时间，我们墙裂建议使用CMS垃圾回收器（concurrent mark-and-sweep GC）。虽然CMS GC会稍微降低系统的总体吞吐量，但我们仍建议使用它，因为CMS GC能使批次处理的时间保持在一个比较恒定的水平上。最后，你需要确保在驱动器（通过spark-submit中的–driver-java-options设置）和执行器（使用spark.executor.extraJavaOptions配置参数）上都设置了CMS GC。
                </p>
            </li>
            <li>
                <p><strong>其他提示</strong>: 如果还想进一步减少GC开销，以下是更进一步的可以尝试的手段：</p>
                <ul>
                    <li>使用 <code>OFF_HEAP</code>  堆外内存来持久化RDD。详见Spark编程指南 <a href="rdd-programming-guide.html#rdd-persistence">Spark Programming Guide</a>。</li>
                    <li>使用更多但是更小的执行器进程。这样GC压力就会分散到更多的JVM堆中。</li>
                </ul>
            </li>
        </ul>

        <hr />

        <h5 class="no_toc" id="important-points-to-remember">要记住的点:</h5>
        <ul>
            <li>
                <p>

                    DStream与单个接收器相关联。为了获得读并行性，需要创建多个接收器（receivers），即多个DStream。接收器在执行器（executor）内运行。它占据一个 core(内核)。确保在遇定 receiver slots 后有足够的内核进行处理，即spark.cores.max应考虑receiver slots。接收器以循环方式分配给执行器。

                </p>
            </li>
            <li>
                <p>
                    当从流源接收数据时，接收器创建数据块。每隔blockInterval毫秒生成一个新的数据块。在batchInterval期间创建N个数据块，其中N = batchInterval / blockInterval。这些块由当前执行程序的BlockManager分发给其他执行程序的块BlockManager。之后，将在驱动程序上运行的网络输入跟踪器通知块位置以进行进一步处理。
                </p>
            </li>
            <li>
                <p>
                    在驱动程序上为在 batchInterval 期间创建的块创建一个 RDD。 batchInterval 期间生成的块是 RDD 的分区。每个分区都是spark中的任务。 blockInterval == batchinterval意味着创建了一个分区，并且可能在本地处理它。
                </p>
            </li>
            <li>
                <p>
                    除非 non-local scheduling（非本地调度）进行，否则块上的 map tasks（映射任务）将在 executors（接收 block，复制块的另一个块）中进行处理。具有更大的blockinterval意味着更大的块。 spark.locality.wait的值很高，增加了在本地节点上处理块的机会。需要在这两个参数之间找到平衡，以确保在本地处理更大的块。
                </p>
            </li>
            <li>
                <p>
                    而不是依赖于 batchInterval 和 blockInterval，您可以通过调用 inputDstream.repartition(n) 来定义 number of partitions（分区数）。这样可以随机重新组合 RDD 中的数据，创建 n 个分区。是的，为了更大的 parallelism（并行性）。虽然是 shuffle 的代价。RDD 的处理由 driver’s jobscheduler 作为一项工作安排。在给定的时间点，只有一个 job 是 active 的。因此，如果一个作业正在执行，则其他作业将排队。

                </p>
            </li>
            <li>
                <p> 如果您有两个 dstream，将会有两个 RDD 形成，并且将创建两个将被安排在另一个之后的作业。为了避免这种情况，你可以联合两个 dstream。这将确保为 dstream 的两个 RDD 形成一个 unionRDD。这个 unionRDD 然后被认为是一个 single job（单一的工作）。但 RDD 的 partitioning（分区）不受影响。
                </p>
            </li>
            <li>
                <p> 如果 batch processing time（批处理时间）超过 batchinterval（批次间隔），那么显然 receiver 的内存将会开始填满，最终会抛出 exceptions（最可能是 BlockNotFoundException）。目前没有办法暂停 receiver。使用 SparkConf 配置 spark.streaming.receiver.maxRate，receiver 的 rate 可以受到限制。  </p>
            </li>
        </ul>

        <hr />
        <hr />

        <h1 id="fault-tolerance-semantics">容错语义</h1>
        <p>本节中，我们将讨论Spark Streaming应用在出现失败时的具体行为。

        </p>

        <h2 class="no_toc" id="background">背景</h2>
        <p>
            要理解Spark Streaming所提供的容错语义，我们首先需要回忆一下Spark RDD所提供的基本容错语义。

        </p>

        <ol>
            <li>RDD 是不可变的，可重新计算的，分布式数据集。每个 RDD 都记录了其创建算子的血统信息，其中每个算子都以可容错的数据集作为输入数据。</li>
            <li>如果 RDD 的某个分区因为节点失效而丢失，则该分区可以根据 RDD 的血统信息以及相应的原始输入数据集重新计算出来。</li>
            <li>假定所有 RDD transformation 算子计算过程都是确定性的，那么通过这些算子得到的最终 RDD 总是包含相同的数据，而与 Spark 集群的是否故障无关。</li>
        </ol>

        <p>Spark 主要操作的是可容错文件系统的数据，如：HDFS 或 S3。因此，所有从这些可容错数据源产生的 RDD 也是可容错的。<br>
            然而，对于 Spark Streaming 并非如此，因为多数情况下 Streaming 需要从网络远端接收数据，这就导致 Streaming 的数据源并不可靠（尤其是对于使用了fileStream的应用）。要实现RDD相同的容错属性，接收的数据在集群中的工作节点中的多个 Spark executors 之间进行复制（默认 replication factor（备份因子）为 2）。因此一旦出现故障，系统需要恢复两种数据：</p>

        <ol>
            <li><em>接收并保存了副本的数据</em> - 数据不会因为单个worker节点故障而丢失，因为有副本！</li>
            <li><em>接收但尚未保存副本数据</em> - 因为数据并没有副本，所以一旦故障，只能从数据源重新获取。</li>
        </ol>

        <p>此外，还有两种可能的故障类型需要考虑：</p>

        <ol>
            <li><em>Worker节点故障 </em> - 任何运行执行器的worker节点一旦故障，节点上内存中的数据都会丢失。如果这些节点上有接收器在运行，那么其包含的缓存数据也会丢失。</li>
            <li><em>Driver节点故障</em> - 如果Spark Streaming的驱动节点故障，那么很显然SparkContext对象就没了，所有执行器及其内存数据也会丢失。</li>
        </ol>

        <p>有了以上这些基本知识，下面我们就进一步了解一下Spark Streaming的容错语义。

        </p>

        <h2 class="no_toc" id="definitions">定义</h2>
        <p>流式系统的可靠度语义可以据此来分类：单条记录在系统中被处理的次数保证。一个流式系统可能提供保证必定是以下三种之一（不管系统是否出现故障）</p>

        <ol>
            <li><em>至多一次（At most once）</em>: 每条记录要么被处理一次，要么就没有处理。</li>
            <li><em>至少一次（At least once）</em>: 每条记录至少被处理过一次（一次或多次）。这种保证能确保没有数据丢失，比“至多一次”要强。但有可能出现数据重复。</li>
            <li><em>精确一次（Exactly once）</em>: 每条记录都精确地只被处理一次 – 也就是说，既没有数据丢失，也不会出现数据重复。这是三种保证中最强的一种。</li>
        </ol>

        <h2 class="no_toc" id="basic-semantics">基础语义</h2>
        <p>任何流式处理系统一般都会包含以下三个数据处理步骤：</p>

        <ol>
            <li>
                <p><em>数据接收（Receiving the data）: 从数据源拉取数据。</em>: The data is received from sources using Receivers or otherwise.</p>
            </li>
            <li>
                <p><em>数据转换（Transforming the data）</em>: 将接收到的数据进行转换（使用DStream和RDD transformation算子）。</p>
            </li>
            <li>
                <p><em>数据推送（Pushing out the data）</em>: 将转换后最终数据推送到外部文件系统，数据库或其他展示系统。</p>
            </li>
        </ol>

        <p>
            如果 Streaming 应用需要做到端到端的“精确一次”的保证，那么就必须在以上三个步骤中各自都保证精确一次：即，每条记录必须，只接收一次、处理一次、推送一次。下面让我们在Spark Streaming的上下文环境中来理解一下这三个步骤的语义：
        </p>

        <ol>
            <li>
                <p><em>数据接收</em>:
                    不同数据源提供的保证不同，下一节再详细讨论。
                </p>
            </li>
            <li>
                <p><em>数据转换</em>:
                    所有的数据都会被“精确一次”处理，这要归功于RDD提供的保障。即使出现故障，只要数据源还能访问，最终所转换得到的RDD总是包含相同的内容。
                </p>
            </li>
            <li>
                <p><em>数据推送</em>:
                    输出操作默认保证“至少一次”的语义，是否能“精确一次”还要看所使用的输出算子（是否幂等）以及下游系统（是否支持事务）。不过用户也可以开发自己的事务机制来实现“精确一次”语义。这个后续会有详细讨论。
                </p>
            </li>
        </ol>

        <h2 class="no_toc" id="semantics-of-received-data">接收数据语义</h2>
        <p>
            不同的输入源提供不同的数据可靠性级别，从“至少一次”到“精确一次”。
        </p>

        <h3 class="no_toc" id="with-files">从文件接收数据</h3>
        <p>
            如果所有的输入数据都来源于可容错的文件系统，如HDFS，那么Spark Streaming就能在任何故障中恢复并处理所有的数据。这种情况下就能保证精确一次语义，也就是说不管出现什么故障，所有的数据总是精确地只处理一次，不多也不少。
        </p>

        <h3 class="no_toc" id="with-receiver-based-sources">基于接收器接收数据</h3>
        <p>对于基于接收器的输入源，容错语义将同时依赖于故障场景和接收器类型。前面也已经提到过，spark Streaming主要有两种类型的接收器：</p>

        <ol>
            <li><em>可靠接收器</em> - 这类接收器会在数据接收并保存好副本后，向可靠数据源发送确认信息。这类接收器故障时，是不会给缓存的（已接收但尚未保存副本）数据发送确认信息。因此，一旦接收器重启，没有收到确认的数据，会重新从数据源再获取一遍，所以即使有故障也不会丢数据。</li>
            <li><em>不可靠接收器</em> - 这类接收器不会发送确认信息，因此一旦worker和driver出现故障，就有可能会丢失数据。</li>
        </ol>

        <p>对于不同的接收器，我们可以获得如下不同的语义。如果一个worker节点故障了，对于可靠接收器来说，不会有数据丢失。而对于不可靠接收器，缓存的（接收但尚未保存副本）数据可能会丢失。如果driver节点故障了，除了接收到的数据之外，其他的已经接收且已经保存了内存副本的数据都会丢失，这将会影响有状态算子的计算结果。</p>

        <p>
            为了避免丢失已经收到且保存副本的数，从 spark 1.2 开始引入了WAL（<a href="#deploying-applications">write-ahead logs enabled</a>），以便将这些数据写入到可容错的存储中。只要你使用可靠接收器，同时启用WAL（write ahead logs enabled），那么久再也不用为数据丢失而担心了。并且这时候，还能提供“至少一次”的语义保证。
        </p>

        <p>下表总结了故障情况下的各种语义：</p>

        <table class="table">
            <tr>
                <th style="width:30%">部署场景</th>
                <th>Worker 故障</th>
                <th>Driver 故障</th>
            </tr>
            <tr>
                <td>
                    <i>Spark 1.1及以前版本</i> 或者<br />
                    <i>Spark 1.2及以后版本，且未开启WAL</i>
                </td>
                <td>
                    若使用不可靠接收器，则可能丢失缓存（已接收但尚未保存副本）数据；<br />
                    若使用可靠接收器，则没有数据丢失，且提供至少一次处理语义<br />
                </td>
                <td>
                    若使用不可靠接收器，则缓存数据和已保存数据都可能丢失；<br />
                    若使用可靠接收器，则没有缓存数据丢失<br />
                    但已保存数据可能丢失，且不提供语义保证<br />
                </td>
            </tr>
            <tr>
                <td><i>Spark 1.2及以后版本，并启用WAL</i></td>
                <td>
                    若使用可靠接收器，则没有数据丢失，<br />
                    且提供至少一次语义保证
                </td>
                <td>
                    若使用可靠接收器和文件，则无数据丢失<br />
                    且提供至少一次语义保证
                </td>
            </tr>
            <tr>
                <td></td>
                <td></td>
                <td></td>
            </tr>
        </table>

        <h3 class="no_toc" id="with-kafka-direct-api">从Kafka Direct API接收数据</h3>
        <p>从Spark 1.3开始，我们引入Kafka Direct API，该API能为Kafka数据源提供“精确一次”语义保证。有了这个输入API，再加上输出算子的“精确一次”保证，你就能真正实现端到端的“精确一次”语义保证。（改功能截止Spark 1.6.1还是实验性的）更详细的说明见：<a href="streaming-kafka-integration.html">Kafka Integration Guide</a>。</p>

        <h2 class="no_toc" id="semantics-of-output-operations">输出算子的语义</h2>
        <p>
            输出算子（如 foreachRDD）提供“至少一次”语义保证，也就是说，如果worker故障，单条输出数据可能会被多次写入外部实体中。不过这对于文件系统来说是可以接受的（使用saveAs***Files 多次保存文件会覆盖之前的），所以我们需要一些额外的工作来实现“精确一次”语义。主要有两种实现方式：
        </p>

        <ul>
            <li>
                <p><em>幂等更新（Idempotent updates）</em>:就是说多次操作，产生的结果相同。例如，多次调用saveAs***Files保存的文件总是包含相同的数据。</p>
            </li>
            <li>
                <p><em>Transactional updates</em>: 所有的更新都是事务性的，这样一来就能保证更新的原子性。以下是一种实现方式：</p>

                <ul>
                    <li> 用批次时间（在foreachRDD中可用）和分区索引创建一个唯一标识，该标识代表流式应用中唯一的一个数据块。</li>
                    <li>
                        <p>基于这个标识建立更新事务，并使用数据块数据更新外部系统。也就是说，如果该标识未被提交，则原子地将标识代表的数据更新到外部系统。否则，就认为该标识已经被提交，直接忽略之。</p>

                        <pre><code>dstream.foreachRDD { (rdd, time) =&gt;
  rdd.foreachPartition { partitionIterator =&gt;
    val partitionId = TaskContext.get.partitionId()
    val uniqueId = generateUniqueId(time.milliseconds, partitionId)
    // use this uniqueId to transactionally commit the data in partitionIterator
  }
}
</code></pre>
                    </li>
                </ul>
            </li>
        </ul>

        <hr />
        <hr />

        <h1 id="where-to-go-from-here">快速链接</h1>
        <ul>
            <li>附加指南
                <ul>
                    <li><a href="streaming-kafka-integration.html">Kafka 集成指南</a></li>
                    <li><a href="streaming-kinesis-integration.html">Kinesis 集成指南</a></li>
                    <li><a href="streaming-custom-receivers.html">自定义 Receiver（接收器）指南</a></li>
                </ul>
            </li>
            <li>第三方 DStream 数据源可以在 <a href="https://spark.apache.org/third-party-projects.html"> 第三方项目</a> 上查看</li>
            <li>API 文档
                <ul>
                    <li>Scala 文档
                        <ul>
                            <li><a href="api/scala/index.html#org.apache.spark.streaming.StreamingContext">StreamingContext</a> 和
                                <a href="api/scala/index.html#org.apache.spark.streaming.dstream.DStream">DStream</a></li>
                            <li><a href="api/scala/index.html#org.apache.spark.streaming.kafka.KafkaUtils$">KafkaUtils</a>,
                                <a href="api/scala/index.html#org.apache.spark.streaming.flume.FlumeUtils$">FlumeUtils</a>,
                                <a href="api/scala/index.html#org.apache.spark.streaming.kinesis.KinesisUtils$">KinesisUtils</a>,</li>
                        </ul>
                    </li>
                    <li>Java 文档
                        <ul>
                            <li><a href="api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html">JavaStreamingContext</a>,
                                <a href="api/java/index.html?org/apache/spark/streaming/api/java/JavaDStream.html">JavaDStream</a> 和
                                <a href="api/java/index.html?org/apache/spark/streaming/api/java/JavaPairDStream.html">JavaPairDStream</a></li>
                            <li><a href="api/java/index.html?org/apache/spark/streaming/kafka/KafkaUtils.html">KafkaUtils</a>,
                                <a href="api/java/index.html?org/apache/spark/streaming/flume/FlumeUtils.html">FlumeUtils</a>,
                                <a href="api/java/index.html?org/apache/spark/streaming/kinesis/KinesisUtils.html">KinesisUtils</a></li>
                        </ul>
                    </li>
                    <li>Python 文档
                        <ul>
                            <li><a href="api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext">StreamingContext</a> 和 <a href="api/python/pyspark.streaming.html#pyspark.streaming.DStream">DStream</a></li>
                            <li><a href="api/python/pyspark.streaming.html#pyspark.streaming.kafka.KafkaUtils">KafkaUtils</a></li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li>更多的示例在 <a href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/streaming">Scala</a>
                和 <a href="https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples/streaming">Java</a>
                和 <a href="https://github.com/apache/spark/tree/master/examples/src/main/python/streaming">Python</a></li>
            <li>描述 Spark Streaming 的 <a href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-259.pdf">Paper</a> 和 <a href="http://youtu.be/g171ndOHgJ0">video</a> 。</li>
        </ul>


    </div>

    <!-- /container -->
</div>

<script src="js/vendor/jquery-1.12.4.min.js"></script>
<script src="js/vendor/bootstrap.min.js"></script>
<script src="js/vendor/anchor.min.js"></script>
<script src="js/main.js"></script>

<!-- MathJax Section -->
<script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
<script>
    // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
    // We could use "//cdn.mathjax...", but that won't support "file://".
    (function(d, script) {
        script = d.createElement('script');
        script.type = 'text/javascript';
        script.async = true;
        script.onload = function(){
            MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                    displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                    processEscapes: true,
                    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                }
            });
        };
        script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
            'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
            '?config=TeX-AMS-MML_HTMLorMML';
        d.getElementsByTagName('head')[0].appendChild(script);
    }(document));
</script>
</body>
</html>
