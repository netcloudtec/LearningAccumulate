<!DOCTYPE html>
<!--[if lt IE 7]>
<html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>
<html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>
<html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" xmlns="http://www.w3.org/1999/html"> <!--<![endif]-->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>RDD Programming Guide - Spark 3.1.2 Documentation</title>

    <meta name="description" content="Spark 2.3.1 programming guide in Java, Scala and Python">


    <link rel="stylesheet" href="css/bootstrap.min.css">
    <style>
        body {
            padding-top: 60px;
            padding-bottom: 40px;
        }
    </style>
    <meta name="viewport" content="width=device-width">
    <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
    <link rel="stylesheet" href="css/main.css">

    <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

    <link rel="stylesheet" href="css/pygments-default.css">


    <!-- Google analytics script -->
    <script type="text/javascript">
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-32518208-2']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script');
            ga.type = 'text/javascript';
            ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(ga, s);
        })();
    </script>


</head>
<body>
<!--[if lt IE 7]>
<p class="chromeframe">You are using an outdated browser. <a href="http://browsehappy.com/">Upgrade your browser
    today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better
    experience this site.</p>
<![endif]-->

<!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

<div class="navbar navbar-fixed-top" id="topbar">
    <div class="navbar-inner">
        <div class="container">
            <div class="brand"><a href="index.html">
                <img src="img/spark-logo-hd.png" style="height:50px;"/></a><span class="version">3.1.2</span>
            </div>
            <ul class="nav">
                <!--TODO(andyk): Add class="active" attribute to li some how.-->
                <li><a href="index.html">概述</a></li>

                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">编程指南<b
                            class="caret"></b></a>
                    <ul class="dropdown-menu">
                        <li><a href="quick-start.html">快速入门</a></li>
                        <li><a href="rdd-programming-guide.html">Spark 编程指南</a></li>
                        <li><a href="sql-programming-guide.html">SQL, DataFrames, and Datasets</a></li>
                        <li><a href="structured-streaming-programming-guide.html">Structured Streaming</a></li>
                        <li><a href="streaming-programming-guide.html">Spark Streaming (DStreams)</a></li>
                        <li><a href="ml-guide.html">MLlib (Machine Learning)</a></li>
                        <li><a href="graphx-programming-guide.html">GraphX (Graph Processing)</a></li>
                        <li><a href="sparkr.html">SparkR (R on Spark)</a></li>
                    </ul>
                </li>

                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">API 文档<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        <li><a href="api/scala/index.html#org.apache.spark.package">Scala</a></li>
                        <li><a href="api/java/index.html">Java</a></li>
                        <li><a href="api/python/index.html">Python</a></li>
                        <li><a href="api/R/index.html">R</a></li>
                        <li><a href="api/sql/index.html">SQL, Built-in Functions</a></li>
                    </ul>
                </li>

                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">部署<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        <li><a href="cluster-overview.html">Overview</a></li>
                        <li><a href="./latest/submitting-applications.html">Submitting Applications</a></li>
                        <li class="divider"></li>
                        <li><a href="spark-standalone.html">Spark Standalone</a></li>
                        <li><a href="running-on-mesos.html">Mesos</a></li>
                        <li><a href="running-on-yarn.html">YARN</a></li>
                        <li><a href="running-on-kubernetes.html">Kubernetes</a></li>
                    </ul>
                </li>

                <li class="dropdown">
                    <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">更多<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        <li><a href="configuration.html">Configuration</a></li>
                        <li><a href="monitoring.html">Monitoring</a></li>
                        <li><a href="tuning.html">Tuning Guide</a></li>
                        <li><a href="job-scheduling.html">Job Scheduling</a></li>
                        <li><a href="security.html">Security</a></li>
                        <li><a href="hardware-provisioning.html">Hardware Provisioning</a></li>
                        <li class="divider"></li>
                        <li><a href="building-spark.html">Building Spark</a></li>
                        <li><a href="http://spark.apache.org/contributing.html">Contributing to Spark</a></li>
                        <li><a href="http://spark.apache.org/third-party-projects.html">Third Party Projects</a></li>
                    </ul>
                </li>
            </ul>
            <!--<p class="navbar-text pull-right"><span class="version-text">v2.3.1</span></p>-->
        </div>
    </div>
</div>

<div class="container-wrapper">

    <div class="content" id="content">

        <h1 class="title">Spark 编程指南</h1>


        <ul id="markdown-toc">
            <li><a href="#overview" id="markdown-toc-overview">概述</a></li>
            <li><a href="#linking-with-spark" id="markdown-toc-linking-with-spark">Spark依赖</a></li>
            <li><a href="#initializing-spark" id="markdown-toc-initializing-spark">初始化Spark</a>
                <ul>
                    <li><a href="#using-the-shell" id="markdown-toc-using-the-shell">使用Shell</a></li>
                </ul>
            </li>
            <li><a href="#resilient-distributed-datasets-rdds" id="markdown-toc-resilient-distributed-datasets-rdds">弹性分布式数据集(RDDs)</a>
                <ul>
                    <li><a href="#parallelized-collections" id="markdown-toc-parallelized-collections">并行集合</a></li>
                    <li><a href="#external-datasets" id="markdown-toc-external-datasets">外部 Datasets</a></li>
                    <li><a href="#rdd-operations" id="markdown-toc-rdd-operations">RDD 操作</a>
                        <ul>
                            <li><a href="#basics" id="markdown-toc-basics">基础</a></li>
                            <li><a href="#passing-functions-to-spark" id="markdown-toc-passing-functions-to-spark">传递函数到Spark</a>
                            </li>
                            <li><a href="#understanding-closures-" id="markdown-toc-understanding-closures-">闭包理解 <a
                                    name="ClosuresLink"></a></a>
                                <ul>
                                    <li><a href="#example" id="markdown-toc-example">示例</a></li>
                                    <li><a href="#local-vs-cluster-modes" id="markdown-toc-local-vs-cluster-modes">本地 vs
                                        集群模式</a></li>
                                    <li><a href="#printing-elements-of-an-rdd"
                                           id="markdown-toc-printing-elements-of-an-rdd">打印RDD中的元素</a></li>
                                </ul>
                            </li>
                            <li><a href="#working-with-key-value-pairs" id="markdown-toc-working-with-key-value-pairs">使用Key-Value键值对</a>
                            </li>
                            <li><a href="#transformations" id="markdown-toc-transformations">Transformations (转换)</a></li>
                            <li><a href="#actions" id="markdown-toc-actions">Actions (行动) </a></li>
                            <li><a href="#shuffle-operations" id="markdown-toc-shuffle-operations">Shuffle 操作</a>
                                <ul>
                                    <li><a href="#background" id="markdown-toc-background">Background (幕后)</a></li>
                                    <li><a href="#performance-impact" id="markdown-toc-performance-impact">性能影响</a></li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                    <li><a href="#rdd-persistence" id="markdown-toc-rdd-persistence">RDD Persistence (持久化)</a>
                        <ul>
                            <li><a href="#which-storage-level-to-choose"
                                   id="markdown-toc-which-storage-level-to-choose">如何选择存储级别 ?</a></li>
                            <li><a href="#removing-data" id="markdown-toc-removing-data">删除数据</a></li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li><a href="#shared-variables" id="markdown-toc-shared-variables">共享变量</a>
                <ul>
                    <li><a href="#broadcast-variables" id="markdown-toc-broadcast-variables">广播变量</a></li>
                    <li><a href="#accumulators" id="markdown-toc-accumulators"> Accumulators (累加器)</a></li>
                </ul>
            </li>
            <li><a href="#deploying-to-a-cluster" id="markdown-toc-deploying-to-a-cluster">部署应用到集群</a></li>
            <li><a href="#launching-spark-jobs-from-java--scala"
                   id="markdown-toc-launching-spark-jobs-from-java--scala">从Java / Scala 运行spark任务</a></li>
            <li><a href="#unit-testing" id="markdown-toc-unit-testing">单元测试</a></li>
            <li><a href="#where-to-go-from-here" id="markdown-toc-where-to-go-from-here">快速链接</a></li>
        </ul>

        <h1 id="overview">概述</h1>

        <p>在一个较高的概念上来说，每一个Spark应用程序都包含一个<em>驱动程序</em>，该程序运行着用户的main方法，并在集群上执行各种并行操作。<strong style="color: red">Spark提供的主要抽象是弹性分布式数据集(RDD)，它是跨集群节点分区的元素集合，可以并行操作。</strong>
            RDDs是通过从Hadoop文件系统(或任何Hadoop支持的其他文件系统)中的文件开始，或者从驱动程序中现有的Scala集合开始，并对其进行转换而创建的。用户还可能要求Spark在内存中<strong style="color: red">持久化一个RDD</strong>，从而允许在并行操作之间高效地重用它。最后，RDDs自动从节点故障中恢复。
        </p>

        <p>
            Spark中的第二个抽象是可用于并行操作的<strong style="color: red">共享变量</strong>。默认情况下，当Spark的一个函数作为一组不同节点上的任务并行运行时，它将函数中使用的每个变量的副本发送给每个任务。有时候，一个变量需要在整个任务中，或者在任务和 driver program（驱动程序）之间来共享。Spark支持两种类型的共享变量:广播变量(可以用于在所有节点的内存中缓存一个值)和累加器(他是一个只能被 “added（增加）” 的变量，例如 counters 和 sums)。</p>

        <p>本指南在Spark支持的每种语言中都展示了这些特性。如果您启动了Spark的交互式shell——Scala shell的bin/ Spark shell或者Python
            shell的bin/pyspark，这是最容易的。</p>

        <h1 id="linking-with-spark">Spark依赖</h1>

        <div class="codetabs">

            <div data-lang="scala">

                <p>Spark 3.1.2 默认使用 Scala 2.12 来构建和发布直到运行。（当然，Spark 也可以与其它的 Scala 版本一起运行）。为了使用 Scala 编写应用程序，您需要使用可兼容的 Scala 版本（例如，2.12.X）。</p>

                <p>要编写一个Spark应用程序，您需要在Spark上添加一个Maven依赖。Spark可通过Maven 中央仓库获取:</p>

                <pre><code>groupId = org.apache.spark
artifactId = spark-core_2.12
version = 3.1.2
</code></pre>

                <p>此外，如果您想访问一个 HDFS 集群，则需要针对您的 HDFS 版本添加一个 hadoop-client（hadoop 客户端）依赖。</p>

                <pre><code>groupId = org.apache.hadoop
artifactId = hadoop-client
version = &lt;your-hdfs-version&gt;
</code></pre>

                <p>最后，需要将一些Spark类导入到程序中。加上以下几行:</p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span
                        class="k">import</span> <span class="nn">org.apache.spark.SparkContext</span>
<span class="k">import</span> <span class="nn">org.apache.spark.SparkConf</span></code></pre>
                </figure>

                <p>在Spark 1.3.0之前，您需要显式地导入org.apache.spark.SparkContext._。启用必要的隐式转换</p>

            </div>

            <div data-lang="java">

                <p>Spark 3.1.2 支持
                    <a href="http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html">lambda
                        表达式</a>
                    以简明地编写函数，否则您可以使用
                    <a href="api/java/index.html?org/apache/spark/api/java/function/package-summary.html">org.apache.spark.api.java.function</a>
                    包中的类。</p>

                <p>请注意，Spark 3.1.2中删除了对Java 7的支持。</p>

                <p>要用Java编写Spark应用程序，需要在Spark上添加依赖项。 Spark可通过Maven Central获得:</p>

                <pre><code>groupId = org.apache.spark
artifactId = spark-core_2.12
version = 3.1.2
</code></pre>

                <p>此外，如果您希望访问HDFS群集，则需要为您的HDFS版本添加
                    <code>hadoop-client</code>依赖项。</p>

                <pre><code>groupId = org.apache.hadoop
artifactId = hadoop-client
version = &lt;your-hdfs-version&gt;
</code></pre>

                <p>最后，您需要将一些Spark类导入到您的程序中。 添加以下行:</p>

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span
                        class="kn">import</span> <span class="nn">org.apache.spark.api.java.JavaSparkContext</span><span
                        class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.api.java.JavaRDD</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.SparkConf</span><span class="o">;</span></code></pre>
                </figure>

            </div>

            <div data-lang="python">

                <p>Spark 2.3.1 works with Python 2.7+ or Python 3.4+. It can use the standard CPython interpreter,
                    so C libraries like NumPy can be used. It also works with PyPy 2.3+.</p>

                <p>Python 2.6 support was removed in Spark 3.1.2.</p>

                <p>Spark applications in Python can either be run with the <code>bin/spark-submit</code> script which
                    includes Spark at runtime, or by including including it in your setup.py as:</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span>    <span
                        class="n">install_requires</span><span class="o">=</span><span class="p">[</span>
        <span class="s1">&#39;pyspark=={site.SPARK_VERSION}&#39;</span>
    <span class="p">]</span></code></pre>
                </figure>

                <p>To run Spark applications in Python without pip installing PySpark, use the
                    <code>bin/spark-submit</code> script located in the Spark directory.
                    This script will load Spark&#8217;s Java/Scala libraries and allow you to submit applications to a
                    cluster.
                    You can also use <code>bin/pyspark</code> to launch an interactive Python shell.</p>

                <p>If you wish to access HDFS data, you need to use a build of PySpark linking
                    to your version of HDFS.
                    <a href="http://spark.apache.org/downloads.html">Prebuilt packages</a> are also available on the
                    Spark homepage
                    for common HDFS versions.</p>

                <p>Finally, you need to import some Spark classes into your program. Add the following line:</p>

                <figure class="highlight">
                    <pre><code class="language-python" data-lang="python"><span></span><span
                            class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span
                            class="n">SparkContext</span><span class="p">,</span> <span
                            class="n">SparkConf</span></code></pre>
                </figure>

                <p>PySpark requires the same minor version of Python in both driver and workers. It uses the default
                    python version in PATH,
                    you can specify which version of Python you want to use by <code>PYSPARK_PYTHON</code>, for example:
                </p>

                <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ <span
                        class="nv">PYSPARK_PYTHON</span><span class="o">=</span>python3.4 bin/pyspark
$ <span class="nv">PYSPARK_PYTHON</span><span class="o">=</span>/opt/pypy-2.5/bin/pypy bin/spark-submit examples/src/main/python/pi.py</code></pre>
                </figure>

            </div>

        </div>

        <h1 id="initializing-spark">初始化Spark</h1>

        <div class="codetabs">

            <div data-lang="scala">

                <p>Spark程序必须做的第一件事是创建一个SparkContext对象，该对象告诉Spark如何访问集群。要创建一个SparkContext，首先需要构建一个包含应用程序信息的SparkConf对象。
                </p>

                <p>每一个 JVM 可能只能激活一个 SparkContext 对象。在创新一个新的对象之前，必须调用 stop() 该方法停止活跃的 SparkContext。</p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span
                        class="k">val</span> <span class="n">conf</span> <span class="k">=</span> <span
                        class="k">new</span> <span class="nc">SparkConf</span><span class="o">().</span><span class="n">setAppName</span><span
                        class="o">(</span><span class="n">appName</span><span class="o">).</span><span class="n">setMaster</span><span
                        class="o">(</span><span class="n">master</span><span class="o">)</span>
<span class="k">new</span> <span class="nc">SparkContext</span><span class="o">(</span><span class="n">conf</span><span
                            class="o">)</span></code></pre>
                </figure>

            </div>

            <div data-lang="java">

                <p>Spark程序必须做的第一件事是创建一个JavaSparkContext对象，该对象告诉Spark如何访问集群。要创建一个SparkContext，首先需要构建一个包含应用程序信息的SparkConf对象。

                    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span
                        class="n">SparkConf</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span
                        class="n">SparkConf</span><span class="o">().</span><span class="na">setAppName</span><span
                        class="o">(</span><span class="n">appName</span><span class="o">).</span><span class="na">setMaster</span><span
                        class="o">(</span><span class="n">master</span><span class="o">);</span>
<span class="n">JavaSparkContext</span> <span class="n">sc</span> <span class="o">=</span> <span
                            class="k">new</span> <span class="n">JavaSparkContext</span><span class="o">(</span><span
                            class="n">conf</span><span class="o">);</span></code></pre>
                </figure>

            </div>

            <div data-lang="python">

                <p>The first thing a Spark program must do is to create a <a
                        href="api/python/pyspark.html#pyspark.SparkContext">SparkContext</a> object, which tells Spark
                    how to access a cluster. To create a <code>SparkContext</code> you first need to build a <a
                            href="api/python/pyspark.html#pyspark.SparkConf">SparkConf</a> object
                    that contains information about your application.</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span
                        class="n">conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">()</span><span
                        class="o">.</span><span class="n">setAppName</span><span class="p">(</span><span class="n">appName</span><span
                        class="p">)</span><span class="o">.</span><span class="n">setMaster</span><span
                        class="p">(</span><span class="n">master</span><span class="p">)</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span
                            class="n">conf</span><span class="o">=</span><span class="n">conf</span><span
                            class="p">)</span></code></pre>
                </figure>

            </div>

        </div>

        <p><code>appName</code>参数是应用程序在集群UI上显示的名称。
            <code>master</code>是一个<a href="submitting-applications.html#master-urls">Spark, Mesos or YARN 集群
                URL</a>,
            或者是&#8220;local&#8221; 字符串以local模式执行.
            实际上，在集群上运行时，您不希望在程序中硬编码master，而是使用<code>spark-submit</code>在那里<a
                    href="submitting-applications.html">启动应用程序</a>
            并接收它。但是，对于本地测试和单元测试，您可以通过“local”运行Spark进程中。</p>

        <h2 id="using-the-shell">使用Shell</h2>

        <div class="codetabs">

            <div data-lang="scala">

                <p>在Spark shell中，一个特殊的 interpreter-aware（可用的解析器）SparkContext 已经为您创建好了，称之为 sc 的变量。。创建自己的SparkContext将不起作用。
                    您可以使用 --master参数设置SparkContext连接到哪个master上，并且您可以通过将逗号分隔的列表传递给——JARs参数来将jar添加到类路径中。您还可以向shell会话添加依赖项(例如Spark包)，方法是向——Packages参数提供一个用逗号分隔的Maven坐标列表。可能存在依赖关系的任何其他存储库(例如Sonatype)都可以传递到 --repositories 参数。例如，要明确使用四个核（CPU）来运行 bin/spark-shell，使用:</p>

                <figure class="highlight">
                    <pre><code class="language-bash"
                               data-lang="bash"><span></span>$ ./bin/spark-shell --master local<span
                            class="o">[</span><span class="m">4</span><span class="o">]</span></code></pre>
                </figure>

                <p>或者，添加代码。jar的类路径，使用:</p>

                <figure class="highlight">
                    <pre><code class="language-bash"
                               data-lang="bash"><span></span>$ ./bin/spark-shell --master local<span
                            class="o">[</span><span class="m">4</span><span
                            class="o">]</span> --jars code.jar</code></pre>
                </figure>

                <p>使用Maven坐标包括一个依赖项:

                </p>

                <figure class="highlight">
                    <pre><code class="language-bash"
                               data-lang="bash"><span></span>$ ./bin/spark-shell --master local<span
                            class="o">[</span><span class="m">4</span><span class="o">]</span> --packages <span
                            class="s2">&quot;org.example:example:0.1&quot;</span></code></pre>
                </figure>

                <p>要获得完整的选项列表，请运行spark-shell—help。在幕后，spark-shell调用更通用的spark提交脚本。</p>

            </div>

            <div data-lang="python">

                <p>In the PySpark shell, a special interpreter-aware SparkContext is already created for you, in the
                    variable called <code>sc</code>. Making your own SparkContext will not work. You can set which
                    master the
                    context connects to using the <code>--master</code> argument, and you can add Python .zip, .egg or
                    .py files
                    to the runtime path by passing a comma-separated list to <code>--py-files</code>. You can also add
                    dependencies
                    (e.g. Spark Packages) to your shell session by supplying a comma-separated list of Maven coordinates
                    to the <code>--packages</code> argument. Any additional repositories where dependencies might exist
                    (e.g. Sonatype)
                    can be passed to the <code>--repositories</code> argument. Any Python dependencies a Spark package
                    has (listed in
                    the requirements.txt of that package) must be manually installed using <code>pip</code> when
                    necessary.
                    For example, to run <code>bin/pyspark</code> on exactly four cores, use:</p>

                <figure class="highlight">
                    <pre><code class="language-bash" data-lang="bash"><span></span>$ ./bin/pyspark --master local<span
                            class="o">[</span><span class="m">4</span><span class="o">]</span></code></pre>
                </figure>

                <p>Or, to also add <code>code.py</code> to the search path (in order to later be able to <code>import
                    code</code>), use:</p>

                <figure class="highlight">
                    <pre><code class="language-bash" data-lang="bash"><span></span>$ ./bin/pyspark --master local<span
                            class="o">[</span><span class="m">4</span><span class="o">]</span> --py-files code.py</code></pre>
                </figure>

                <p>For a complete list of options, run <code>pyspark --help</code>. Behind the scenes,
                    <code>pyspark</code> invokes the more general <a href="submitting-applications.html"><code>spark-submit</code>
                        script</a>.</p>

                <p>It is also possible to launch the PySpark shell in <a href="http://ipython.org">IPython</a>, the
                    enhanced Python interpreter. PySpark works with IPython 1.0.0 and later. To
                    use IPython, set the <code>PYSPARK_DRIVER_PYTHON</code> variable to <code>ipython</code> when
                    running <code>bin/pyspark</code>:</p>

                <figure class="highlight">
                    <pre><code class="language-bash" data-lang="bash"><span></span>$ <span class="nv">PYSPARK_DRIVER_PYTHON</span><span
                            class="o">=</span>ipython ./bin/pyspark</code></pre>
                </figure>

                <p>To use the Jupyter notebook (previously known as the IPython notebook),</p>

                <figure class="highlight">
                    <pre><code class="language-bash" data-lang="bash"><span></span>$ <span class="nv">PYSPARK_DRIVER_PYTHON</span><span
                            class="o">=</span>jupyter <span class="nv">PYSPARK_DRIVER_PYTHON_OPTS</span><span class="o">=</span>notebook ./bin/pyspark</code></pre>
                </figure>

                <p>You can customize the <code>ipython</code> or <code>jupyter</code> commands by setting <code>PYSPARK_DRIVER_PYTHON_OPTS</code>.
                </p>

                <p>After the Jupyter Notebook server is launched, you can create a new &#8220;Python 2&#8221; notebook
                    from
                    the &#8220;Files&#8221; tab. Inside the notebook, you can input the command <code>%pylab
                        inline</code> as part of
                    your notebook before you start to try Spark from the Jupyter notebook.</p>

            </div>

        </div>

        <h1 id="resilient-distributed-datasets-rdds">弹性分布式数据集(RDDs)</h1>

        <p>
            Spark围绕弹性分布式数据集(RDD)的概念展开，它是一个容错且可以执行并行操作的元素的集合。创建RDDs有两种方法:在<strong style="color:red">驱动程序</strong>中 parallelizing (并行化)已经存在的集合，或在外部存储系统中引用一个数据集，例如共享文件系统、HDFS、HBase或提供Hadoop
            InputFormat的任何数据源。</p>

        <h2 id="parallelized-collections">Parallelized (并行) 集合</h2>

        <div class="codetabs">

            <div data-lang="scala">

                <p>在<strong style="color:red">驱动程序</strong>中的已经存在的集合(Scala
                    Seq)上通过调用SparkContext的parallelize方法来创建并行集合。集合的元素被复制，形成一个可以并行操作的分布式数据集。例如，下面是如何创建一个并行的集合来保存数字1到5:</p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span
                        class="k">val</span> <span class="n">data</span> <span class="k">=</span> <span
                        class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span
                        class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span
                        class="o">,</span> <span class="mi">4</span><span class="o">,</span> <span
                        class="mi">5</span><span class="o">)</span>
<span class="k">val</span> <span class="n">distData</span> <span class="k">=</span> <span class="n">sc</span><span
                            class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">data</span><span
                            class="o">)</span></code></pre>
                </figure>

                <p>一旦创建，该distributed dataset (分布式数据集) (distData)可以并行执行操作。例如，我们可以调用distData.reduce((a, b) => a +
                    b)用于将数组的元素相加。稍后我们将描述对分布式数据集的操作。</p>

            </div>

            <div data-lang="java">

                <p>Parallelized collections are created by calling <code>JavaSparkContext</code>&#8217;s <code>parallelize</code>
                    method on an existing <code>Collection</code> in your driver program. The elements of the collection
                    are copied to form a distributed dataset that can be operated on in parallel. For example, here is
                    how to create a parallelized collection holding the numbers 1 to 5:</p>

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span
                        class="n">List</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span
                        class="n">data</span> <span class="o">=</span> <span class="n">Arrays</span><span
                        class="o">.</span><span class="na">asList</span><span class="o">(</span><span
                        class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span
                        class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span
                        class="mi">4</span><span class="o">,</span> <span class="mi">5</span><span class="o">);</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span
                            class="n">distData</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span
                            class="na">parallelize</span><span class="o">(</span><span class="n">data</span><span
                            class="o">);</span></code></pre>
                </figure>

                <p>Once created, the distributed dataset (<code>distData</code>) can be operated on in parallel. For
                    example, we might call <code>distData.reduce((a, b) -&gt; a + b)</code> to add up the elements of
                    the list.
                    We describe operations on distributed datasets later on.</p>

            </div>

            <div data-lang="python">

                <p>Parallelized collections are created by calling <code>SparkContext</code>&#8217;s
                    <code>parallelize</code> method on an existing iterable or collection in your driver program. The
                    elements of the collection are copied to form a distributed dataset that can be operated on in
                    parallel. For example, here is how to create a parallelized collection holding the numbers 1 to 5:
                </p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span
                        class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span
                        class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span
                        class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span
                        class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">distData</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span
                            class="n">parallelize</span><span class="p">(</span><span class="n">data</span><span
                            class="p">)</span></code></pre>
                </figure>

                <p>Once created, the distributed dataset (<code>distData</code>) can be operated on in parallel. For
                    example, we can call <code>distData.reduce(lambda a, b: a + b)</code> to add up the elements of the
                    list.
                    We describe operations on distributed datasets later on.</p>

            </div>

        </div>

        <p>并行集合中一个很重要参数是 <strong style="color: red">partitions（分区）的数量，它可用来切割 dataset（数据集）。Spark将为集群的每个分区运行一个任务(分区和任务是1:1关系)。通常，您需要为集群中的每个CPU分配2-3个分区。</strong>通常，Spark尝试根据集群自动设置分区数量。但是，您也可以通过将它作为第二个参数传递给parallelize(例如sc.parallelize(data,
            10))来手动设置它。注意:代码中的一些地方使用术语片(分区的同义词)来维护向后兼容性。</p>
        <h5>-------------------------------------------------------------------------------------------文档扩展-------------------------------------------------------------------------------------------</h5>
        <h5>分区的理解：</h5>
        <p><strong>一、什么是分区以及为什么要分区?
        </strong></p>
        <p>1）在HDFS中数据的物理性存储是以Block数据块的形式存储。</p>
        <p>2）RDD是Spark加载HDFS数据到内存中的抽象，里面不存储数据。RDD是由partion组成的，partion被分布在多个节点上。partion是数据的逻辑存储，实际上不存储数据集。</p>
        <p>3）Spark RDD 是一种分布式的数据集，由于数据量很大，因此要它被切分并存储在各个结点的分区当中。从而当我们对RDD进行操作时，实际上是对每个分区中的数据并行操作。
        </p>
        <p style="text-align: center;">
            <img src="img/partion.png"/>
        </p>
        <p style="text-align: center;">
            <img src="img/relation.png"/>
        </p>
        <p style="text-align: center;">
            <img src="img/shuffle.png"/>
        </p>
        <p><strong>二、分区的3种方式</strong></p>
        <p>1、HashPartitioner</p>
        <pre><code class="language-java">scala&gt; val counts = sc.parallelize(List((1,'a'),(1,'aa'),(2,'b'),(2,'bb'),(3,'c')), 3)
.partitionBy(new HashPartitioner(3))</code></pre>
        <p style="text-align:justify;"><span
                style="color:rgb(51,51,51);font-family:'q_serif', Georgia, Times, 'Times New Roman', 'Hiragino Kaku Gothic Pro', Meiryo, serif;">HashPartitioner确定分区的方式：partition = key.hashCode () % numPartitions</span>
        </p>
        <p style="text-align:justify;"><span
                style="color:rgb(51,51,51);font-family:'q_serif', Georgia, Times, 'Times New Roman', 'Hiragino Kaku Gothic Pro', Meiryo, serif;"><br/></span>
        </p>
        <p>2、RangePartitioner</p>
        <pre><code class="language-java">scala&gt; val counts = sc.parallelize(List((1,'a'),(1,'aa'),(2,'b'),(2,'bb'),(3,'c')), 3)
.partitionBy(new RangePartitioner(3,counts))</code></pre>
        <p>RangePartitioner会对key值进行排序，然后将key值被划分成3份key值集合。</p>
        <p><br/></p>
        <p>3、CustomPartitioner</p>
        <p>CustomPartitioner可以根据自己具体的应用需求，自定义分区。</p>
        <pre><code class="language-java">class CustomPartitioner(numParts: Int) extends Partitioner {
 override def numPartitions: Int = numParts
 override def getPartition(key: Any): Int =
 {
       if(key==1)){
	0
       } else if (key==2){
       1} else{
       2 }
  }
}
scala&gt; val counts = sc.parallelize(List((1,'a'),(1,'aa'),(2,'b'),(2,'bb'),(3,'c')), 3).partitionBy(new CustomPartitioner(3))</code></pre>
        <br/>
        <p><strong>三、理解从HDFS读入文件默认是怎样分区的</strong></p>
        <p style="text-align:left;">Spark从HDFS读入文件的分区数默认等于HDFS文件的块数(blocks)，HDFS<span
                style="font-family:'-apple-system', 'SF UI Text', Arial, 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'WenQuanYi Micro Hei', sans-serif, SimHei, SimSun;background-color:rgb(255,255,255);">中的block是分布式存储的最小单元</span>。如果我们上传一个30GB的非压缩的文件到HDFS，HDFS默认的块容量大小128MB，因此该文件在HDFS上会被分为235块(30GB/128MB)；Spark读取SparkContext.textFile()读取该文件，默认分区数等于块数即235。
        </p>
        <p style="text-align:left;"><br/></p>
        <p><strong>四、如何设置合理的分区数</strong></p>
        <p>1、分区数越多越好吗？</p>
        <p>不是的，分区数太多意味着任务数太多，每次调度任务也是很耗时的，所以分区数太多会导致总体耗时增多。</p>
        <p>2、分区数太少会有什么影响？</p>
        <p>分区数太少的话，会导致一些结点没有分配到任务；另一方面，分区数少则每个分区要处理的数据量就会增大，从而对每个结点的内存要求就会提高；还有分区数不合理，会导致数据倾斜问题。</p>
        <p>3、合理的分区数是多少？如何设置？</p>
        <p>总核数=executor-cores * num-executor </p>
        <p>一般合理的分区数设置为总核数的2~3倍</p>
        <p></p>
        <p></p>

        <h2 id="external-datasets">外部 Datasets</h2>

        <div class="codetabs">

            <div data-lang="scala">

                <p>Spark可以从Hadoop支持的任何存储源创建分布式数据集，包括本地文件系统、HDFS、Cassandra、HBase、<a href="http://wiki.apache.org/hadoop/AmazonS3">Amazon
                    S3</a>等。Spark支持文本文件、<a
                        href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">SequenceFiles</a>和任何其他Hadoop <a
                        href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html">InputFormat</a>。
                </p>

                <p>
                    可以使用 SparkContext 的 textFile 方法来创建文本文件的 RDD。此方法接受文件的URI(机器上的本地路径或hdfs://、s3a://等URI)，并且读取它们作为一个 lines（行）的集合。下面是一个示例调用:
                </p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span
                        class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">distFile</span> <span
                        class="k">=</span> <span class="n">sc</span><span class="o">.</span><span
                        class="n">textFile</span><span class="o">(</span><span
                        class="s">&quot;data.txt&quot;</span><span class="o">)</span>
<span class="n">distFile</span><span class="k">:</span> <span class="kt">org.apache.spark.rdd.RDD</span><span class="o">[</span><span
                            class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="n">data</span><span
                            class="o">.</span><span class="n">txt</span> <span class="nc">MapPartitionsRDD</span><span
                            class="o">[</span><span class="err">10</span><span class="o">]</span> <span
                            class="n">at</span> <span class="n">textFile</span> <span class="n">at</span> <span
                            class="o">&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span
                            class="mi">26</span></code></pre>
                </figure>

                <p>一旦创建，数据集操作就可以对<code>distFile</code>进行操作。例如，我们可以使用map和reduce函数将所有行的大小相加，
                    <code>distFile.map(s =&gt; s.length).reduce((a, b) =&gt; a + b)</code>.</p>

                <p>用Spark读取文件的一些注意事项:</p>

                <ul>
                    <li>
                        <p>如果使用本地文件系统上的路径，则必须在工作节点上的同一路径上访问该文件(也就是说本地系统必须存在这个加载的文件)。要么将文件复制到所有工作者，要么使用一个挂载网络的共享文件系统。</p>
                    </li>
                    <li>
                        <p>Spark所有基于文件的Input (输入) 方法，包括文本文件，支持在目录、压缩文件和通配符上运行。例如，可以使用textFile(“/my/directory”)、textFile(“/my/directory/*.txt”)和textFile(“/my/directory/*.gz”)。</code></p>
                    </li>
                    <li>
                        <p>textFile 方法也可以通过第二个可选的参数来控制该文件的分区数量. 默认情况下, Spark 为文件的每一个 block（块）创建的一 个 partition 分区（HDFS 中块大小默认是 128MB），当然你也可以通过传递一个较大的值来要求一个较高的分区数量。请注意，分区的数量不能够小于块的数量。</p>
                    </li>
                </ul>

                <p>除了文本文件，Spark的Scala API还支持其他几种数据格式:</p>

                <ul>
                    <li>
                        <p>SparkContext。wholeTextFiles允许您读取包含多个小文本文件的目录，并将它们作为(文件名、内容)对返回。这与textFile相反，textFile将在每个文件中每行返回一条记录。分区由数据局部性决定，在某些情况下，数据局部性可能导致分区太少。对于这些情况，wholetextfile提供了控制最小分区数量的第二个可选参数。</p>
                    </li>
                    <li>
                        <p>对于<a
                                href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">SequenceFiles</a>,
                            使用SparkContext的<code>sequenceFile[K, V]</code> 方法，其中<code>K</code>和
                            <code>V</code>是文件中的键和值的类型。这些应该是Hadoop<a
                                    href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/Writable.html">可写接口</a>
                            的子类，比如 <a
                                    href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/IntWritable.html">IntWritable</a>
                            和
                            <a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/Text.html">Text</a>.
                            此外，Spark还允许为一些常见的可写程序指定本机类型;例如，<code>sequenceFile[Int, String]</code> 将自动读取IntWritables和
                            Texts。</p>
                    </li>
                    <li>
                        <p>对于其他Hadoop inputformat，您可以使用<code>SparkContext.hadoopRDD</code> 方法,
                            接受任意的<code>JobConf</code>和输入格式类、键类和值类。
                            将这些设置为与使用输入源的Hadoop作业相同的方式。您还可以使用<code>SparkContext.newAPIHadoopRDD</code>基于
                            MapReduce API (<code>org.apache.hadoop.mapreduce</code>)的输入格式。</p>
                    </li>
                    <li>
                        <p>RDD.saveAsObjectFile和SparkContext.objectFile支持以由序列化的Java对象组成的简单格式保存RDD。虽然这不如Avro这样的专用格式高效，但它提供了一种简单的方式来保存任何RDD。</p>
                    </li>
                </ul>

            </div>

            <div data-lang="java">

                <p>Spark can create distributed datasets from any storage source supported by Hadoop, including your
                    local file system, HDFS, Cassandra, HBase, <a href="http://wiki.apache.org/hadoop/AmazonS3">Amazon
                        S3</a>, etc. Spark supports text files, <a
                            href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">SequenceFiles</a>,
                    and any other Hadoop <a
                            href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html">InputFormat</a>.
                </p>

                <p>Text file RDDs can be created using <code>SparkContext</code>&#8217;s <code>textFile</code> method.
                    This method takes an URI for the file (either a local path on the machine, or a <code>hdfs://</code>,
                    <code>s3a://</code>, etc URI) and reads it as a collection of lines. Here is an example invocation:
                </p>

                <figure class="highlight">
                    <pre><code class="language-java" data-lang="java"><span></span><span class="n">JavaRDD</span><span
                            class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span
                            class="n">distFile</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span
                            class="na">textFile</span><span class="o">(</span><span
                            class="s">&quot;data.txt&quot;</span><span class="o">);</span></code></pre>
                </figure>

                <p>Once created, <code>distFile</code> can be acted on by dataset operations. For example, we can add up
                    the sizes of all the lines using the <code>map</code> and <code>reduce</code> operations as follows:
                    <code>distFile.map(s -&gt; s.length()).reduce((a, b) -&gt; a + b)</code>.</p>

                <p>Some notes on reading files with Spark:</p>

                <ul>
                    <li>
                        <p>If using a path on the local filesystem, the file must also be accessible at the same path on
                            worker nodes. Either copy the file to all workers or use a network-mounted shared file
                            system.</p>
                    </li>
                    <li>
                        <p>All of Spark&#8217;s file-based input methods, including <code>textFile</code>, support
                            running on directories, compressed files, and wildcards as well. For example, you can use
                            <code>textFile("/my/directory")</code>, <code>textFile("/my/directory/*.txt")</code>, and
                            <code>textFile("/my/directory/*.gz")</code>.</p>
                    </li>
                    <li>
                        <p>The <code>textFile</code> method also takes an optional second argument for controlling the
                            number of partitions of the file. By default, Spark creates one partition for each block of
                            the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number
                            of partitions by passing a larger value. Note that you cannot have fewer partitions than
                            blocks.</p>
                    </li>
                </ul>

                <p>Apart from text files, Spark&#8217;s Java API also supports several other data formats:</p>

                <ul>
                    <li>
                        <p><code>JavaSparkContext.wholeTextFiles</code> lets you read a directory containing multiple
                            small text files, and returns each of them as (filename, content) pairs. This is in contrast
                            with <code>textFile</code>, which would return one record per line in each file.</p>
                    </li>
                    <li>
                        <p>For <a
                                href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">SequenceFiles</a>,
                            use SparkContext&#8217;s <code>sequenceFile[K, V]</code> method where <code>K</code> and
                            <code>V</code> are the types of key and values in the file. These should be subclasses of
                            Hadoop&#8217;s <a
                                    href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/Writable.html">Writable</a>
                            interface, like <a
                                    href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/IntWritable.html">IntWritable</a>
                            and
                            <a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/Text.html">Text</a>.
                        </p>
                    </li>
                    <li>
                        <p>For other Hadoop InputFormats, you can use the <code>JavaSparkContext.hadoopRDD</code>
                            method, which takes an arbitrary <code>JobConf</code> and input format class, key class and
                            value class. Set these the same way you would for a Hadoop job with your input source. You
                            can also use <code>JavaSparkContext.newAPIHadoopRDD</code> for InputFormats based on the
                            &#8220;new&#8221; MapReduce API (<code>org.apache.hadoop.mapreduce</code>).</p>
                    </li>
                    <li>
                        <p><code>JavaRDD.saveAsObjectFile</code> and <code>JavaSparkContext.objectFile</code> support
                            saving an RDD in a simple format consisting of serialized Java objects. While this is not as
                            efficient as specialized formats like Avro, it offers an easy way to save any RDD.</p>
                    </li>
                </ul>

            </div>

            <div data-lang="python">

                <p>PySpark can create distributed datasets from any storage source supported by Hadoop, including your
                    local file system, HDFS, Cassandra, HBase, <a href="http://wiki.apache.org/hadoop/AmazonS3">Amazon
                        S3</a>, etc. Spark supports text files, <a
                            href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">SequenceFiles</a>,
                    and any other Hadoop <a
                            href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html">InputFormat</a>.
                </p>

                <p>Text file RDDs can be created using <code>SparkContext</code>&#8217;s <code>textFile</code> method.
                    This method takes an URI for the file (either a local path on the machine, or a <code>hdfs://</code>,
                    <code>s3a://</code>, etc URI) and reads it as a collection of lines. Here is an example invocation:
                </p>

                <figure class="highlight">
                    <pre><code class="language-python" data-lang="python"><span></span><span
                            class="o">&gt;&gt;&gt;</span> <span class="n">distFile</span> <span class="o">=</span> <span
                            class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span
                            class="p">(</span><span class="s2">&quot;data.txt&quot;</span><span
                            class="p">)</span></code></pre>
                </figure>

                <p>Once created, <code>distFile</code> can be acted on by dataset operations. For example, we can add up
                    the sizes of all the lines using the <code>map</code> and <code>reduce</code> operations as follows:
                    <code>distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)</code>.</p>

                <p>Some notes on reading files with Spark:</p>

                <ul>
                    <li>
                        <p>If using a path on the local filesystem, the file must also be accessible at the same path on
                            worker nodes. Either copy the file to all workers or use a network-mounted shared file
                            system.</p>
                    </li>
                    <li>
                        <p>All of Spark&#8217;s file-based input methods, including <code>textFile</code>, support
                            running on directories, compressed files, and wildcards as well. For example, you can use
                            <code>textFile("/my/directory")</code>, <code>textFile("/my/directory/*.txt")</code>, and
                            <code>textFile("/my/directory/*.gz")</code>.</p>
                    </li>
                    <li>
                        <p>The <code>textFile</code> method also takes an optional second argument for controlling the
                            number of partitions of the file. By default, Spark creates one partition for each block of
                            the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number
                            of partitions by passing a larger value. Note that you cannot have fewer partitions than
                            blocks.</p>
                    </li>
                </ul>

                <p>Apart from text files, Spark&#8217;s Python API also supports several other data formats:</p>

                <ul>
                    <li>
                        <p><code>SparkContext.wholeTextFiles</code> lets you read a directory containing multiple small
                            text files, and returns each of them as (filename, content) pairs. This is in contrast with
                            <code>textFile</code>, which would return one record per line in each file.</p>
                    </li>
                    <li>
                        <p><code>RDD.saveAsPickleFile</code> and <code>SparkContext.pickleFile</code> support saving an
                            RDD in a simple format consisting of pickled Python objects. Batching is used on pickle
                            serialization, with default batch size 10.</p>
                    </li>
                    <li>
                        <p>SequenceFile and Hadoop Input/Output Formats</p>
                    </li>
                </ul>

                <p><strong>Note</strong> this feature is currently marked <code>Experimental</code> and is intended for
                    advanced users. It may be replaced in future with read/write support based on Spark SQL, in which
                    case Spark SQL is the preferred approach.</p>

                <p><strong>Writable Support</strong></p>

                <p>PySpark SequenceFile support loads an RDD of key-value pairs within Java, converts Writables to base
                    Java types, and pickles the
                    resulting Java objects using <a href="https://github.com/irmen/Pyrolite/">Pyrolite</a>. When saving
                    an RDD of key-value pairs to SequenceFile,
                    PySpark does the reverse. It unpickles Python objects into Java objects and then converts them to
                    Writables. The following
                    Writables are automatically converted:</p>

                <table class="table">
                    <tr>
                        <th>Writable Type</th>
                        <th>Python Type</th>
                    </tr>
                    <tr>
                        <td>Text</td>
                        <td>unicode str</td>
                    </tr>
                    <tr>
                        <td>IntWritable</td>
                        <td>int</td>
                    </tr>
                    <tr>
                        <td>FloatWritable</td>
                        <td>float</td>
                    </tr>
                    <tr>
                        <td>DoubleWritable</td>
                        <td>float</td>
                    </tr>
                    <tr>
                        <td>BooleanWritable</td>
                        <td>bool</td>
                    </tr>
                    <tr>
                        <td>BytesWritable</td>
                        <td>bytearray</td>
                    </tr>
                    <tr>
                        <td>NullWritable</td>
                        <td>None</td>
                    </tr>
                    <tr>
                        <td>MapWritable</td>
                        <td>dict</td>
                    </tr>
                </table>

                <p>Arrays are not handled out-of-the-box. Users need to specify custom <code>ArrayWritable</code>
                    subtypes when reading or writing. When writing,
                    users also need to specify custom converters that convert arrays to custom
                    <code>ArrayWritable</code> subtypes. When reading, the default
                    converter will convert custom <code>ArrayWritable</code> subtypes to Java <code>Object[]</code>,
                    which then get pickled to Python tuples. To get
                    Python <code>array.array</code> for arrays of primitive types, users need to specify custom
                    converters.</p>

                <p><strong>Saving and Loading SequenceFiles</strong></p>

                <p>Similarly to text files, SequenceFiles can be saved and loaded by specifying the path. The key and
                    value
                    classes can be specified, but for standard Writables this is not required.</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span
                        class="o">&gt;&gt;&gt;</span> <span class="n">rdd</span> <span class="o">=</span> <span
                        class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span
                        class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span
                        class="p">,</span> <span class="mi">4</span><span class="p">))</span><span
                        class="o">.</span><span class="n">map</span><span class="p">(</span><span
                        class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span
                        class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span> <span
                        class="o">*</span> <span class="n">x</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rdd</span><span class="o">.</span><span
                            class="n">saveAsSequenceFile</span><span class="p">(</span><span class="s2">&quot;path/to/file&quot;</span><span
                            class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">sc</span><span
                            class="o">.</span><span class="n">sequenceFile</span><span class="p">(</span><span
                            class="s2">&quot;path/to/file&quot;</span><span class="p">)</span><span
                            class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;a&#39;</span><span
                            class="p">),</span> <span class="p">(</span><span class="mi">2</span><span
                            class="p">,</span> <span class="sa">u</span><span class="s1">&#39;aa&#39;</span><span
                            class="p">),</span> <span class="p">(</span><span class="mi">3</span><span
                            class="p">,</span> <span class="sa">u</span><span class="s1">&#39;aaa&#39;</span><span
                            class="p">)]</span></code></pre>
                </figure>

                <p><strong>Saving and Loading Other Hadoop Input/Output Formats</strong></p>

                <p>PySpark can also read any Hadoop InputFormat or write any Hadoop OutputFormat, for both &#8216;new&#8217;
                    and &#8216;old&#8217; Hadoop MapReduce APIs.
                    If required, a Hadoop configuration can be passed in as a Python dict. Here is an example using the
                    Elasticsearch ESInputFormat:</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span
                        class="err">$</span> <span class="o">./</span><span class="nb">bin</span><span
                        class="o">/</span><span class="n">pyspark</span> <span class="o">--</span><span
                        class="n">jars</span> <span class="o">/</span><span class="n">path</span><span
                        class="o">/</span><span class="n">to</span><span class="o">/</span><span
                        class="n">elasticsearch</span><span class="o">-</span><span class="n">hadoop</span><span
                        class="o">.</span><span class="n">jar</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">conf</span> <span class="o">=</span> <span class="p">{</span><span
                            class="s2">&quot;es.resource&quot;</span> <span class="p">:</span> <span class="s2">&quot;index/type&quot;</span><span
                            class="p">}</span>  <span
                            class="c1"># assume Elasticsearch is running on localhost defaults</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span
                            class="o">.</span><span class="n">newAPIHadoopRDD</span><span class="p">(</span><span
                            class="s2">&quot;org.elasticsearch.hadoop.mr.EsInputFormat&quot;</span><span
                            class="p">,</span>
                             <span class="s2">&quot;org.apache.hadoop.io.NullWritable&quot;</span><span
                            class="p">,</span>
                             <span class="s2">&quot;org.elasticsearch.hadoop.mr.LinkedMapWritable&quot;</span><span
                            class="p">,</span>
                             <span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span
                            class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rdd</span><span class="o">.</span><span class="n">first</span><span
                            class="p">()</span>  <span class="c1"># the result is a MapWritable that is converted to a Python dict</span>
<span class="p">(</span><span class="sa">u</span><span class="s1">&#39;Elasticsearch ID&#39;</span><span
                            class="p">,</span>
 <span class="p">{</span><span class="sa">u</span><span class="s1">&#39;field1&#39;</span><span class="p">:</span> <span
                            class="bp">True</span><span class="p">,</span>
  <span class="sa">u</span><span class="s1">&#39;field2&#39;</span><span class="p">:</span> <span
                            class="sa">u</span><span class="s1">&#39;Some Text&#39;</span><span class="p">,</span>
  <span class="sa">u</span><span class="s1">&#39;field3&#39;</span><span class="p">:</span> <span
                            class="mi">12345</span><span class="p">})</span></code></pre>
                </figure>

                <p>Note that, if the InputFormat simply depends on a Hadoop configuration and/or input path, and
                    the key and value classes can easily be converted according to the above table,
                    then this approach should work well for such cases.</p>

                <p>If you have custom serialized binary data (such as loading data from Cassandra / HBase), then you
                    will first need to
                    transform that data on the Scala/Java side to something which can be handled by Pyrolite&#8217;s
                    pickler.
                    A <a href="api/scala/index.html#org.apache.spark.api.python.Converter">Converter</a> trait is
                    provided
                    for this. Simply extend this trait and implement your transformation code in the
                    <code>convert</code>
                    method. Remember to ensure that this class, along with any dependencies required to access your
                    <code>InputFormat</code>, are packaged into your Spark job jar and included on the PySpark
                    classpath.</p>

                <p>See the <a href="https://github.com/apache/spark/tree/master/examples/src/main/python">Python
                    examples</a> and
                    the
                    <a href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/pythonconverters">Converter
                        examples</a>
                    for examples of using Cassandra / HBase <code>InputFormat</code> and <code>OutputFormat</code> with
                    custom converters.</p>

            </div>
        </div>

        <h2 id="rdd-operations">RDD 操作</h2>

        <p>RDDs支持两种类型的操作: transformations (转换) , 它会在一个已存在的 dataset 上创建一个新的 dataset,和action (行动), 将在 dataset 上运行的计算结果返回到 driver 程序 。例如，map是一个转换算子，它通过一个函数传递每个数据集元素，并返回一个新RDD。另一方面，reduce是一个使用某种函数聚合RDD的所有元素并将最终结果返回给驱动程序的action(尽管还有一个返回分布式数据集的并行reduceByKey)。</p>

        <p>Spark中的所有transformations (转换) 都是 lazy（懒加载的），因为它们不会立即计算结果。相反，<strong style="color: red">他们只记住应用于一些基本数据集(例如文件)的转换</strong>。只有当action执行将结果返回给驱动程序时，才能计算转换。这种设计使Spark运行更加高效。例如，我们可以意识到通过map创建的数据集将用于reduce，并且只将reduce的结果返回给驱动程序，而不是更大的映射数据集。</p>

        <p>默认情况下，<strong style="color: red">每次在RDD上运行action时，都可能重新计算每个转换后的RDD</strong>。但是，您也可以使用持久(或缓存)方法在内存中持久化RDD，在这种情况下，Spark将在集群中保留元素，以便在下一次查询时更快地访问它。还支持在磁盘上持久化RDDs，或跨多个节点进行复制。</p>

        <h3 id="basics">基础</h3>

        <div class="codetabs">

            <div data-lang="scala">

                <p>为了说明RDD的基础知识，请考虑下面的简单程序:</p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span
                        class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span
                        class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span
                        class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">lineLengths</span> <span class="k">=</span> <span class="n">lines</span><span
                            class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">s</span> <span
                            class="k">=&gt;</span> <span class="n">s</span><span class="o">.</span><span class="n">length</span><span
                            class="o">)</span>
<span class="k">val</span> <span class="n">totalLength</span> <span class="k">=</span> <span
                            class="n">lineLengths</span><span class="o">.</span><span class="n">reduce</span><span
                            class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span
                            class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span> <span
                            class="o">+</span> <span class="n">b</span><span class="o">)</span></code></pre>
                </figure>

                <p>第一行从外部文件定义了一个基本的RDD。但这个数据集并未加载到内存中或即将被 action (行动),lines 仅仅是一个类似指针的东西，指向该文件。
                    第二行定义了 lineLengths 作为 map transformation 的结果。请注意，由于 laziness（延迟加载）lineLengths 不会被立即计算。最后，我们运行reduce，这是一个action。Spark 分发计算任务到不同的机器上运行，每台机器都运行在 map 的一部分并本地运行 reduce，仅仅返回它聚合后的结果给驱动程序。</p>

                <p>如果我们以后还想使用linelength，我们可以添加:

                </p>

                <figure class="highlight">
                    <pre><code class="language-scala" data-lang="scala"><span></span><span
                            class="n">lineLengths</span><span class="o">.</span><span class="n">persist</span><span
                            class="o">()</span></code></pre>
                </figure>

                <p>在reduce之前，这会导致linelength在第一次计算后被保存在内存中。

                </p>

            </div>

            <div data-lang="java">

                <p>To illustrate RDD basics, consider the simple program below:</p>

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span
                        class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span
                        class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span
                        class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span
                        class="o">);</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span
                            class="n">lineLengths</span> <span class="o">=</span> <span class="n">lines</span><span
                            class="o">.</span><span class="na">map</span><span class="o">(</span><span
                            class="n">s</span> <span class="o">-&gt;</span> <span class="n">s</span><span
                            class="o">.</span><span class="na">length</span><span class="o">());</span>
<span class="kt">int</span> <span class="n">totalLength</span> <span class="o">=</span> <span
                            class="n">lineLengths</span><span class="o">.</span><span class="na">reduce</span><span
                            class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span
                            class="o">)</span> <span class="o">-&gt;</span> <span class="n">a</span> <span
                            class="o">+</span> <span class="n">b</span><span class="o">);</span></code></pre>
                </figure>

                <p>The first line defines a base RDD from an external file. This dataset is not loaded in memory or
                    otherwise acted on: <code>lines</code> is merely a pointer to the file.
                    The second line defines <code>lineLengths</code> as the result of a <code>map</code> transformation.
                    Again, <code>lineLengths</code>
                    is <em>not</em> immediately computed, due to laziness.
                    Finally, we run <code>reduce</code>, which is an action. At this point Spark breaks the computation
                    into tasks
                    to run on separate machines, and each machine runs both its part of the map and a local reduction,
                    returning only its answer to the driver program.</p>

                <p>If we also wanted to use <code>lineLengths</code> again later, we could add:</p>

                <figure class="highlight">
                    <pre><code class="language-java" data-lang="java"><span></span><span
                            class="n">lineLengths</span><span class="o">.</span><span class="na">persist</span><span
                            class="o">(</span><span class="n">StorageLevel</span><span class="o">.</span><span
                            class="na">MEMORY_ONLY</span><span class="o">());</span></code></pre>
                </figure>

                <p>before the <code>reduce</code>, which would cause <code>lineLengths</code> to be saved in memory
                    after the first time it is computed.</p>

            </div>

            <div data-lang="python">

                <p>To illustrate RDD basics, consider the simple program below:</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span
                        class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span
                        class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;data.txt&quot;</span><span
                        class="p">)</span>
<span class="n">lineLengths</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span
                            class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span
                            class="n">s</span><span class="p">:</span> <span class="nb">len</span><span
                            class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="n">totalLength</span> <span class="o">=</span> <span class="n">lineLengths</span><span
                            class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span
                            class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span
                            class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span></code></pre>
                </figure>

                <p>The first line defines a base RDD from an external file. This dataset is not loaded in memory or
                    otherwise acted on: <code>lines</code> is merely a pointer to the file.
                    The second line defines <code>lineLengths</code> as the result of a <code>map</code> transformation.
                    Again, <code>lineLengths</code>
                    is <em>not</em> immediately computed, due to laziness.
                    Finally, we run <code>reduce</code>, which is an action. At this point Spark breaks the computation
                    into tasks
                    to run on separate machines, and each machine runs both its part of the map and a local reduction,
                    returning only its answer to the driver program.</p>

                <p>If we also wanted to use <code>lineLengths</code> again later, we could add:</p>

                <figure class="highlight">
                    <pre><code class="language-python" data-lang="python"><span></span><span
                            class="n">lineLengths</span><span class="o">.</span><span class="n">persist</span><span
                            class="p">()</span></code></pre>
                </figure>

                <p>before the <code>reduce</code>, which would cause <code>lineLengths</code> to be saved in memory
                    after the first time it is computed.</p>

            </div>

        </div>

        <h3 id="passing-functions-to-spark">向Spark传递函数</h3>

        <div class="codetabs">

            <div data-lang="scala">

                <p>当 driver 程序在集群上运行时，Spark 的 API 在很大程度上依赖于传递函数。有 2 种推荐的方式来做到这一点:
                    </p>

                <ul>
                    <li><a href="http://docs.scala-lang.org/tour/basics.html#functions">匿名函数语法</a>,
                        可用于短代码段。
                    </li>
                    <li>全局单例对象中的静态方法。例如，您可以定义对象myfunction，然后传递MyFunctions.func1,如下所示:
                    </li>
                </ul>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span
                        class="k">object</span> <span class="nc">MyFunctions</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">func1</span><span class="o">(</span><span class="n">s</span><span
                            class="k">:</span> <span class="kt">String</span><span class="o">)</span><span
                            class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span
                            class="o">{</span> <span class="o">...</span> <span class="o">}</span>
<span class="o">}</span>

<span class="n">myRdd</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="nc">MyFunctions</span><span
                            class="o">.</span><span class="n">func1</span><span class="o">)</span></code></pre>
                </figure>

                <p>注意，虽然也可以在类实例中传递对方法的引用(与单例对象相反)，但这需要将包含该类的对象与方法一起发送。例如,考虑:</p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span
                        class="k">class</span> <span class="nc">MyClass</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">func1</span><span class="o">(</span><span class="n">s</span><span
                            class="k">:</span> <span class="kt">String</span><span class="o">)</span><span
                            class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span
                            class="o">{</span> <span class="o">...</span> <span class="o">}</span>
  <span class="k">def</span> <span class="n">doStuff</span><span class="o">(</span><span class="n">rdd</span><span
                            class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span
                            class="o">])</span><span class="k">:</span> <span class="kt">RDD</span><span
                            class="o">[</span><span class="kt">String</span><span class="o">]</span> <span
                            class="k">=</span> <span class="o">{</span> <span class="n">rdd</span><span
                            class="o">.</span><span class="n">map</span><span class="o">(</span><span
                            class="n">func1</span><span class="o">)</span> <span class="o">}</span>
<span class="o">}</span></code></pre>
                </figure>

                <p>在这里，如果我们创建一个新的MyClass实例并在其上调用doStuff，其中的映射引用了MyClass实例的func1方法，因此整个对象需要被发送到集群。这类似于编写rdd。map(x = > this.func1(x))。


                    以类似的方式，访问外部对象的字段将引用整个对象:</p>


                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span
                        class="k">class</span> <span class="nc">MyClass</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">field</span> <span class="k">=</span> <span
                            class="s">&quot;Hello&quot;</span>
  <span class="k">def</span> <span class="n">doStuff</span><span class="o">(</span><span class="n">rdd</span><span
                            class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span
                            class="o">])</span><span class="k">:</span> <span class="kt">RDD</span><span
                            class="o">[</span><span class="kt">String</span><span class="o">]</span> <span
                            class="k">=</span> <span class="o">{</span> <span class="n">rdd</span><span
                            class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span
                            class="k">=&gt;</span> <span class="n">field</span> <span class="o">+</span> <span
                            class="n">x</span><span class="o">)</span> <span class="o">}</span>
<span class="o">}</span></code></pre>
                </figure>

                <p>相当于写rdd.map(x => this.field + x)，它引用了所有这些。为了避免这个问题，最简单的方法是将字段复制到本地变量中，而不是从外部访问它:</p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span
                        class="k">def</span> <span class="n">doStuff</span><span class="o">(</span><span
                        class="n">rdd</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span
                        class="kt">String</span><span class="o">])</span><span class="k">:</span> <span
                        class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span
                        class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">field_</span> <span class="k">=</span> <span class="k">this</span><span
                            class="o">.</span><span class="n">field</span>
  <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span
                            class="n">x</span> <span class="k">=&gt;</span> <span class="n">field_</span> <span
                            class="o">+</span> <span class="n">x</span><span class="o">)</span>
<span class="o">}</span></code></pre>
                </figure>

            </div>

            <div data-lang="java">

                <p>Spark&#8217;s API relies heavily on passing functions in the driver program to run on the cluster.
                    In Java, functions are represented by classes implementing the interfaces in the
                    <a href="api/java/index.html?org/apache/spark/api/java/function/package-summary.html">org.apache.spark.api.java.function</a>
                    package.
                    There are two ways to create such functions:</p>

                <ul>
                    <li>Implement the Function interfaces in your own class, either as an anonymous inner class or a
                        named one,
                        and pass an instance of it to Spark.
                    </li>
                    <li>Use <a href="http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html">lambda
                        expressions</a>
                        to concisely define an implementation.
                    </li>
                </ul>

                <p>While much of this guide uses lambda syntax for conciseness, it is easy to use all the same APIs
                    in long-form. For example, we could have written our code above as follows:</p>

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span
                        class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span
                        class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span
                        class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span
                        class="o">);</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span
                            class="n">lineLengths</span> <span class="o">=</span> <span class="n">lines</span><span
                            class="o">.</span><span class="na">map</span><span class="o">(</span><span
                            class="k">new</span> <span class="n">Function</span><span class="o">&lt;</span><span
                            class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span
                            class="o">&gt;()</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="n">Integer</span> <span class="nf">call</span><span
                            class="o">(</span><span class="n">String</span> <span class="n">s</span><span
                            class="o">)</span> <span class="o">{</span> <span class="k">return</span> <span
                            class="n">s</span><span class="o">.</span><span class="na">length</span><span
                            class="o">();</span> <span class="o">}</span>
<span class="o">});</span>
<span class="kt">int</span> <span class="n">totalLength</span> <span class="o">=</span> <span
                            class="n">lineLengths</span><span class="o">.</span><span class="na">reduce</span><span
                            class="o">(</span><span class="k">new</span> <span class="n">Function2</span><span
                            class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span
                            class="o">,</span> <span class="n">Integer</span><span class="o">&gt;()</span> <span
                            class="o">{</span>
  <span class="kd">public</span> <span class="n">Integer</span> <span class="nf">call</span><span
                            class="o">(</span><span class="n">Integer</span> <span class="n">a</span><span
                            class="o">,</span> <span class="n">Integer</span> <span class="n">b</span><span
                            class="o">)</span> <span class="o">{</span> <span class="k">return</span> <span
                            class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">;</span> <span
                            class="o">}</span>
<span class="o">});</span></code></pre>
                </figure>

                <p>Or, if writing the functions inline is unwieldy:</p>

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span
                        class="kd">class</span> <span class="nc">GetLength</span> <span
                        class="kd">implements</span> <span class="n">Function</span><span class="o">&lt;</span><span
                        class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span
                        class="o">{</span>
  <span class="kd">public</span> <span class="n">Integer</span> <span class="nf">call</span><span
                            class="o">(</span><span class="n">String</span> <span class="n">s</span><span
                            class="o">)</span> <span class="o">{</span> <span class="k">return</span> <span
                            class="n">s</span><span class="o">.</span><span class="na">length</span><span
                            class="o">();</span> <span class="o">}</span>
<span class="o">}</span>
<span class="kd">class</span> <span class="nc">Sum</span> <span class="kd">implements</span> <span
                            class="n">Function2</span><span class="o">&lt;</span><span class="n">Integer</span><span
                            class="o">,</span> <span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span
                            class="o">&gt;</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="n">Integer</span> <span class="nf">call</span><span
                            class="o">(</span><span class="n">Integer</span> <span class="n">a</span><span
                            class="o">,</span> <span class="n">Integer</span> <span class="n">b</span><span
                            class="o">)</span> <span class="o">{</span> <span class="k">return</span> <span
                            class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">;</span> <span
                            class="o">}</span>
<span class="o">}</span>

<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span
                            class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span
                            class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span
                            class="o">);</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span
                            class="n">lineLengths</span> <span class="o">=</span> <span class="n">lines</span><span
                            class="o">.</span><span class="na">map</span><span class="o">(</span><span
                            class="k">new</span> <span class="n">GetLength</span><span class="o">());</span>
<span class="kt">int</span> <span class="n">totalLength</span> <span class="o">=</span> <span
                            class="n">lineLengths</span><span class="o">.</span><span class="na">reduce</span><span
                            class="o">(</span><span class="k">new</span> <span class="n">Sum</span><span
                            class="o">());</span></code></pre>
                </figure>

                <p>Note that anonymous inner classes in Java can also access variables in the enclosing scope as long
                    as they are marked <code>final</code>. Spark will ship copies of these variables to each worker node
                    as it does
                    for other languages.</p>

            </div>

            <div data-lang="python">

                <p>Spark&#8217;s API relies heavily on passing functions in the driver program to run on the cluster.
                    There are three recommended ways to do this:</p>

                <ul>
                    <li><a href="https://docs.python.org/2/tutorial/controlflow.html#lambda-expressions">Lambda
                        expressions</a>,
                        for simple functions that can be written as an expression. (Lambdas do not support
                        multi-statement
                        functions or statements that do not return a value.)
                    </li>
                    <li>Local <code>def</code>s inside the function calling into Spark, for longer code.</li>
                    <li>Top-level functions in a module.</li>
                </ul>

                <p>For example, to pass a longer function than can be supported using a <code>lambda</code>, consider
                    the code below:</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span
                        class="sd">&quot;&quot;&quot;MyScript.py&quot;&quot;&quot;</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span
                            class="p">:</span>
    <span class="k">def</span> <span class="nf">myFunc</span><span class="p">(</span><span class="n">s</span><span
                            class="p">):</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span
                            class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span
                            class="p">)</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span
                            class="n">words</span><span class="p">)</span>

    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span
                            class="o">...</span><span class="p">)</span>
    <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span
                            class="s2">&quot;file.txt&quot;</span><span class="p">)</span><span class="o">.</span><span
                            class="n">map</span><span class="p">(</span><span class="n">myFunc</span><span
                            class="p">)</span></code></pre>
                </figure>

                <p>Note that while it is also possible to pass a reference to a method in a class instance (as opposed
                    to
                    a singleton object), this requires sending the object that contains that class along with the
                    method.
                    For example, consider:</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span
                        class="k">class</span> <span class="nc">MyClass</span><span class="p">(</span><span class="nb">object</span><span
                        class="p">):</span>
    <span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="bp">self</span><span
                            class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">s</span>
    <span class="k">def</span> <span class="nf">doStuff</span><span class="p">(</span><span class="bp">self</span><span
                            class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span
                            class="p">(</span><span class="bp">self</span><span class="o">.</span><span
                            class="n">func</span><span class="p">)</span></code></pre>
                </figure>

                <p>Here, if we create a <code>new MyClass</code> and call <code>doStuff</code> on it, the
                    <code>map</code> inside there references the
                    <code>func</code> method <em>of that <code>MyClass</code> instance</em>, so the whole object needs
                    to be sent to the cluster.</p>

                <p>In a similar way, accessing fields of the outer object will reference the whole object:</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span
                        class="k">class</span> <span class="nc">MyClass</span><span class="p">(</span><span class="nb">object</span><span
                        class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span
                            class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">field</span> <span class="o">=</span> <span
                            class="s2">&quot;Hello&quot;</span>
    <span class="k">def</span> <span class="nf">doStuff</span><span class="p">(</span><span class="bp">self</span><span
                            class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span
                            class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span
                            class="p">:</span> <span class="bp">self</span><span class="o">.</span><span
                            class="n">field</span> <span class="o">+</span> <span class="n">s</span><span
                            class="p">)</span></code></pre>
                </figure>

                <p>To avoid this issue, the simplest way is to copy <code>field</code> into a local variable instead
                    of accessing it externally:</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span
                        class="k">def</span> <span class="nf">doStuff</span><span class="p">(</span><span class="bp">self</span><span
                        class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
    <span class="n">field</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span
                            class="n">field</span>
    <span class="k">return</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span
                            class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span
                            class="p">:</span> <span class="n">field</span> <span class="o">+</span> <span
                            class="n">s</span><span class="p">)</span></code></pre>
                </figure>

            </div>

        </div>

        <h3 id="understanding-closures-">理解闭包<a name="ClosuresLink"></a></h3>
        <p>在集群中执行代码时，一个关于 Spark 更难的事情是理解变量和方法的范围和生命周期。在作用域之外修改变量的RDD操作经常会引起混淆。在下面的示例中，我们将查看使用foreach()来增加计数器的代码，但其他操作也可能出现类似的问题。</p>
        <h5>-------------------------------------------------------------------------------------------文档扩展-------------------------------------------------------------------------------------------</h5>
        <p>什么叫闭包： 跨作用域（即在work节点访问driver节点）访问函数变量。又指的一个拥有许多变量和绑定了这些变量的环境的表达式（通常是一个函数），因而这些变量也是该表达式的一部分。</p>
        <p style="margin-left:0pt;">代码展示：</p>

        <table border="1" cellspacing="0" style="width:477.2pt;"><tbody><tr><td style="vertical-align:top;width:477.2pt;">
            <p style="margin-left:0pt;"><strong><span style="color:#000080;"><strong>def </strong></span></strong><span style="color:#000000;">main(args:Array[</span><span style="color:#1948a6;">String</span><span style="color:#000000;">]):Unit = {</span><br />
                <strong><span style="color:#000080;"><strong>val </strong></span></strong><span style="color:#000000;">conf = </span><strong><span style="color:#000080;"><strong>new </strong></span></strong><span style="color:#000000;">SparkConf().setMaster(</span><strong><span style="color:#658aba;"><strong>"local"</strong></span></strong><span style="color:#000000;">).setAppName(</span><strong><span style="color:#658aba;"><strong>"Test100"</strong></span></strong><span style="color:#000000;">)</span><br />
                <strong><span style="color:#000080;"><strong>val </strong></span></strong><span style="color:#000000;">sc = </span><strong><span style="color:#000080;"><strong>new </strong></span></strong><span style="color:#000000;">SparkContext(conf)</span><br /><br />
                <strong><span style="color:#000080;"><strong>val </strong></span></strong><span style="color:#000000;">rdd = sc.parallelize(</span><em><span style="color:#1948a6;"><em>List</em></span></em><span style="color:#000000;">(</span><span style="color:#0000ff;">1</span><span style="color:#000000;">,</span><span style="color:#0000ff;">2</span><span style="color:#000000;">,</span><span style="color:#0000ff;">3</span><span style="color:#000000;">))</span><br />
                <strong><span style="color:#000080;"><strong>var </strong></span></strong><span style="color:#000000;">counter = </span><span style="color:#0000ff;">0</span><br />
                <em><span style="color:#808080;"><em>//warn: don't do this</em></span></em><br /><em>  </em><span style="color:#000000;">rdd.foreach(x =&gt; counter += x)</span><br />
                <em><span style="color:#000000;"><em>println</em></span></em><span style="color:#000000;">(</span><strong><span style="color:#658aba;"><strong>"Counter value: "</strong></span></strong><span style="color:#000000;">+counter)</span><br /><br /><span style="color:#000000;">  sc.stop()</span><br /><span style="color:#000000;">}</span></p>

            <p style="margin-left:0pt;"> </p>
        </td>
        </tr></tbody></table><p style="margin-left:0pt;"><span style="color:#000000;">问题分析： </span></p>

        <p style="margin-left:0pt;"><span style="color:#000000;">counter是在foreach函数外部定义的，也就是说是在driver程序中定义，而foreach函数是属于RDD的，RDD函数的执行位置为各个worker节点上（或者是在worker进程），main函数是在driver节点上（或者说driver进程上）执行的，所以当counter变量在driver中定义，被RDD函数使用的时候就出现了</span><span style="color:#000000;">”</span><span style="color:#000000;">跨域</span><span style="color:#000000;">”</span><span style="color:#000000;">的问题,也就是闭包问题</span></p>

        <p style="margin-left:0pt;"><span style="color:#000000;">问题解释： </span></p>

        <p style="margin-left:0pt;"><span style="color:#000000;">由于main函数和RDD对象的foreach函数是属于不同</span><span style="color:#000000;">”</span><span style="color:#000000;">闭包</span><span style="color:#000000;">”</span><span style="color:#000000;">的,所以，传进foreach函数的counter是一个副本，初始值都为0。foreach中叠加的是counter的副本，不管副本如何变化，都不会影响到main函数中的counter，所以最终结果还是0。</span></p>
        <p>当用户提交了一个用scala语言写的Spark程序，Spark框架会调用哪些组件呢？首先，这个Spark程序就是一个“Application”，程序里面的mian函数就是“Driver Program”， 前面已经讲到它的作用，只是，dirver程序的可能运行在客户端，也有可有可能运行在spark集群中，这取决于spark作业提交时参数的选定，比如，yarn-client和yarn-cluster就是分别运行在客户端和spark集群中。在driver程序中会有RDD对象的相关代码操作，比如下面代码的newRDD.map()</p>

        <pre class="prettyprint"><code class=" hljs scala"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Test</span>{</span>
  <span class="hljs-keyword">def</span> main(args: Array[String]) {
    <span class="hljs-keyword">val</span> sc = <span class="hljs-keyword">new</span> SparkContext(<span class="hljs-keyword">new</span> SparkConf())
    <span class="hljs-keyword">val</span> newRDD = sc.textFile(<span class="hljs-string">""</span>)

    newRDD.map(data =&gt; {
      <span class="hljs-comment">//do something</span>
      println(data.toString)
    })
  }
}</code></pre>

        <p>涉及到RDD的代码，比如上面RDD的map操作，它们是在Worker节点上面运行的，所以spark会透明地帮用户把这些涉及到RDD操作的代码传给相应的worker节点。如果在RDD map函数中调用了在函数外部定义的对象，因为这些对象需要通过网络从driver所在节点传给其他的worker节点，所以要求这些类是可序列化的，比如在Java或者scala中实现Serializable类，除了java这种序列化机制，还可以选择其他方式，使得序列化工作更加高效。worker节点接收到程序之后，在spark资源管理器的指挥下运行RDD程序。不同worker节点之间的运行操作是并行的。</p>

        <p>在worker节点上所运行的RDD中代码的变量是保存在worker节点上面的，在spark编程中，很多时候用户需要在driver程序中进行相关数据操作之后把该数据传给RDD对象的方法以做进一步处理，这时候，spark框架会自动帮用户把这些数据通过网络传给相应的worker节点。除了这种以变量的形式定义传输数据到worker节点之外，spark还另外提供了两种机制，分别是broadcast和accumulator。相比于变量的方式，在一定场景下使用broadcast比较有优势，因为所广播的数据在每一个worker节点上面只存一个副本，而在spark算子中使用到的外部变量会在每一个用到它的task中保存一个副本，即使这些task在同一个节点上面。所以当数据量比较大的时候，建议使用广播而不是外部变量。</p>
        <h5>----------------------------------------------------------------------------------文档扩展结束-----------------------------------------------------------------------------------</h5>



        <h4 id="example">示例</h4>

        <p>考虑下面简单的RDD元素和，它的行为可能不同，这取决于是否在同一个JVM中执行。一个常见的例子是在本地模式下运行Spark (- master = local[n])，而不是将Spark应用程序部署到集群(例如通过Spark -submit to YARN):</p>

        <div class="codetabs">

            <div data-lang="scala">

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span
                        class="k">var</span> <span class="n">counter</span> <span class="k">=</span> <span
                        class="mi">0</span>
<span class="k">var</span> <span class="n">rdd</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span
                            class="n">parallelize</span><span class="o">(</span><span class="n">data</span><span
                            class="o">)</span>

<span class="c1">// Wrong: Don&#39;t do this!!</span>
<span class="n">rdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">x</span> <span
                            class="k">=&gt;</span> <span class="n">counter</span> <span class="o">+=</span> <span
                            class="n">x</span><span class="o">)</span>

<span class="n">println</span><span class="o">(</span><span class="s">&quot;Counter value: &quot;</span> <span
                            class="o">+</span> <span class="n">counter</span><span class="o">)</span></code></pre>
                </figure>

            </div>

            <div data-lang="java">

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span
                        class="kt">int</span> <span class="n">counter</span> <span class="o">=</span> <span
                        class="mi">0</span><span class="o">;</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span
                            class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span
                            class="o">.</span><span class="na">parallelize</span><span class="o">(</span><span
                            class="n">data</span><span class="o">);</span>

<span class="c1">// Wrong: Don&#39;t do this!!</span>
<span class="n">rdd</span><span class="o">.</span><span class="na">foreach</span><span class="o">(</span><span
                            class="n">x</span> <span class="o">-&gt;</span> <span class="n">counter</span> <span
                            class="o">+=</span> <span class="n">x</span><span class="o">);</span>

<span class="n">println</span><span class="o">(</span><span class="s">&quot;Counter value: &quot;</span> <span
                            class="o">+</span> <span class="n">counter</span><span class="o">);</span></code></pre>
                </figure>

            </div>

            <div data-lang="python">

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span
                        class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span
                            class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Wrong: Don&#39;t do this!!</span>
<span class="k">def</span> <span class="nf">increment_counter</span><span class="p">(</span><span
                            class="n">x</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">counter</span>
    <span class="n">counter</span> <span class="o">+=</span> <span class="n">x</span>
<span class="n">rdd</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">increment_counter</span><span
                            class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Counter value: &quot;</span><span
                            class="p">,</span> <span class="n">counter</span><span class="p">)</span></code></pre>
                </figure>

            </div>

        </div>

        <h4 id="local-vs-cluster-modes">local模式与集群模式</h4>
        上述代码的行为是未定义的，并且可能不按预期工作。为了执行Job，Spark将RDD操作的处理分解为task，每个task由excutor执行。
        闭包是excutor在RDD上执行计算时必须可见的那些变量和方法(在本例中是foreach())。这个闭包被序列化并发送给每个执行程序。</p>

        <p><strong style="color: red">发送到每个执行程序的闭包中的变量现在都是副本</strong>，因此，在foreach函数中引用计数器时，它不再是驱动节点上的计数器。驱动节点的内存中仍然有一个计数器，但是执行程序不再可见!执行器只看到序列化闭包中的副本。因此，计数器的最终值仍然为零，因为计数器上的所有操作都引用了序列化闭包中的值。</p>

        <p>在本地模式中，在某些情况下，foreach函数将实际在与驱动程序相同的JVM中执行，并引用相同的原始计数器，并可能实际更新它。
        </p>

        <p>为了确保这些场景中定义良好的行为，应该使用累加器。Spark中的累加器专门用于在集群中的工作节点之间分割执行时安全地更新变量。本指南的累加器部分更详细地讨论了这些问题。</p>

        <p>一般来说，闭包——像循环或局部定义的方法这样的结构，不应该用来改变某些全局状态。Spark不定义或保证从闭包外部引用对象的突变行为。一些这样做的代码可能在本地模式下工作，但这只是偶然的，这样的代码在分布式模式下不会像预期的那样工作。如果需要一些全局聚合，可以使用累加器。</p>

        <h4 id="printing-elements-of-an-rdd">RDD的打印元素</h4>
        <p>另一个常见的习惯用法是尝试使用RDD .foreach(println)或RDD .map(println)打印RDD的元素。在一台机器上，这将生成预期的输出并打印所有RDD元素。但是，在集群模式下，执行程序调用的stdout的输出现在写到了执行程序的stdout，而不是驱动程序上的，所以驱动程序上的stdout不会显示这些!要打印驱动程序上的所有元素，<strong style="color: red">可以使用collect()方法首先将RDD带到驱动节点</strong>，例如:RDD .collect().foreach(println)。但是，这可能导致驱动程序内存不足，因为collect()将整个RDD提取到一台机器上;如果只需要打印RDD的一些元素，更安全的方法是使用take(): RDD .take(100).foreach(println)。
        </p>

        <h3 id="working-with-key-value-pairs">使用键-值对</h3>

        <div class="codetabs">

            <div data-lang="scala">

                <p>虽然大多数Spark操作都是在包含任何类型对象的RDDs上进行的，但是有一些特殊操作只在键-值对的RDDs上可用。最常见的是分布式的“shuffle”操作，例如通过键对元素进行分组或聚合。</p>

                <p>在Scala中，这些操作在包含
                    <a href="http://www.scala-lang.org/api/2.12.8/index.html#scala.Tuple2">Tuple2</a>对象的
                    RDDs上自动可用(语言中的内置元组，通过简单的编写(a, b))。键-值对操作在类中是可用的，它会自动包装元组的RDD。</p>

                <p>例如，下面的代码使用键-值对上的<code>reduceByKey</code> 操作来计算文件中每行文本出现多少次:</p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span
                        class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span
                        class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span
                        class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">pairs</span> <span class="k">=</span> <span class="n">lines</span><span
                            class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">s</span> <span
                            class="k">=&gt;</span> <span class="o">(</span><span class="n">s</span><span
                            class="o">,</span> <span class="mi">1</span><span class="o">))</span>
<span class="k">val</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">pairs</span><span
                            class="o">.</span><span class="n">reduceByKey</span><span class="o">((</span><span
                            class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span
                            class="k">=&gt;</span> <span class="n">a</span> <span class="o">+</span> <span
                            class="n">b</span><span class="o">)</span></code></pre>
                </figure>

                <p>例如，我们还可以使用<code>counts.sortByKey()</code>按字母顺序排序，最后使用
                    <code>counts.collect()</code>将它们作为对象数组返回给驱动程序。</p>

                <p><strong>注意:</strong>当在键-值对操作中使用自定义对象作为键时，必须确保自定义 <code>equals()</code>法与匹配的<code>hashCode()</code> 方法一起使用。有关详细信息，请参阅
                   <a href="http://docs.oracle.com/javase/7/docs/api/java/lang/Object.html#hashCode()">Object.hashCode()
                       文档中概述的契约。</a></p>

            </div>

            <div data-lang="java">

                <p>While most Spark operations work on RDDs containing any type of objects, a few special operations are
                    only available on RDDs of key-value pairs.
                    The most common ones are distributed &#8220;shuffle&#8221; operations, such as grouping or
                    aggregating the elements
                    by a key.</p>

                <p>In Java, key-value pairs are represented using the
                    <a href="http://www.scala-lang.org/api/2.12.8/index.html#scala.Tuple2">scala.Tuple2</a> class
                    from the Scala standard library. You can simply call <code>new Tuple2(a, b)</code> to create a
                    tuple, and access
                    its fields later with <code>tuple._1()</code> and <code>tuple._2()</code>.</p>

                <p>RDDs of key-value pairs are represented by the
                    <a href="api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html">JavaPairRDD</a> class. You
                    can construct
                    JavaPairRDDs from JavaRDDs using special versions of the <code>map</code> operations, like
                    <code>mapToPair</code> and <code>flatMapToPair</code>. The JavaPairRDD will have both standard RDD
                    functions and special
                    key-value ones.</p>

                <p>For example, the following code uses the <code>reduceByKey</code> operation on key-value pairs to
                    count how
                    many times each line of text occurs in a file:</p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span
                        class="nc">JavaRDD</span><span class="o">&lt;</span><span class="nc">String</span><span
                        class="o">&gt;</span> <span class="n">lines</span> <span class="k">=</span> <span
                        class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span
                        class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">);</span>
<span class="nc">JavaPairRDD</span><span class="o">&lt;</span><span class="nc">String</span><span
                            class="o">,</span> <span class="nc">Integer</span><span class="o">&gt;</span> <span
                            class="n">pairs</span> <span class="k">=</span> <span class="n">lines</span><span class="o">.</span><span
                            class="n">mapToPair</span><span class="o">(</span><span class="n">s</span> <span class="o">-&gt;</span> <span
                            class="k">new</span> <span class="nc">Tuple2</span><span class="o">(</span><span
                            class="n">s</span><span class="o">,</span> <span class="mi">1</span><span
                            class="o">));</span>
<span class="nc">JavaPairRDD</span><span class="o">&lt;</span><span class="nc">String</span><span
                            class="o">,</span> <span class="nc">Integer</span><span class="o">&gt;</span> <span
                            class="n">counts</span> <span class="k">=</span> <span class="n">pairs</span><span
                            class="o">.</span><span class="n">reduceByKey</span><span class="o">((</span><span
                            class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span
                            class="o">-&gt;</span> <span class="n">a</span> <span class="o">+</span> <span
                            class="n">b</span><span class="o">);</span></code></pre>
                </figure>

                <p>We could also use <code>counts.sortByKey()</code>, for example, to sort the pairs alphabetically, and
                    finally
                    <code>counts.collect()</code> to bring them back to the driver program as an array of objects.</p>

                <p><strong>Note:</strong> when using custom objects as the key in key-value pair operations, you must be
                    sure that a
                    custom <code>equals()</code> method is accompanied with a matching <code>hashCode()</code> method.
                    For full details, see
                    the contract outlined in the <a
                            href="http://docs.oracle.com/javase/7/docs/api/java/lang/Object.html#hashCode()">Object.hashCode()
                        documentation</a>.</p>

            </div>

            <div data-lang="python">

                <p>While most Spark operations work on RDDs containing any type of objects, a few special operations are
                    only available on RDDs of key-value pairs.
                    The most common ones are distributed &#8220;shuffle&#8221; operations, such as grouping or
                    aggregating the elements
                    by a key.</p>

                <p>In Python, these operations work on RDDs containing built-in Python tuples such as <code>(1,
                    2)</code>.
                    Simply create such tuples and then call your desired operation.</p>

                <p>For example, the following code uses the <code>reduceByKey</code> operation on key-value pairs to
                    count how
                    many times each line of text occurs in a file:</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span
                        class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span
                        class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;data.txt&quot;</span><span
                        class="p">)</span>
<span class="n">pairs</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span
                            class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span
                            class="n">s</span><span class="p">:</span> <span class="p">(</span><span
                            class="n">s</span><span class="p">,</span> <span class="mi">1</span><span
                            class="p">))</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span
                            class="n">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span
                            class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span
                            class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span></code></pre>
                </figure>

                <p>We could also use <code>counts.sortByKey()</code>, for example, to sort the pairs alphabetically, and
                    finally
                    <code>counts.collect()</code> to bring them back to the driver program as a list of objects.</p>

            </div>

        </div>

        <h3 id="transformations">转换</h3>

        <p>下表列出了Spark支持的一些常见转换。有关详细信息，请参阅RDD API doc
            (<a href="api/scala/index.html#org.apache.spark.rdd.RDD">Scala</a>,
            <a href="api/java/index.html?org/apache/spark/api/java/JavaRDD.html">Java</a>,
            <a href="api/python/pyspark.html#pyspark.RDD">Python</a>,
            <a href="api/R/index.html">R</a>)
            和pair RDD 方法文档
            (<a href="api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions">Scala</a>,
            <a href="api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html">Java</a>)
            。</p>

        <table class="table">
            <tr>
                <th style="width:25%">Transformation</th>
                <th>Meaning</th>
            </tr>
            <tr>
                <td><b>map</b>(<i>func</i>)</td>
                <td> 在Map中，我们会传入一个<i>func</i>，该函数对每个输入经过func处理后都会返回一个输出。是1:1的关系。<br>
                     如:map函数每次输入一行，经过func处理后，输出处理后的一行。

                </td>
            </tr>
            <tr>
                <td><b>filter</b>(<i>func</i>)</td>
                <td> 参数是函数，函数会过滤掉不符合条件的元素，返回值是新的RDD。
                </td>
            </tr>
            <tr>
                <td><b>flatMap</b>(<i>func</i>)</td>
                <td> 是map的一种扩展。在flatMap中，我们会传入一个函数，该函数对每个输入都会返回一个数组或者元组、集合,字符串的输出（而不是一个元素），然后，flatMap把生成的多个集合（数组）“拍扁”成为一个集合(数组)或者字符串RDD。
                </td>
            </tr>
            <tr>
                <td><b>mapPartitions</b>(<i>func</i>) <a name="MapPartLink"></a></td>
                <td> 与map类似，遍历的单位是每个partition上的数据。
                </td>
            </tr>
            <tr>
                <td><b>mapPartitionsWithIndex</b>(<i>func</i>)</td>
                <td>与mapPartitions类似，但是mapPartitionsWithIndex含有分区的索引。
                </td>
            </tr>
            <tr>
                <td><b>sample</b>(<i>withReplacement</i>, <i>fraction</i>, <i>seed</i>)</td>
                <td>随机抽样算子，withReplacement参数：是否有放回的抽样，true为放回抽样。<br></br>
                    fraction参数:表示抽样比例，一个大于0,小于或等于1的小数值,用于控制要读取的数据所占整个数据集的概率.
                    seed:随机种子数。通常情况下seed不指定的话默认随机生成。给定一个默认值，使得下次抽取的数据相同。
                </td>
            </tr>
            <tr>
                <td><b>union</b>(<i>otherDataset</i>)</td>
                <td>是数据合并，返回一个新的数据集，由原数据集和otherDataset联合而成。
                </td>
            </tr>
            <tr>
                <td><b>intersection</b>(<i>otherDataset</i>)</td>
                <td> 参数是RDD，求出两个RDD的共同元素。
                </td>
            </tr>
            <tr>
                <td><b>distinct</b>([<i>numPartitions</i>]))</td>
                <td>将RDD里的元素进行去重操作。
           </tr>
            <tr>
                <td><b>groupByKey</b>([<i>numPartitions</i>]) <a name="GroupByLink"></a></td>
                <td> 当调用(K, V)对的数据集时，返回(K, Iterable)对的数据集。
                    注意:如果您是为了在每个键上执行聚合(例如求和或平均值)而进行分组，那么使用reduceByKey或aggregateByKey将获得更好的性能。
                    注意:默认情况下，输出中的并行程度取决于父RDD的分区数量。您可以传递一个可选的numPartitions参数来设置不同数量的任务。
                </td>
            </tr>
            <tr>
                <td><b>reduceByKey</b>(<i>func</i>, [<i>numPartitions</i>]) <a name="ReduceByLink"></a></td>
                <td> 当调用(K、V)的数据集对,返回一个数据集(K、V)对每个键的值在哪里聚合使用给定reduce function func,必须(V,V)= > V型像groupByKey,减少任务的数量通过一个可选的第二个参数是可配置的。
                </td>
            </tr>
            <tr>
                <td><b>aggregateByKey</b>(<i>zeroValue</i>)(<i>seqOp</i>, <i>combOp</i>, [<i>numPartitions</i>]) <a
                        name="AggregateByLink"></a></td>
                <td> 当调用（K，V）对的数据集时，返回（K，U）对的数据集。对PairRDD中相同的Key值进行聚合操作，在聚合过程中使用给定的组合函数(sqpOp和comOp)和“zeroValue”值(初始化值)聚合每个键的值。 aggregateByKey最终返回值的类型不需要和最初RDD中value的类型一致。 与<code>groupByKey</code>类似，reduce任务的数量可通过可选的第二个参数进行配置。
<br>
                    执行过程：<br>
                    1) 将每个分区内key相同数据放到一起 (按照key进行聚合)。<br>
                    2) 对每个分区中相同key的value(同时初始化值作为每个key中的value一部分)执行seqOp函数。<br>
                    3) 整体聚合(将2)过程的结果进一步执行combOp函数；这个时候初始化值不在参与。<br>
                </td>
            </tr>
            <tr>
                <td><b>sortByKey</b>([<i>ascending</i>], [<i>numPartitions</i>]) <a name="SortByLink"></a></td>
                <td> 作用在K,V格式的RDD上，对key进行升序或者降序排序.
                </td>
            </tr>
            <tr>
                <td><b>join</b>(<i>otherDataset</i>, [<i>numPartitions</i>]) <a name="JoinLink"></a></td>
                <td> 当调用类型为(K, V)和(K, W)的数据集时，返回一个(K， (V, W))对的数据集，每个键对应的所有元素对。外部连接通过左侧连接、右侧连接和fullOuterJoin得到支持。
                </td>
            </tr>
            <tr>
                <td><b>cogroup</b>(<i>otherDataset</i>, [<i>numPartitions</i>]) <a name="CogroupLink"></a></td>
                <td> 当调用类型为(K, V)和(K, W)的数据集时，返回一个(K， (Iterable， Iterable)元组的数据集。这个操作也称为groupWith。
                </td>
            </tr>
            <tr>
                <td><b>cartesian</b>(<i>otherDataset</i>)</td>
                <td>当调用类型T和U的数据集时，返回(T, U)对(所有元素对)的数据集。
                </td>
            </tr>
            <tr>
                <td><b>pipe</b>(<i>command</i>, <i>[envVars]</i>)</td>
                <td> Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements
                    are written to the
                    process's stdin and lines output to its stdout are returned as an RDD of strings.
                </td>
            </tr>
            <tr>
                <td><b>coalesce</b>(<i>numPartitions</i>,<i>boolean</i> ) <a name="CoalesceLink"></a></td>
                <td>将RDD中的分区数量减少到numpartition,第二个参数 表示是否产生shuffle。用于在过滤大型数据集后更有效地运行操作。
                </td>
            </tr>
            <tr>
                <td><b>repartition</b>(<i>numPartitions</i>)</td>
                <td> 重新洗牌RDD中的数据，随机创建更多或更少的分区，并在它们之间进行平衡。这总是在网络上打乱所有的数据。<a name="RepartitionLink"></a></td>
            </tr>
            <tr>
                <td><b>repartitionAndSortWithinPartitions</b>(<i>partitioner</i>) <a name="Repartition2Link"></a></td>
                <td> 根据给定的分区器重新分区RDD，并在每个生成的分区中，按键对记录进行排序。这比调用repartition并在每个分区中进行排序更有效，因为它可以将排序推入shuffle机制。
                </td>
            </tr>
        </table>

        <h3 id="actions">行动算子</h3>

        <p>下表列出了一些 Spark 常用的 actions 操作。参考
            RDD API 文档
            (<a href="api/scala/index.html#org.apache.spark.rdd.RDD">Scala</a>,
            <a href="api/java/index.html?org/apache/spark/api/java/JavaRDD.html">Java</a>,
            <a href="api/python/pyspark.html#pyspark.RDD">Python</a>,
            <a href="api/R/index.html">R</a>)</p>

        <p>和 pair RDD 函数文档
            (<a href="api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions">Scala</a>,
            <a href="api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html">Java</a>)
            for details.</p>

        <table class="table">
            <tr>
                <th>Action</th>
                <th>Meaning</th>
            </tr>
            <tr>
                <td><b>reduce</b>(<i>func</i>)</td>
                <td> 根据聚合逻辑聚合数据集中的每个元素。
                </td>
            </tr>
            <tr>
                <td><b>collect</b>()</td>
                <td> 在驱动程序中以数组的形式返回数据集的所有元素。这通常在filter或返回足够小的数据子集的其他操作之后有用。
                    返回RDD所有元素,将计算结果回收到Driver端(适合数据量比较小的情形)。
                </td>
            </tr>
            <tr>
                <td><b>count</b>()</td>
                <td>返回数据集元素的数量</td>
            </tr>
            <tr>
                <td><b>first</b>()</td>
                <td>返回数据集对的第一个元素(类似于take(1)).</td>
            </tr>
            <tr>
                <td><b>take</b>(<i>n</i>)</td>
                <td>返回数据集对的前<i>n</i>个元素</td>
            </tr>
            <tr>
                <td><b>takeSample</b>(<i>withReplacement</i>, <i>num</i>, [<i>seed</i>])</td>
                <td>从RDD里返回任意一些元素（结果类型为数组）;到底返回多少由第二个参数指定。<br>
                    参数一:withReplacement 是否放。<br>
                    参数二:num 返回多少个数据元素。<br>
                    参数三：seed 每次进行获取数据时候，种子不变，那么得到的结果不变。<br>
                </td>
            </tr>
            <tr>
                <td><b>takeOrdered</b>(<i>n</i>, <i>[ordering]</i>)</td>
                <td>使用其自然顺序或自定义比较器返回RDD的前n个元素。
                </td>
            </tr>
            <tr>
                <td><b>saveAsTextFile</b>(<i>path</i>)</td>
                <td> 将数据集的元素写入本地文件系统、HDFS或任何hadoop支持的文件系统中给定目录中的文本文件(或一组文本文件)。Spark将在每个元素上调用toString，将其转换为文件中的一行文本。
                </td>
            </tr>
            <tr>
                <td><b>saveAsSequenceFile</b>(<i>path</i>) <br/> (Java and Scala)</td>
                <td> 在本地文件系统、HDFS或任何Hadoop支持的文件系统中的给定路径中，将数据集的元素写成Hadoop SequenceFile。这在实现Hadoop可写接口的键-值对的RDDs上是可用的。在Scala中，它还可以用于隐式转换为可写的类型(Spark包含基本类型的转换，如Int、Double、String等)。
                </td>
            </tr>
            <tr>
                <td><b>saveAsObjectFile</b>(<i>path</i>) <br/> (Java and Scala)</td>
                <td> 使用Java序列化以简单格式编写数据集的元素，然后可以使用
                    <code>SparkContext.objectFile()</code>加载这些元素。
                </td>
            </tr>
            <tr>
                <td><b>countByKey</b>() <a name="CountByLink"></a></td>
                <td>只在类型为(K, V)的RDDs上可用。统计一个RDD中各个key的出现次数。返回一个map，map的key是元素的值，value是出现的次数。


                </td>
            </tr>
            <tr>
                <td><b>foreach</b>(<i>func</i>)</td>
                <td>循环遍历数据集中的每个元素，运行相应的逻辑。

                    注意:修改foreach()之外的累加器以外的变量可能会导致未定义的行为。有关更多细节，请参阅理解闭包。
                </td>
            </tr>
        </table>


        <h3 id="shuffle-operations">Shuffle操作</h3>

        <p>Spark中的某些操作会触发一个称为shuffle的事件。shuffle是Spark的一种重新分配数据的机制，这样数据就可以在不同分区之间进行分组。这通常涉及在 executors 和 机器之间拷贝数据，使shuffle成为一个复杂且昂贵的操作。</p>

        <h4 id="background">背景</h4>

        <p>要理解在shuffle期间发生了什么，我们可以考虑reduceByKey操作的示例。reduceByKey操作生成一个新的RDD，其中单个键的所有值组合成一个元组——键和对与该键相关的所有值执行reduce函数的结果。挑战在于，单个键的所有值不一定都驻留在同一个分区，甚至是同一台机器上，但它们必须共存才能计算结果。</p>

        <p>在Spark中，数据通常不会跨分区分布，以便在特定操作的必要位置分布。在计算过程中，单个任务将在单个分区上操作——因此，为了组织要执行的单个reduceByKey reduce任务的所有数据，Spark需要执行all-to-all操作。它必须从所有分区中读取，以找到所有键的所有值，然后将跨分区的值组合在一起，以计算每个键的最终结果——这称为shuffle。</p>

        <p>虽然新打乱的数据的每个分区中的元素集合是确定的，分区本身的排序也是确定的，但是这些元素的排序是不确定的。如果你想在shuffle之后得到可预测的有序数据，那么就可以使用:</p>

        <ul>
            <li>例如.sort，对每个分区进行排序</li>
            <li>repartitionandsortwithinpartition用于高效地对分区进行排序，同时重新分区

            </li>
        </ul>

        <p>产生shuffle的操作包括<strong>repartition</strong>操作
            <a href="#RepartitionLink"><code>repartition</code></a> and <a
                    href="#CoalesceLink"><code>coalesce</code></a>, <strong>&#8216;ByKey</strong> 操作
            (除了计数外)像<a href="#GroupByLink"><code>groupByKey</code></a>和 <a
                    href="#ReduceByLink"><code>reduceByKey</code></a>, 以及
            <strong>join</strong>操作像<a href="#CogroupLink"><code>cogroup</code></a>和<a
                    href="#JoinLink"><code>join</code></a>.</p>

        <h4 id="performance-impact">Performance Impact</h4>
        <p>TShuffle是一种昂贵的操作，因为它涉及到磁盘I/O、数据序列化和网络I/O。
            要组织shuffle的数据，Spark生成一组任务——map任务来组织数据，以及一组reduce任务来聚合数据。这个术语来自MapReduce，与Spark的map和reduce操作没有直接关系。
        </p>

        <p>在内部，单个map任务的结果会保存在内存中，直到无法匹配为止。然后，根据目标分区对它们进行排序，并将它们写入单个文件。在reduce方面，任务读取相关的排序块</p>

        <p>某些shuffle操作可能会消耗大量的堆内存，因为它们使用内存中的数据结构来组织在传输记录之前或之后的记录。具体来说，reduceByKey和aggregateByKey在map端创建这些结构，ByKey操作在reduce端生成这些结构。当数据不适合内存时，Spark会将这些表泄漏到磁盘上，从而增加磁盘I/O的额外开销，并增加垃圾收集。</p>

        <p>Shuffle还会在磁盘上生成大量的中间文件。在Spark 1.3中，这些文件被保存，直到相应的RDDs不再使用并被垃圾收集。这样做是为了在重新计算沿袭时不需要重新创建shuffle文件。如果应用程序保留对这些RDDs的引用，或者GC不频繁启动，那么垃圾收集可能在很长一段时间之后才会发生。这意味着长时间运行的Spark作业可能会消耗大量磁盘空间。临时存储目录由spark.local指定。配置Spark上下文时的dir配置参数。</p>

        <p>可以通过调整各种配置参数来调整Shuffle行为。参见Spark配置指南中的“Shuffle行为”部分。        </p>

        <h2 id="rdd-persistence">RDD 持久化</h2>

        <p>Spark最重要的功能之一是跨操作在内存中持久化(或缓存)数据集。当您持久化一个RDD时，每个节点将它在内存中计算的任何分区存储起来，并在该数据集(或从该数据集派生的数据集)上的其他操作中重用它们。这使得未来的动作更快(通常超过10倍)。缓存是迭代算法和快速交互使用的关键工具。  </p>

        <p您可以使用persist()或cache()方法将RDD标记为持久化。第一次在操作中计算它时，它将保存在节点上的内存中。Spark缓存是容错的——如果RDD的任何分区丢失，它将使用最初创建它的转换自动重新计算。</p>

        <p>此外，每个持久化的RDD都可以使用不同的存储级别进行存储，例如，允许您在磁盘上持久化数据集，在内存中持久化数据集，但是作为序列化的Java对象(以节省空间)，跨节点复制数据集。这些级别是通过传递一个StorageLevel对象(<a
                    href="api/scala/index.html#org.apache.spark.storage.StorageLevel">Scala</a>,
            <a href="api/java/index.html?org/apache/spark/storage/StorageLevel.html">Java</a>,
            <a href="api/python/pyspark.html#pyspark.StorageLevel">Python</a>)
            to <code>persist()</code>. cache()方法是使用默认存储级别StorageLevel的简写方式。MEMORY_ONLY(在内存中存储反序列化的对象)。全部存储级别为:</p>

        <table class="table">
            <tr>
                <th style="width:23%">Storage Level</th>
                <th>Meaning</th>
            </tr>
            <tr>
                <td> MEMORY_ONLY</td>
                <td> Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some
                    partitions will
                    not be cached and will be recomputed on the fly each time they're needed. This is the default level.
                </td>
            </tr>
            <tr>
                <td> MEMORY_AND_DISK</td>
                <td> Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the
                    partitions that don't fit on disk, and read them from there when they're needed.
                </td>
            </tr>
            <tr>
                <td> MEMORY_ONLY_SER <br/> (Java and Scala)</td>
                <td> Store RDD as <i>serialized</i> Java objects (one byte array per partition).
                    This is generally more space-efficient than deserialized objects, especially when using a
                    <a href="tuning.html">fast serializer</a>, but more CPU-intensive to read.
                </td>
            </tr>
            <tr>
                <td> MEMORY_AND_DISK_SER <br/> (Java and Scala)</td>
                <td> Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of
                    recomputing them on the fly each time they're needed.
                </td>
            </tr>
            <tr>
                <td> DISK_ONLY</td>
                <td> Store the RDD partitions only on disk.</td>
            </tr>
            <tr>
                <td> MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.</td>
                <td> Same as the levels above, but replicate each partition on two cluster nodes.</td>
            </tr>
            <tr>
                <td> OFF_HEAP (experimental)</td>
                <td> Similar to MEMORY_ONLY_SER, but store the data in
                    <a href="configuration.html#memory-management">off-heap memory</a>. This requires off-heap memory to
                    be enabled.
                </td>
            </tr>
        </table>

        <p><strong>注意:</strong> <em>在Python中，存储的对象总是使用Pickle库进行序列化，因此选择序列化级别并不重要。Python中可用的存储级别包括
            <code>MEMORY_ONLY</code>, <code>MEMORY_ONLY_2</code>,
            <code>MEMORY_AND_DISK</code>, <code>MEMORY_AND_DISK_2</code>, <code>DISK_ONLY</code>, and
            <code>DISK_ONLY_2</code>.</em></p>

        <p>Spark还会在shuffle操作中自动持久化一些中间数据(例如reduceByKey)，甚至在没有用户调用persist的情况下也是如此。这样做是为了避免在转移期间节点失败时重新计算整个输入。如果用户打算重用生成的RDD，我们仍然建议他们调用持久化。</p>

        <h3 id="which-storage-level-to-choose">选择哪个存储级别?</h3>

        <p>Spark的存储级别是为了在内存使用和CPU效率之间提供不同的权衡。我们建议通过以下过程选择一个:</p>

        <ul>
            <li>
                <p>如果您的RDDs非常适合默认存储级别(MEMORY_ONLY)，那么就让它这样吧。这是最有效的cpu选项，允许RDDs上的操作以尽可能快的速度运行。
                </p>
            </li>
            <li>
                <p>如果没有，请尝试使用MEMORY_ONLY_SER并选择一个快速序列化库，以使对象更节省空间，但访问速度仍然相当快。(Java和Scala)
                </p>
            </li>
            <li>
                <p>不要泄漏到磁盘，除非计算数据集的函数很昂贵，或者它们过滤了大量数据。否则，重新计算分区可能与从磁盘读取分区一样快。</p>
            </li>
            <li>
                <p>如果需要快速的故障恢复(例如，如果使用Spark服务于web应用程序的请求)，则使用复制存储级别。通过重新计算丢失的数据，所有存储级别都提供了完全的容错能力，但是复制的存储级别允许您在RDD上继续运行任务，而无需等待重新计算丢失的分区。</p>
            </li>
        </ul>

        <h3 id="removing-data">删除数据(删除的是缓存的数据)

        </h3>

        <p>Spark自动监视每个节点上的缓存使用情况，并以最近最少使用的方式删除旧数据分区。如果您想手动删除一个RDD，而不是等待它从缓存中退出，请使用RDD.unpersist()方法。</p>

        <h1 id="shared-variables">共享变量</h1>

        <p>通常，在向 Spark 传递函数时,比如使用 map() 函数或 reduce() 函数在远程集群节点上执行时，它在函数中使用的所有变量是驱动器程序中定义的变量。但是集群中（节点）运行的每个任务（一个节点可能多个任务）都会得到这些变量的一份新的副本，更新这些副本的值也不会影响驱动器中的对应变量。但是，Spark 的两个共享变量，累加器与广播变量，分别为结果聚合与广播这两种常见的通信模式突破了这一限制。</p>

        <h2 id="broadcast-variables">广播变量</h2>

        <p>广播变量允许程序员将只读变量缓存在每台机器上，而不是将它的副本与任务一起发送出去（每台节点可能有多个task，也就意味着多个副本）。例如，可以使用它们以高效的方式为每个节点提供一个大型输入数据集的副本(即：一台机器一个广播变量而不是每台机器中多个任务多个副本)。Spark还尝试使用高效的广播算法来分配广播变量，以降低通信成本。</p>

        <p>Spark操作通过一组阶段执行，由分布式“shuffle”操作分隔。Spark自动广播任务在每个阶段所需的公共数据。以这种方式广播的数据在运行每个任务之前以序列化的形式缓存和反序列化。这意味着，只有当跨多个阶段的任务需要相同的数据或以反序列化的形式缓存数据时，显式地创建广播变量才有用。</p>
         <h4>-----------------------------------------------------文档扩展-----------------------------------------------------</h4>
        用一段代码来更直观的解释： <br>
        <img src="img/brocast_sample01.png">
        <p> list是在driver端创建的，但是因为需要在excutor端使用，所以driver会把list以task的形式发送到excutor端，如果有很多个task，就会有很多给excutor端携带很多个list，如果这个list非常大的时候，就可能会造成内存溢出（如下图所示）。这个时候就引出了广播变量。 <p>
        <img src="img/brocast_sample02.png">
        <p>使用广播变量后：</p>
        <img src="img/brocast_sample03.png"><br>
        使用广播变量的过程很简单：<br>

        (1) 通过对一个类型 T 的对象调用 SparkContext.broadcast 创建出一个 Broadcast[T] 对象。任何可序列化的类型都可以这么实现。<br>

        (2) 通过 value 属性访问该对象的值（在 Java 中为 value() 方法）。<br>

        (3) 变量只会被发到各个节点一次，应作为只读值处理。<br>
        注意事项：<br>

        能不能将一个RDD使用广播变量广播出去？<br>

        不能，因为RDD是不存储数据的。可以将RDD的结果广播出去。<br>

        广播变量只能在Driver端定义，不能在Executor端定义。<br>

        在Driver端可以修改广播变量的值，在Executor端无法修改广播变量的值。<br>



        <p style="color: red">Broadcast变量是通过调用SparkContext.broadcast(v)从变量v中创建的。broadcast变量是v的包装器，它的值可以通过调用value方法来访问。下面的代码显示如下:</p>

        <div class="codetabs">

            <div data-lang="scala">

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span
                        class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">broadcastVar</span> <span
                        class="k">=</span> <span class="n">sc</span><span class="o">.</span><span
                        class="n">broadcast</span><span class="o">(</span><span class="nc">Array</span><span
                        class="o">(</span><span class="mi">1</span><span class="o">,</span> <span
                        class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">))</span>
<span class="n">broadcastVar</span><span class="k">:</span> <span class="kt">org.apache.spark.broadcast.Broadcast</span><span
                            class="o">[</span><span class="kt">Array</span><span class="o">[</span><span
                            class="kt">Int</span><span class="o">]]</span> <span class="k">=</span> <span class="nc">Broadcast</span><span
                            class="o">(</span><span class="mi">0</span><span class="o">)</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">broadcastVar</span><span class="o">.</span><span
                            class="n">value</span>
<span class="n">res0</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span
                            class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Array</span><span
                            class="o">(</span><span class="mi">1</span><span class="o">,</span> <span
                            class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span
                            class="o">)</span></code></pre>
                </figure>

            </div>

            <div data-lang="java">

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span
                        class="n">Broadcast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">[]&gt;</span> <span
                        class="n">broadcastVar</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span
                        class="na">broadcast</span><span class="o">(</span><span class="k">new</span> <span class="kt">int</span><span
                        class="o">[]</span> <span class="o">{</span><span class="mi">1</span><span
                        class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span
                        class="mi">3</span><span class="o">});</span>

<span class="n">broadcastVar</span><span class="o">.</span><span class="na">value</span><span class="o">();</span>
<span class="c1">// returns [1, 2, 3]</span></code></pre>
                </figure>

            </div>

            <div data-lang="python">

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span
                        class="o">&gt;&gt;&gt;</span> <span class="n">broadcastVar</span> <span class="o">=</span> <span
                        class="n">sc</span><span class="o">.</span><span class="n">broadcast</span><span
                        class="p">([</span><span class="mi">1</span><span class="p">,</span> <span
                        class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="o">&lt;</span><span class="n">pyspark</span><span class="o">.</span><span class="n">broadcast</span><span
                            class="o">.</span><span class="n">Broadcast</span> <span class="nb">object</span> <span
                            class="n">at</span> <span class="mh">0x102789f10</span><span class="o">&gt;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">broadcastVar</span><span class="o">.</span><span
                            class="n">value</span>
<span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span
                            class="p">,</span> <span class="mi">3</span><span class="p">]</span></code></pre>
                </figure>

            </div>

        </div>

        <p>在创建广播变量之后，应该在集群上运行的任何函数中使用它，这样v就不会被多次发送到节点。此外，对象v在广播后不应该进行修改，以确保所有节点都能获得广播变量的相同值(例如，如果稍后将该变量发送到新节点)。</p>

        <h2 id="accumulators">累加器</h2>

        <p>Accumulators（累加器）是一个仅可以执行 “added”（添加）的变量来通过一个关联和交换操作，因此可以高效地执行支持并行。累加器可以用于实现 counter（ 计数，类似在 MapReduce 中那样）或者 sums（求和）。原生 Spark 支持数值型的累加器，并且程序员可以添加新的支持类型。</p>

        <p>作为一个用户，您可以创建 accumulators（累加器）并且重命名. 如下图所示, 一个命名的 accumulator 累加器（在这个例子中是 counter）将显示在 web UI 中，用于修改该累加器的阶段。 Spark 在 “Tasks” 任务表中显示由任务修改的每个累加器的值。</p>

        <p style="text-align: center;">
            <img src="img/spark-webui-accumulators.png" title="Accumulators in the Spark UI"
                 alt="Accumulators in the Spark UI"/>
        </p>

        <p>在UI中跟踪累加器对于理解运行阶段的进展很有用(注意:Python中还不支持这种方法)。</p>

        <div class="codetabs">

            <div data-lang="scala">

                <p>可以通过调用SparkContext.longAccumulator()或SparkContext.doubleAccumulator()来分别累积Long或Double类型的值来创建一个数值累加器。然后，集群上正在运行的任务就可以使用 add 方法来累计数值。然而，他们无法读它的值。只有driver program (驱动程序)可以使用累加器的 value 方法读取累加器的值。


                    </p>

                <p>下面的代码显示了一个accumulator （累加器），被用于对一个数组中的元素求和:

                </p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span
                        class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">accum</span> <span
                        class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">longAccumulator</span><span
                        class="o">(</span><span class="s">&quot;My Accumulator&quot;</span><span class="o">)</span>
<span class="n">accum</span><span class="k">:</span> <span class="kt">org.apache.spark.util.LongAccumulator</span> <span
                            class="o">=</span> <span class="nc">LongAccumulator</span><span class="o">(</span><span
                            class="n">id</span><span class="k">:</span> <span class="err">0</span><span
                            class="o">,</span> <span class="n">name</span><span class="k">:</span> <span
                            class="kt">Some</span><span class="o">(</span><span class="kt">My</span> <span class="kt">Accumulator</span><span
                            class="o">),</span> <span class="n">value</span><span class="k">:</span> <span
                            class="err">0</span><span class="o">)</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">sc</span><span class="o">.</span><span
                            class="n">parallelize</span><span class="o">(</span><span class="nc">Array</span><span
                            class="o">(</span><span class="mi">1</span><span class="o">,</span> <span
                            class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span
                            class="o">,</span> <span class="mi">4</span><span class="o">)).</span><span class="n">foreach</span><span
                            class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">accum</span><span
                            class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">x</span><span
                            class="o">))</span>
<span class="o">...</span>
<span class="mi">10</span><span class="o">/</span><span class="mi">09</span><span class="o">/</span><span
                            class="mi">29</span> <span class="mi">18</span><span class="k">:</span><span
                            class="err">41</span><span class="kt">:</span><span class="err">08</span> <span class="kt">INFO</span> <span
                            class="kt">SparkContext:</span> <span class="kt">Tasks</span> <span
                            class="kt">finished</span> <span class="kt">in</span> <span class="err">0</span><span
                            class="kt">.</span><span class="err">317106</span> <span class="kt">s</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">accum</span><span class="o">.</span><span
                            class="n">value</span>
<span class="n">res2</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span
                            class="mi">10</span></code></pre>
                </figure>

                <p>虽然这段代码使用了内置的Long类型累加器支持，但程序员也可以通过子类化AccumulatorV2来创建自己的类型。AccumulatorV2 抽象类有几个需要 override（重写）的方法: reset 方法可将累加器重置为 0;add 方法可将其它值添加到累加器中;merge 方法可将其他同样类型的累加器合并为一个。必须重写的其他方法包含在API文档中。例如，假设我们有一个表示数学向量的MyVector类，我们可以这样写:</p>

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span
                        class="k">class</span> <span class="nc">VectorAccumulatorV2</span> <span
                        class="k">extends</span> <span class="nc">AccumulatorV2</span><span class="o">[</span><span
                        class="kt">MyVector</span>, <span class="kt">MyVector</span><span class="o">]</span> <span
                        class="o">{</span>

  <span class="k">private</span> <span class="k">val</span> <span class="n">myVector</span><span
                            class="k">:</span> <span class="kt">MyVector</span> <span class="o">=</span> <span
                            class="nc">MyVector</span><span class="o">.</span><span class="n">createZeroVector</span>

  <span class="k">def</span> <span class="n">reset</span><span class="o">()</span><span class="k">:</span> <span
                            class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">myVector</span><span class="o">.</span><span class="n">reset</span><span class="o">()</span>
  <span class="o">}</span>

  <span class="k">def</span> <span class="n">add</span><span class="o">(</span><span class="n">v</span><span
                            class="k">:</span> <span class="kt">MyVector</span><span class="o">)</span><span
                            class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span
                            class="o">{</span>
    <span class="n">myVector</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span
                            class="n">v</span><span class="o">)</span>
  <span class="o">}</span>
  <span class="o">...</span>
<span class="o">}</span>

<span class="c1">// Then, create an Accumulator of this type:</span>
<span class="k">val</span> <span class="n">myVectorAcc</span> <span class="k">=</span> <span class="k">new</span> <span
                            class="nc">VectorAccumulatorV2</span>
<span class="c1">// Then, register it into spark context:</span>
<span class="n">sc</span><span class="o">.</span><span class="n">register</span><span class="o">(</span><span class="n">myVectorAcc</span><span
                            class="o">,</span> <span class="s">&quot;MyVectorAcc1&quot;</span><span
                            class="o">)</span></code></pre>
                </figure>

                <p>注意，在开发者定义自己的 AccumulatorV2 类型时， resulting type（返回值类型）可能与添加的元素的类型不一致。

                </p>

            </div>

            <div data-lang="java">

                <p>可以通过调用 <code>SparkContext.longAccumulator()</code> 或 <code>SparkContext.doubleAccumulator()</code>
                    来创建一个数值累加器分别累积 Long 或 Double 类型的值。 然后可以使用群集中的任务进行添加 <code>add</code> 方法。
                    但是，他们看不到它的 value（值）。 只有 driver 可以读取累加器的值，使用它的<code>value</code> 方法。</p>

                <p>下面的代码显示了一个累加器用于将数组的元素相加:</p>

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span
                        class="n">LongAccumulator</span> <span class="n">accum</span> <span class="o">=</span> <span
                        class="n">jsc</span><span class="o">.</span><span class="na">sc</span><span class="o">().</span><span
                        class="na">longAccumulator</span><span class="o">();</span>

<span class="n">sc</span><span class="o">.</span><span class="na">parallelize</span><span class="o">(</span><span
                            class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span
                            class="o">(</span><span class="mi">1</span><span class="o">,</span> <span
                            class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span
                            class="o">,</span> <span class="mi">4</span><span class="o">)).</span><span class="na">foreach</span><span
                            class="o">(</span><span class="n">x</span> <span class="o">-&gt;</span> <span class="n">accum</span><span
                            class="o">.</span><span class="na">add</span><span class="o">(</span><span
                            class="n">x</span><span class="o">));</span>
<span class="c1">// ...</span>
<span class="c1">// 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s</span>

<span class="n">accum</span><span class="o">.</span><span class="na">value</span><span class="o">();</span>
<span class="c1">// returns 10</span></code></pre>
                </figure>

                <p>While this code used the built-in support for accumulators of type Long, programmers can also
                    create their own types by subclassing <a
                            href="api/scala/index.html#org.apache.spark.util.AccumulatorV2">AccumulatorV2</a>.
                    The AccumulatorV2 abstract class has several methods which one has to override: <code>reset</code>
                    for resetting
                    the accumulator to zero, <code>add</code> for adding another value into the accumulator,
                    <code>merge</code> for merging another same-type accumulator into this one. Other methods that must
                    be overridden
                    are contained in the <a href="api/scala/index.html#org.apache.spark.util.AccumulatorV2">API
                        documentation</a>. For example, supposing we had a <code>MyVector</code> class
                    representing mathematical vectors, we could write:</p>

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span
                        class="kd">class</span> <span class="nc">VectorAccumulatorV2</span> <span
                        class="kd">implements</span> <span class="n">AccumulatorV2</span><span
                        class="o">&lt;</span><span class="n">MyVector</span><span class="o">,</span> <span class="n">MyVector</span><span
                        class="o">&gt;</span> <span class="o">{</span>

  <span class="kd">private</span> <span class="n">MyVector</span> <span class="n">myVector</span> <span
                            class="o">=</span> <span class="n">MyVector</span><span class="o">.</span><span class="na">createZeroVector</span><span
                            class="o">();</span>

  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">reset</span><span
                            class="o">()</span> <span class="o">{</span>
    <span class="n">myVector</span><span class="o">.</span><span class="na">reset</span><span class="o">();</span>
  <span class="o">}</span>

  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">add</span><span class="o">(</span><span
                            class="n">MyVector</span> <span class="n">v</span><span class="o">)</span> <span
                            class="o">{</span>
    <span class="n">myVector</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span
                            class="n">v</span><span class="o">);</span>
  <span class="o">}</span>
  <span class="o">...</span>
<span class="o">}</span>

<span class="c1">// Then, create an Accumulator of this type:</span>
<span class="n">VectorAccumulatorV2</span> <span class="n">myVectorAcc</span> <span class="o">=</span> <span class="k">new</span> <span
                            class="n">VectorAccumulatorV2</span><span class="o">();</span>
<span class="c1">// Then, register it into spark context:</span>
<span class="n">jsc</span><span class="o">.</span><span class="na">sc</span><span class="o">().</span><span class="na">register</span><span
                            class="o">(</span><span class="n">myVectorAcc</span><span class="o">,</span> <span
                            class="s">&quot;MyVectorAcc1&quot;</span><span class="o">);</span></code></pre>
                </figure>

                <p>Note that, when programmers define their own type of AccumulatorV2, the resulting type can be
                    different than that of the elements added.</p>

            </div>

            <div data-lang="python">

                <p>An accumulator is created from an initial value <code>v</code> by calling <code>SparkContext.accumulator(v)</code>.
                    Tasks
                    running on a cluster can then add to it using the <code>add</code> method or the <code>+=</code>
                    operator. However, they cannot read its value.
                    Only the driver program can read the accumulator&#8217;s value, using its <code>value</code> method.
                </p>

                <p>The code below shows an accumulator being used to add up the elements of an array:</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span
                        class="o">&gt;&gt;&gt;</span> <span class="n">accum</span> <span class="o">=</span> <span
                        class="n">sc</span><span class="o">.</span><span class="n">accumulator</span><span
                        class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">accum</span>
<span class="n">Accumulator</span><span class="o">&lt;</span><span class="nb">id</span><span class="o">=</span><span
                            class="mi">0</span><span class="p">,</span> <span class="n">value</span><span
                            class="o">=</span><span class="mi">0</span><span class="o">&gt;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">sc</span><span class="o">.</span><span
                            class="n">parallelize</span><span class="p">([</span><span class="mi">1</span><span
                            class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span
                            class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span
                            class="p">])</span><span class="o">.</span><span class="n">foreach</span><span
                            class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span
                            class="p">:</span> <span class="n">accum</span><span class="o">.</span><span
                            class="n">add</span><span class="p">(</span><span class="n">x</span><span
                            class="p">))</span>
<span class="o">...</span>
<span class="mi">10</span><span class="o">/</span><span class="mi">09</span><span class="o">/</span><span
                            class="mi">29</span> <span class="mi">18</span><span class="p">:</span><span
                            class="mi">41</span><span class="p">:</span><span class="mi">08</span> <span
                            class="n">INFO</span> <span class="n">SparkContext</span><span class="p">:</span> <span
                            class="n">Tasks</span> <span class="n">finished</span> <span class="ow">in</span> <span
                            class="mf">0.317106</span> <span class="n">s</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">accum</span><span class="o">.</span><span class="n">value</span>
<span class="mi">10</span></code></pre>
                </figure>

                <p>While this code used the built-in support for accumulators of type Int, programmers can also
                    create their own types by subclassing <a href="api/python/pyspark.html#pyspark.AccumulatorParam">AccumulatorParam</a>.
                    The AccumulatorParam interface has two methods: <code>zero</code> for providing a &#8220;zero value&#8221;
                    for your data
                    type, and <code>addInPlace</code> for adding two values together. For example, supposing we had a
                    <code>Vector</code> class
                    representing mathematical vectors, we could write:</p>

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span
                        class="k">class</span> <span class="nc">VectorAccumulatorParam</span><span
                        class="p">(</span><span class="n">AccumulatorParam</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">zero</span><span class="p">(</span><span class="bp">self</span><span
                            class="p">,</span> <span class="n">initialValue</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Vector</span><span class="o">.</span><span
                            class="n">zeros</span><span class="p">(</span><span class="n">initialValue</span><span
                            class="o">.</span><span class="n">size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">addInPlace</span><span class="p">(</span><span
                            class="bp">self</span><span class="p">,</span> <span class="n">v1</span><span
                            class="p">,</span> <span class="n">v2</span><span class="p">):</span>
        <span class="n">v1</span> <span class="o">+=</span> <span class="n">v2</span>
        <span class="k">return</span> <span class="n">v1</span>

<span class="c1"># Then, create an Accumulator of this type:</span>
<span class="n">vecAccum</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span
                            class="n">accumulator</span><span class="p">(</span><span class="n">Vector</span><span
                            class="p">(</span><span class="o">...</span><span class="p">),</span> <span class="n">VectorAccumulatorParam</span><span
                            class="p">())</span></code></pre>
                </figure>

            </div>

        </div>

        <p>累加器的更新只发生在 action 操作中，Spark 保证每个任务只更新累加器一次，例如，重启任务不会更新值。在 transformations（转换）中， 用户需要注意的是，如果 task（任务）或 job stages（阶段）重新执行，每个任务的更新操作可能会执行多次。</p>

        <p>累加器不会改变 Spark lazy evaluation（懒加载）的模式。如果累加器在 RDD 中的一个操作中进行更新，它们的值仅被更新一次，RDD 被作为 action 的一部分来计算。因此，在一个像 map() 这样的 transformation（转换）时，累加器的更新并没有执行。下面的代码片段证明了这个特性:</p>

        <div class="codetabs">

            <div data-lang="scala">

                <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span
                        class="k">val</span> <span class="n">accum</span> <span class="k">=</span> <span
                        class="n">sc</span><span class="o">.</span><span class="n">longAccumulator</span>
<span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span
                            class="n">x</span> <span class="k">=&gt;</span> <span class="n">accum</span><span class="o">.</span><span
                            class="n">add</span><span class="o">(</span><span class="n">x</span><span
                            class="o">);</span> <span class="n">x</span> <span class="o">}</span>
<span class="c1">// Here, accum is still 0 because no actions have caused the map operation to be computed.</span></code></pre>
                </figure>

            </div>

            <div data-lang="java">

                <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span
                        class="n">LongAccumulator</span> <span class="n">accum</span> <span class="o">=</span> <span
                        class="n">jsc</span><span class="o">.</span><span class="na">sc</span><span class="o">().</span><span
                        class="na">longAccumulator</span><span class="o">();</span>
<span class="n">data</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span
                            class="n">x</span> <span class="o">-&gt;</span> <span class="o">{</span> <span class="n">accum</span><span
                            class="o">.</span><span class="na">add</span><span class="o">(</span><span
                            class="n">x</span><span class="o">);</span> <span class="k">return</span> <span
                            class="n">f</span><span class="o">(</span><span class="n">x</span><span class="o">);</span> <span
                            class="o">});</span>
<span class="c1">// Here, accum is still 0 because no actions have caused the `map` to be computed.</span></code></pre>
                </figure>

            </div>

            <div data-lang="python">

                <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span
                        class="n">accum</span> <span class="o">=</span> <span class="n">sc</span><span
                        class="o">.</span><span class="n">accumulator</span><span class="p">(</span><span
                        class="mi">0</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span
                            class="p">):</span>
    <span class="n">accum</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span
                            class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span
                            class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span
                            class="n">g</span><span class="p">)</span>
<span class="c1"># Here, accum is still 0 because no actions have caused the `map` to be computed.</span></code></pre>
                </figure>

            </div>

        </div>

        <h1 id="deploying-to-a-cluster">部署应用到集群中</h1>

        <p>该 <a href="./latest/submitting-applications.html">应用提交指南 </a> 描述了如何将应用提交到集群中. 简单的说, 在您将应用打包成一个JAR(针对 Java/Scala) 或者一组  <code>.py</code> 或
            <code>.zip</code>  文件 (针对Python), 该
            <code>bin/spark-submit</code>脚本可以让你提交它到任何所支持的 cluster manager 上去。</p>

        <h1 id="launching-spark-jobs-from-java--scala">从 Java / Scala 启动 Spark jobs</h1>

        <p>该<a
                href="api/java/index.html?org/apache/spark/launcher/package-summary.html">org.apache.spark.launcher</a>
            package 提供了 classes 用于使用简单的 Java API 来作为一个子进程启动 Spark jobs。</p>

        <h1 id="unit-testing">单元测试</h1>

        <p>Spark 可以使用流行的单元测试框架进行单元测试。在将 master URL 设置为 <code>local</code> 来测试时会简单的创建一个 <code>SparkContext</code>运行您的操作，然后调用
             <code>SparkContext.stop()</code>  将该作业停止。因为 Spark 不支持在同一个程序中并行的运行两个 contexts，所以需要确保使用 finally 块或者测试框架的的 tearDown 方法将 context 停止。</p>

        <h1 id="where-to-go-from-here">快速链接</h1>

        <p>您可以在 Spark 网站上看一下 <a href="http://spark.apache.org/examples.html"> Spark 程序示例</a>
            此外, Spark 在<code>examples</code> 目录中包含了许多示例
            (<a href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples">Scala</a>,
            <a href="https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples">Java</a>,
            <a href="https://github.com/apache/spark/tree/master/examples/src/main/python">Python</a>,
            <a href="https://github.com/apache/spark/tree/master/examples/src/main/r">R</a>).
            您可以通过传递 class name 到 Spark 的 bin/run-example 脚本以运行 Java 和 Scala 示例; 例如:</p>

        <pre><code>./bin/run-example SparkPi
</code></pre>

        <p>针对 Python 示例，使用  <code>spark-submit</code> 来代替:</p>

        <pre><code>./bin/spark-submit examples/src/main/python/pi.py
</code></pre>

        <p>针对 R 示例，使用 <code>spark-submit</code>来代替:</p>

        <pre><code>./bin/spark-submit examples/src/main/r/dataframe.R
</code></pre>

        <p>针对应用程序的优化, 该<a href="configuration.html">配置</a> and
            <a href="tuning.html">优化</a> 指南一些最佳实践的信息. 这些优化建议在确保你的数据以高效的格式存储在内存中尤其重要. 针对部署参考, 该<a href="cluster-overview.html">cluster mode overview</a> 描述了分布式操作和支持的 cluster managers 集群管理器的组件.</p>

        <p>最后,所有的 API 文档可在
            <a href="api/scala/#org.apache.spark.package">Scala</a>, <a href="api/java/">Java</a>, <a
                    href="api/python/">Python</a> 和 <a href="api/R/">R</a>中获取。</p>


    </div>

    <!-- /container -->
    <p>总结：</p>
    <p>1)   HDFS中的Block与RDD的partion的关系区别</p>
    <p>2)   什么是RDD？RDD的5大特性、哪里体现分布式、哪里体现弹性、RDD存储数据吗？</p>
    <p>3)   RDD的partion合理设置、太大、太小带来的问题</p>
    <p>4)   转换、行动算子的使用</p>
    <p>5)   RDD的持久化操作</p>
    <p>6)   shuffle操作，如何避免shuffle产生</p>
    <p>6)   共享变量</p>
    <p> <a href="https://blog.csdn.net/yangshaojun1992/article/details/88362074">Spark总结</a></p>
</div>

<script src="js/vendor/jquery-1.8.0.min.js"></script>
<script src="js/vendor/bootstrap.min.js"></script>
<script src="js/vendor/anchor.min.js"></script>
<script src="js/main.js"></script>

<!-- MathJax Section -->
<script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });



</script>
<script>
    // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
    // We could use "//cdn.mathjax...", but that won't support "file://".
    (function (d, script) {
        script = d.createElement('script');
        script.type = 'text/javascript';
        script.async = true;
        script.onload = function () {
            MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [["$", "$"], ["\\\\(", "\\\\)"]],
                    displayMath: [["$$", "$$"], ["\\[", "\\]"]],
                    processEscapes: true,
                    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                }
            });
        };
        script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
            'cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        d.getElementsByTagName('head')[0].appendChild(script);
    }(document));
</script>
</body>
</html>
